{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32311e0",
   "metadata": {},
   "source": [
    "# Week 1 Jupyter Notebook – Linear Regression 1\n",
    "Each week, you will apply the concepts of that week to your Integrated Capstone Project’s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data, using the methods you have learned about in this course and in this program, and draw interesting conclusions. \n",
    "\n",
    "For Week 1, include concepts such as linear regression with polynomial terms, interaction terms, multicollinearity, variance inflation factor and regression, and categorical and continuous features. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "& was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "# Load Virtual Environment\n",
    "!& \"c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Scripts\\Activate.ps1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bda5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9a73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = 'C:/Users/tbran/Python/repos/Semester 3 Repos/capstone/'\n",
    "data_path = project_path + 'data/'\n",
    "src_path = project_path + 'src/'\n",
    "model_path = project_path + 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49883f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df = pd.read_csv(data_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7713e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(df, datetime_cols=None, fillna=False):\n",
    "    \"\"\"\n",
    "    Cleans and optimizes a DataFrame:\n",
    "    - Converts object datetime columns to datetime64\n",
    "    - Converts object columns with repeated values to category\n",
    "    - Downcasts numeric columns to smallest safe type\n",
    "    - Optionally fills NaNs before downcasting\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to optimize\n",
    "        datetime_cols (list): List of column names to convert to datetime\n",
    "        fillna (bool): If True, fills NaNs before downcasting\n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Convert datetime columns\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # 2. Convert object columns to category if appropriate\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < 0.5:  # heuristic: less than 50% unique\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # 3. Downcast numeric columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "    \n",
    "    for col in int_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in float_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    print(f\"Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_column_names(df, remove_accents=True):\n",
    "    \"\"\"\n",
    "    Cleans DataFrame column names:\n",
    "    - Strips whitespace\n",
    "    - Converts to lowercase\n",
    "    - Replaces spaces & special chars with underscores\n",
    "    - Removes duplicate underscores\n",
    "    - Optionally removes accents\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame whose columns to clean\n",
    "        remove_accents (bool): If True, strips accents from characters\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    def _clean(col):\n",
    "        col = col.strip().lower()\n",
    "        if remove_accents:\n",
    "            col = ''.join(\n",
    "                c for c in unicodedata.normalize('NFKD', col)\n",
    "                if not unicodedata.combining(c)\n",
    "            )\n",
    "        col = re.sub(r'[^0-9a-zA-Z]+', '_', col)  # replace non-alphanumeric with _\n",
    "        col = re.sub(r'_+', '_', col)             # collapse multiple underscores\n",
    "        col = col.strip('_')                      # remove leading/trailing underscores\n",
    "        return col\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.columns = [_clean(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline(df, target, \n",
    "                                  high_card_threshold=20, \n",
    "                                  scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for linear regression:\n",
    "    - One-hot encodes low-cardinality categorical columns\n",
    "    - Target encodes high-cardinality categorical columns\n",
    "    - Optionally scales numeric columns\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame (including target column)\n",
    "        target (str): Name of target column\n",
    "        high_card_threshold (int): Unique value cutoff for high-cardinality\n",
    "        scale_numeric (bool): Whether to scale numeric features\n",
    "        \n",
    "    Returns:\n",
    "        pipeline (ColumnTransformer): Preprocessing transformer\n",
    "        low_card_cols (list): Low-cardinality categorical columns\n",
    "        high_card_cols (list): High-cardinality categorical columns\n",
    "        num_cols (list): Numeric columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target])\n",
    "    \n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [col for col in cat_cols if X[col].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [col for col in cat_cols if X[col].nunique() > high_card_threshold]\n",
    "    \n",
    "    # Transformers\n",
    "    low_card_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    high_card_transformer = TargetEncoder()\n",
    "    num_transformer = StandardScaler() if scale_numeric else 'passthrough'\n",
    "    \n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', low_card_transformer, low_card_cols),\n",
    "            ('high_card', high_card_transformer, high_card_cols),\n",
    "            ('num', num_transformer, num_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor, low_card_cols, high_card_cols, num_cols\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df, features=None, vif_thresh=10.0):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) safely:\n",
    "    - Removes constant columns\n",
    "    - Removes perfectly collinear columns\n",
    "    - Returns sorted VIF table\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with numeric features\n",
    "        features (list): Optional list of features to check; defaults to all numeric\n",
    "        vif_thresh (float): Threshold for flagging high VIF\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: VIF table\n",
    "    \"\"\"\n",
    "    # Select numeric columns if features not provided\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # 1. Drop constant columns\n",
    "    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"Dropping constant columns: {constant_cols}\")\n",
    "        X.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    # 2. Drop perfectly collinear columns\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    perfect_corr_cols = [col for col in upper.columns if any(upper[col] == 1.0)]\n",
    "    if perfect_corr_cols:\n",
    "        print(f\"Dropping perfectly collinear columns: {perfect_corr_cols}\")\n",
    "        X.drop(columns=perfect_corr_cols, inplace=True)\n",
    "    \n",
    "    # 3. Calculate VIF\n",
    "    X_const = X.assign(const=1)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_const.values, i) for i in range(len(X.columns))]\n",
    "    })\n",
    "    \n",
    "    # 4. Sort by VIF\n",
    "    vif_data.sort_values(by=\"VIF\", ascending=False, inplace=True)\n",
    "    \n",
    "    # 5. Flag high VIF\n",
    "    vif_data[\"High_VIF\"] = vif_data[\"VIF\"] > vif_thresh\n",
    "    \n",
    "    return vif_data\n",
    "\n",
    "def fit_polynomial_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and returns the fitted model and transformed features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "def add_interaction_terms(df, features):\n",
    "    \"\"\"\n",
    "    Adds pairwise interaction terms between given features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    interaction_array = poly.fit_transform(df[features])\n",
    "    interaction_df = pd.DataFrame(interaction_array, columns=poly.get_feature_names_out(features))\n",
    "    return pd.concat([df.reset_index(drop=True), interaction_df], axis=1)\n",
    "\n",
    "def preprocess_features(df, categorical_cols, numeric_cols):\n",
    "    \"\"\"\n",
    "    Returns a ColumnTransformer that one-hot encodes categorical columns\n",
    "    and passes numeric columns through unchanged.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "            ('num', 'passthrough', numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def regression_summary(X, y):\n",
    "    \"\"\"\n",
    "    Fits an OLS regression model using statsmodels and prints the summary.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1404dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n"
     ]
    }
   ],
   "source": [
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2046a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n",
      "R^2 score: 0.043766421822242996\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f91cb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature           VIF  High_VIF\n",
      "13      largehubairportdest           inf      True\n",
      "12     mediumhubairportdest           inf      True\n",
      "10        nonhubairportdest           inf      True\n",
      "11      smallhubairportdest           inf      True\n",
      "7     smallhubairportorigin           inf      True\n",
      "6       nonhubairportorigin           inf      True\n",
      "18        nonhubairlinedest           inf      True\n",
      "19      smallhubairlinedest           inf      True\n",
      "16   mediumhubairlineorigin           inf      True\n",
      "17    largehubairlineorigin           inf      True\n",
      "20     mediumhubairlinedest           inf      True\n",
      "21      largehubairlinedest           inf      True\n",
      "15    smallhubairlineorigin           inf      True\n",
      "14      nonhubairlineorigin           inf      True\n",
      "8    mediumhubairportorigin  9.007199e+15      True\n",
      "9     largehubairportorigin  9.007199e+15      True\n",
      "39               temp_20_30  3.112541e+02      True\n",
      "38               temp_10_20  2.888918e+02      True\n",
      "37                temp_0_10  1.916812e+02      True\n",
      "40               temp_30_40  1.151481e+02      True\n",
      "36               temp_n10_0  7.219096e+01      True\n",
      "34              temperature  1.429181e+01      True\n",
      "35          temp_ninfty_n10  1.244962e+01      True\n",
      "43          windspeedsquare  9.783338e+00     False\n",
      "42                windspeed  8.658978e+00     False\n",
      "1                  arrdelay  8.096892e+00     False\n",
      "0                  depdelay  8.012084e+00     False\n",
      "41            temp_40_infty  5.988440e+00     False\n",
      "3           marketsharedest  3.643296e+00     False\n",
      "2         marketshareorigin  3.596286e+00     False\n",
      "22                     year  3.081408e+00     False\n",
      "45            windgustspeed  2.712213e+00     False\n",
      "31               numflights  2.603665e+00     False\n",
      "5                   hhidest  2.493102e+00     False\n",
      "4                 hhiorigin  2.430416e+00     False\n",
      "44            windgustdummy  2.181984e+00     False\n",
      "33            monopolyroute  1.598653e+00     False\n",
      "32                 distance  1.564135e+00     False\n",
      "51  originmetrogdppercapita  1.519503e+00     False\n",
      "30               loadfactor  1.452184e+00     False\n",
      "53    destmetrogdppercapita  1.450741e+00     False\n",
      "29                 capacity  1.332612e+00     False\n",
      "50           originmetropop  1.326248e+00     False\n",
      "52             destmetropop  1.326226e+00     False\n",
      "49           snowtracedummy  1.181472e+00     False\n",
      "27          originairportid  1.173991e+00     False\n",
      "25                dayofweek  1.165972e+00     False\n",
      "28            destairportid  1.160076e+00     False\n",
      "48                snowdummy  1.105665e+00     False\n",
      "26            scheduledhour  1.084384e+00     False\n",
      "46                raindummy  1.061760e+00     False\n",
      "23                    month  1.048324e+00     False\n",
      "47           raintracedummy  1.024877e+00     False\n",
      "24               dayofmonth  1.000590e+00     False\n"
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(df)\n",
    "print(vif_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
