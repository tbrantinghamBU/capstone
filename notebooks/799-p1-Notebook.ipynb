{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d58ef2",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa4899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "& was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "# Load Virtual Environment\n",
    "\n",
    "!& \"c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Scripts\\Activate.ps1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bda5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dates & holidays\n",
    "import holidays\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Scikit-learn: preprocessing & pipelines\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "import category_encoders as ce  # if you need additional encoders\n",
    "\n",
    "# Scikit-learn: models\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Lasso,\n",
    "    Ridge,\n",
    "    ElasticNet,\n",
    "    LogisticRegression\n",
    ")\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Scikit-learn: model selection & metrics\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Feature selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Distributions\n",
    "from scipy.stats import loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9a73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = 'C:/Users/tbran/Python/repos/Semester 3 Repos/capstone/'\n",
    "data_path = project_path + 'data/'\n",
    "src_path = project_path + 'src/'\n",
    "model_path = project_path + 'models/'\n",
    "\n",
    "# Initialize results list\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc5532",
   "metadata": {},
   "source": [
    "# Data Prep "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7983db",
   "metadata": {},
   "source": [
    "## Data Prep Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7713e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(df, datetime_cols=None, fillna=False):\n",
    "    \"\"\"\n",
    "    Cleans and optimizes a DataFrame:\n",
    "    - Converts object datetime columns to datetime64\n",
    "    - Converts object columns with repeated values to category\n",
    "    - Downcasts numeric columns to smallest safe type\n",
    "    - Optionally fills NaNs before downcasting\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to optimize\n",
    "        datetime_cols (list): List of column names to convert to datetime\n",
    "        fillna (bool): If True, fills NaNs before downcasting\n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Convert datetime columns\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # 2. Convert object columns to category if appropriate\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < 0.5:  # heuristic: less than 50% unique\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # 3. Downcast numeric columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "    \n",
    "    for col in int_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in float_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    print(f\"Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_column_names(df, remove_accents=True):\n",
    "    \"\"\"\n",
    "    Cleans DataFrame column names:\n",
    "    - Strips whitespace\n",
    "    - Converts to lowercase\n",
    "    - Replaces spaces & special chars with underscores\n",
    "    - Removes duplicate underscores\n",
    "    - Optionally removes accents\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame whose columns to clean\n",
    "        remove_accents (bool): If True, strips accents from characters\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    def _clean(col):\n",
    "        col = col.strip().lower()\n",
    "        if remove_accents:\n",
    "            col = ''.join(\n",
    "                c for c in unicodedata.normalize('NFKD', col)\n",
    "                if not unicodedata.combining(c)\n",
    "            )\n",
    "        col = re.sub(r'[^0-9a-zA-Z]+', '_', col)  # replace non-alphanumeric with _\n",
    "        col = re.sub(r'_+', '_', col)             # collapse multiple underscores\n",
    "        col = col.strip('_')                      # remove leading/trailing underscores\n",
    "        return col\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.columns = [_clean(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_dual_preprocessors(df, target, feature_cols,\n",
    "                             high_card_threshold=20, scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Build regression + tree preprocessors using an explicit feature list.\n",
    "    \"\"\"\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target]\n",
    "\n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [c for c in cat_cols if X[c].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [c for c in cat_cols if X[c].nunique() > high_card_threshold]\n",
    "\n",
    "    regression_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', OneHotEncoder(handle_unknown='ignore', sparse_output=False), low_card_cols),\n",
    "            ('high_card', ce.TargetEncoder(), high_card_cols),\n",
    "            ('num', StandardScaler() if scale_numeric else 'passthrough', num_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tree_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', 'passthrough', cat_cols),\n",
    "            ('num', 'passthrough', num_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return regression_preprocessor, tree_preprocessor, X, y\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline(df, target, \n",
    "                                  high_card_threshold=20, \n",
    "                                  scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for linear regression:\n",
    "    - One-hot encodes low-cardinality categorical columns\n",
    "    - Target encodes high-cardinality categorical columns\n",
    "    - Optionally scales numeric columns\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame (including target column)\n",
    "        target (str): Name of target column\n",
    "        high_card_threshold (int): Unique value cutoff for high-cardinality\n",
    "        scale_numeric (bool): Whether to scale numeric features\n",
    "        \n",
    "    Returns:\n",
    "        pipeline (ColumnTransformer): Preprocessing transformer\n",
    "        low_card_cols (list): Low-cardinality categorical columns\n",
    "        high_card_cols (list): High-cardinality categorical columns\n",
    "        num_cols (list): Numeric columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target])\n",
    "    \n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [col for col in cat_cols if X[col].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [col for col in cat_cols if X[col].nunique() > high_card_threshold]\n",
    "    \n",
    "    # Transformers\n",
    "    low_card_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    high_card_transformer = TargetEncoder()\n",
    "    num_transformer = StandardScaler() if scale_numeric else 'passthrough'\n",
    "    \n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', low_card_transformer, low_card_cols),\n",
    "            ('high_card', high_card_transformer, high_card_cols),\n",
    "            ('num', num_transformer, num_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor, low_card_cols, high_card_cols, num_cols\n",
    "\n",
    "def add_interaction_terms(df, features):\n",
    "    \"\"\"\n",
    "    Adds pairwise interaction terms between given features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    interaction_array = poly.fit_transform(df[features])\n",
    "    interaction_df = pd.DataFrame(interaction_array, columns=poly.get_feature_names_out(features))\n",
    "    return pd.concat([df.reset_index(drop=True), interaction_df], axis=1)\n",
    "\n",
    "def preprocess_features(df, categorical_cols, numeric_cols):\n",
    "    \"\"\"\n",
    "    Returns a ColumnTransformer that one-hot encodes categorical columns\n",
    "    and passes numeric columns through unchanged.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "            ('num', 'passthrough', numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def transform_with_names(preprocessor, X, y=None):\n",
    "    \"\"\"Fit/transform and return a DataFrame with feature names preserved.\"\"\"\n",
    "    Xt = preprocessor.fit_transform(X, y)\n",
    "    cols = preprocessor.get_feature_names_out()\n",
    "    return pd.DataFrame(Xt, columns=cols, index=X.index)\n",
    "\n",
    "\n",
    "def create_features_mend(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Build U.S. holiday calendar for relevant years\n",
    "    us_holidays = holidays.US(years=range(2015, 2027))\n",
    "\n",
    "    # Build a list of (holiday_date, holiday_name)\n",
    "    holiday_items = list(us_holidays.items())\n",
    "\n",
    "    # Create a set of all holiday dates ±3 days\n",
    "    holiday_buffer = {}\n",
    "    for h_date, h_name in holiday_items:\n",
    "        for offset in range(-3, 4):  # -3, -2, -1, 0, +1, +2, +3\n",
    "            holiday_buffer[h_date + pd.Timedelta(days=offset)] = h_name\n",
    "\n",
    "    # Binary flag: within ±3 days of a holiday\n",
    "    if \"scheduleddepartdatetime\" in df.columns:\n",
    "        print(\"Adding holiday features...\")\n",
    "        df['is_holiday_period'] = df['scheduleddepartdatetime'].dt.date.isin(holiday_buffer.keys())\n",
    "\n",
    "        # Categorical holiday name (or \"None\")\n",
    "        def get_holiday_name(d):\n",
    "            return holiday_buffer.get(d, \"None\")\n",
    "\n",
    "        df['holiday_name'] = df['scheduleddepartdatetime'].dt.date.apply(get_holiday_name)\n",
    "    else:\n",
    "        print(\"Column 'scheduleddepartdatetime' not found, skipping holiday features.\")\n",
    "\n",
    "    # --- Existing engineered features ---\n",
    "    if \"scheduleddepartdatetime\" in df.columns:\n",
    "        print(\"Adding time-based features...\")\n",
    "        df[\"dayofweek\"] = df[\"scheduleddepartdatetime\"].dt.dayofweek\n",
    "        df[\"month\"] = df[\"scheduleddepartdatetime\"].dt.month\n",
    "    else:\n",
    "        print(\"Skipping time-based features.\")\n",
    "\n",
    "    if {\"origin\",\"dest\"}.issubset(df.columns):\n",
    "        print(\"Adding route feature...\")\n",
    "        df[\"route\"] = df[\"origin\"].astype(str) + \"_\" + df[\"dest\"].astype(str)\n",
    "    else:\n",
    "        print(\"Skipping route feature.\")\n",
    "\n",
    "    if {\"marketshareorigin\",\"marketsharedest\"}.issubset(df.columns):\n",
    "        print(\"Adding marketshare_diff...\")\n",
    "        df[\"marketshare_diff\"] = df[\"marketshareorigin\"] - df[\"marketsharedest\"]\n",
    "    else:\n",
    "        print(\"Skipping marketshare_diff.\")\n",
    "\n",
    "    if {\"hhiorigin\",\"hhidest\"}.issubset(df.columns):\n",
    "        print(\"Adding hhi_diff...\")\n",
    "        df[\"hhi_diff\"] = df[\"hhiorigin\"] - df[\"hhidest\"]\n",
    "    else:\n",
    "        print(\"Skipping hhi_diff.\")\n",
    "\n",
    "    if {\"temperature\",\"windspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding temp_wind_interaction...\")\n",
    "        df[\"temp_wind_interaction\"] = df[\"temperature\"] * df[\"windspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping temp_wind_interaction.\")\n",
    "\n",
    "    if {\"temperature\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding temp_windgust_interaction...\")\n",
    "        df[\"temp_windgust_interaction\"] = df[\"temperature\"] * df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping temp_windgust_interaction.\")\n",
    "\n",
    "    if {\"windspeed\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding wind_gust_diff...\")\n",
    "        df[\"wind_gust_diff\"] = df[\"windspeed\"] - df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping wind_gust_diff.\")\n",
    "\n",
    "    if {\"raindummy\",\"windspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding rain_wind_interaction...\")\n",
    "        df[\"rain_wind_interaction\"] = df[\"raindummy\"] * df[\"windspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping rain_wind_interaction.\")\n",
    "\n",
    "    if {\"snowdummy\",\"windspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding snow_wind_interaction...\")\n",
    "        df[\"snow_wind_interaction\"] = df[\"snowdummy\"] * df[\"windspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping snow_wind_interaction.\")\n",
    "\n",
    "    if {\"raindummy\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding rain_wind_gust_interaction...\")\n",
    "        df[\"rain_wind_gust_interaction\"] = df[\"raindummy\"] * df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping rain_wind_gust_interaction.\")\n",
    "\n",
    "    if {\"snowdummy\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding snow_wind_gust_interaction...\")\n",
    "        df[\"snow_wind_gust_interaction\"] = df[\"snowdummy\"] * df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping snow_wind_gust_interaction.\")\n",
    "\n",
    "    if {\"originmetropop\",\"destmetropop\"}.issubset(df.columns):\n",
    "        print(\"Adding metropop_diff...\")\n",
    "        df[\"metropop_diff\"] = df[\"originmetropop\"] - df[\"destmetropop\"]\n",
    "    else:\n",
    "        print(\"Skipping metropop_diff.\")\n",
    "\n",
    "    if {\"originmetrogdppercapita\",\"destmetrogdppercapita\"}.issubset(df.columns):\n",
    "        print(\"Adding metrogdp_diff...\")\n",
    "        df[\"metrogdp_diff\"] = df[\"originmetrogdppercapita\"] - df[\"destmetrogdppercapita\"]\n",
    "    else:\n",
    "        print(\"Skipping metrogdp_diff.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def engineer_flight_features_light(df, datetime_col=\"scheduleddepartdatetime\",\n",
    "                                   origin_col=\"origin\", dest_col=\"dest\",\n",
    "                                   carrier_col=\"uniquecarrier\", delay_col=\"depdelay\",\n",
    "                                   distance_col=\"distance\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Datetime parts ---\n",
    "    dt = pd.to_datetime(df[datetime_col])\n",
    "    df[\"year\"] = dt.dt.year\n",
    "    df[\"month\"] = dt.dt.month\n",
    "    df[\"day\"] = dt.dt.day\n",
    "    df[\"hour\"] = dt.dt.hour\n",
    "    df[\"date\"] = dt.dt.floor(\"D\")\n",
    "    df[\"is_weekend\"] = dt.dt.dayofweek >= 5\n",
    "\n",
    "    # --- Route & distance ---\n",
    "    df[\"route\"] = df[origin_col].astype(str) + \"_\" + df[dest_col].astype(str)\n",
    "    if distance_col in df.columns:\n",
    "        df[\"distance_bin\"] = pd.cut(df[distance_col],\n",
    "                                    bins=[0,500,1500,3000,10000],\n",
    "                                    labels=[\"short\",\"medium\",\"long\",\"ultra\"])\n",
    "\n",
    "    # --- Congestion (lighter via transform) ---\n",
    "    df[\"hourly_origin_flights\"] = (\n",
    "        df.groupby([origin_col,\"date\",\"hour\"])[delay_col].transform(\"count\")\n",
    "    )\n",
    "    df[\"daily_route_flights\"] = (\n",
    "        df.groupby([origin_col,dest_col,\"date\"])[delay_col].transform(\"count\")\n",
    "    )\n",
    "\n",
    "    # --- Weather (light) ---\n",
    "    if \"temperature\" in df.columns:\n",
    "        df[\"is_extreme_temp\"] = (df[\"temperature\"] < 0) | (df[\"temperature\"] > 35)\n",
    "    if {\"raindummy\",\"snowdummy\",\"windgustdummy\"}.issubset(df.columns):\n",
    "        df[\"stormy\"] = (df[\"raindummy\"]|df[\"snowdummy\"]|df[\"windgustdummy\"]).astype(int)\n",
    "\n",
    "    # --- Market/demand ---\n",
    "    if {\"capacity\",\"numflights\"}.issubset(df.columns):\n",
    "        df[\"capacity_utilization\"] = df[\"numflights\"] / df[\"capacity\"].replace(0, np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_flight_features_heavy(\n",
    "    df,\n",
    "    datetime_col=\"scheduleddepartdatetime\",\n",
    "    origin_col=\"origin\",\n",
    "    dest_col=\"dest\",\n",
    "    carrier_col=\"uniquecarrier\",\n",
    "    delay_col=\"depdelay\",\n",
    "    distance_col=\"distance\",\n",
    "    window=7\n",
    "):\n",
    "    \"\"\"\n",
    "    Engineer advanced features for flight delay prediction.\n",
    "    Optimized for speed, with print statements and timing checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    df = df.copy()\n",
    "    print(\"Starting feature engineering...\")\n",
    "\n",
    "    # --- Precompute datetime parts once ---\n",
    "    t0 = time.time()\n",
    "    if datetime_col in df.columns:\n",
    "        dt = pd.to_datetime(df[datetime_col])\n",
    "        df[\"year\"] = dt.dt.year\n",
    "        df[\"month\"] = dt.dt.month\n",
    "        df[\"day\"] = dt.dt.day\n",
    "        df[\"hour\"] = dt.dt.hour\n",
    "        df[\"date\"] = dt.dt.floor(\"D\")\n",
    "        df[\"quarter\"] = dt.dt.quarter\n",
    "        df[\"is_weekend\"] = dt.dt.dayofweek >= 5\n",
    "        df[\"part_of_day\"] = pd.cut(\n",
    "            df[\"hour\"],\n",
    "            bins=[0,5,11,16,21,24],\n",
    "            labels=[\"late_night\",\"morning\",\"midday\",\"evening\",\"night\"],\n",
    "            right=False\n",
    "        )\n",
    "        df[\"days_since_year_start\"] = (\n",
    "            dt - pd.to_datetime(df[\"year\"].astype(str) + \"-01-01\")\n",
    "        ).dt.days\n",
    "    print(f\"Datetime features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Route & distance features ---\n",
    "    t0 = time.time()\n",
    "    if {origin_col, dest_col}.issubset(df.columns):\n",
    "        if f\"largehubairport{origin_col}\" in df.columns and f\"largehubairport{dest_col}\" in df.columns:\n",
    "            df[\"hub_to_hub\"] = (\n",
    "                (df[f\"largehubairport{origin_col}\"] == 1) &\n",
    "                (df[f\"largehubairport{dest_col}\"] == 1)\n",
    "            ).astype(int)\n",
    "        df[\"route\"] = df[origin_col].astype(str) + \"_\" + df[dest_col].astype(str)\n",
    "\n",
    "    if distance_col in df.columns:\n",
    "        df[\"distance_bin\"] = pd.cut(\n",
    "            df[distance_col],\n",
    "            bins=[0,500,1500,3000,10000],\n",
    "            labels=[\"short\",\"medium\",\"long\",\"ultra\"]\n",
    "        )\n",
    "    print(f\"Route & distance features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Congestion features ---\n",
    "    t0 = time.time()\n",
    "    if {\"date\",\"hour\",origin_col}.issubset(df.columns):\n",
    "        hourly_counts = (\n",
    "            df.groupby([origin_col,\"date\",\"hour\"])\n",
    "              .size()\n",
    "              .rename(\"hourly_origin_flights\")\n",
    "              .reset_index()\n",
    "        )\n",
    "        df = df.merge(hourly_counts, on=[origin_col,\"date\",\"hour\"], how=\"left\")\n",
    "\n",
    "    if {\"date\",origin_col,dest_col}.issubset(df.columns):\n",
    "        daily_counts = (\n",
    "            df.groupby([origin_col,dest_col,\"date\"])\n",
    "              .size()\n",
    "              .rename(\"daily_route_flights\")\n",
    "              .reset_index()\n",
    "        )\n",
    "        df = df.merge(daily_counts, on=[origin_col,dest_col,\"date\"], how=\"left\")\n",
    "    print(f\"Congestion features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Weather features ---\n",
    "    t0 = time.time()\n",
    "    if \"temperature\" in df.columns:\n",
    "        df[\"is_extreme_temp\"] = (df[\"temperature\"] < 0) | (df[\"temperature\"] > 35)\n",
    "        if \"month\" in df.columns:\n",
    "            monthly_means = df.groupby(\"month\")[\"temperature\"].transform(\"mean\")\n",
    "            df[\"temp_anomaly\"] = df[\"temperature\"] - monthly_means\n",
    "\n",
    "    if {\"raindummy\",\"snowdummy\",\"windgustdummy\"}.issubset(df.columns):\n",
    "        df[\"stormy\"] = (\n",
    "            (df[\"raindummy\"]==1) | (df[\"snowdummy\"]==1) | (df[\"windgustdummy\"]==1)\n",
    "        ).astype(int)\n",
    "    print(f\"Weather features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Rolling averages ---\n",
    "    t0 = time.time()\n",
    "    if {origin_col, delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([origin_col, datetime_col])\n",
    "        df[\"rolling_origin_delay\"] = (\n",
    "            df.groupby(origin_col)[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    if {dest_col, delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([dest_col, datetime_col])\n",
    "        df[\"rolling_dest_delay\"] = (\n",
    "            df.groupby(dest_col)[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    if {carrier_col, delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([carrier_col, datetime_col])\n",
    "        df[\"rolling_carrier_delay\"] = (\n",
    "            df.groupby(carrier_col)[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    if {\"route\", delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([\"route\", datetime_col])\n",
    "        df[\"rolling_route_delay\"] = (\n",
    "            df.groupby(\"route\")[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    print(f\"Rolling averages done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Market/demand features ---\n",
    "    t0 = time.time()\n",
    "    if {\"capacity\",\"numflights\"}.issubset(df.columns):\n",
    "        df[\"capacity_utilization\"] = (\n",
    "            df[\"numflights\"] / df[\"capacity\"].replace(0, np.nan)\n",
    "        )\n",
    "\n",
    "    if {origin_col, dest_col, carrier_col}.issubset(df.columns):\n",
    "        df[\"route_carrier_count\"] = (\n",
    "            df.groupby([origin_col,dest_col])[carrier_col].transform(\"nunique\")\n",
    "        )\n",
    "    print(f\"Market/demand features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Interaction features ---\n",
    "    t0 = time.time()\n",
    "    if {\"is_holiday_period\",\"monopolyroute\"}.issubset(df.columns):\n",
    "        df[\"holiday_monopoly\"] = (\n",
    "            df[\"is_holiday_period\"].astype(int) * df[\"monopolyroute\"].astype(int)\n",
    "        )\n",
    "\n",
    "    if {\"is_extreme_temp\",\"hourly_origin_flights\"}.issubset(df.columns):\n",
    "        df[\"extreme_temp_congestion\"] = (\n",
    "            df[\"is_extreme_temp\"].astype(int) * df[\"hourly_origin_flights\"]\n",
    "        )\n",
    "    print(f\"Interaction features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    print(f\"Total feature engineering time: {time.time()-start_time:.2f}s\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec784f5",
   "metadata": {},
   "source": [
    "## Data Prep Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce00485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n",
      "Adding holiday features...\n",
      "Adding time-based features...\n",
      "Adding route feature...\n",
      "Adding marketshare_diff...\n",
      "Adding hhi_diff...\n",
      "Adding temp_wind_interaction...\n",
      "Adding temp_windgust_interaction...\n",
      "Adding wind_gust_diff...\n",
      "Adding rain_wind_interaction...\n",
      "Adding snow_wind_interaction...\n",
      "Adding rain_wind_gust_interaction...\n",
      "Adding snow_wind_gust_interaction...\n",
      "Adding metropop_diff...\n",
      "Adding metrogdp_diff...\n"
     ]
    }
   ],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df_mend = pd.read_csv(data_path + file_name)\n",
    "\n",
    "df_mend = optimize_dataframe(\n",
    "    df_mend,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df_mend = clean_column_names(df_mend)\n",
    "\n",
    "df_mend_id_cols = ['originairportid', 'destairportid', ]\n",
    "df_mend_cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "df_mend_date_cols = ['scheduleddepartdatetime', ]\n",
    "df_mend_target_cols = ['depdelay','arrdelay',]\n",
    "df_mend_feature_cols = [col for col in df_mend.columns if col not in df_mend_id_cols + df_mend_cat_cols + df_mend_date_cols + df_mend_target_cols]\n",
    "\n",
    "# Remove outliers from dataframe\n",
    "df_mend_clean = df_mend[df_mend['depdelay'] >= -30]\n",
    "\n",
    "# Create engineered features\n",
    "df_mend_clean = create_features_mend(df_mend_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174022cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_19324\\3949246449.py:334: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby([origin_col,\"date\",\"hour\"])[delay_col].transform(\"count\")\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_19324\\3949246449.py:337: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby([origin_col,dest_col,\"date\"])[delay_col].transform(\"count\")\n"
     ]
    }
   ],
   "source": [
    "df_mend_clean = engineer_flight_features_light(df_mend_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b51d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop leakage columns, ID columns, and date columns\n",
    "df_mend_clean = df_mend_clean.drop(columns=['arrdelay'] + df_mend_id_cols + df_mend_date_cols).copy()\n",
    "\n",
    "reg_prep_mend, tree_prep_mend, X_mend, y_mend_numeric = build_dual_preprocessors(df_mend_clean, target='depdelay', feature_cols=df_mend_feature_cols, high_card_threshold=30, scale_numeric=True)\n",
    "\n",
    "# Create binary target for classification (15 min delay threshold)\n",
    "y_mend_binary_15 = (y_mend_numeric >= 15).astype(int)\n",
    "\n",
    "X_reg_mend = transform_with_names(reg_prep_mend, X_mend, y_mend_numeric)\n",
    "X_tree_mend = transform_with_names(tree_prep_mend, X_mend, y_mend_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32311e0",
   "metadata": {},
   "source": [
    "# Week 1 Notebook – Linear Regression 1\n",
    "Each week, you will apply the concepts of that week to your Integrated Capstone Project’s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data, using the methods you have learned about in this course and in this program, and draw interesting conclusions. \n",
    "\n",
    "For Week 1, include concepts such as linear regression with polynomial terms, interaction terms, multicollinearity, variance inflation factor and regression, and categorical and continuous features. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e5e44",
   "metadata": {},
   "source": [
    "## Week 1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a83f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regression_summary(X, y):\n",
    "    \"\"\"\n",
    "    Fits an OLS regression model using statsmodels and prints the summary.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    return model.summary()\n",
    "\n",
    "def fit_polynomial_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and returns the fitted model and transformed features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "\n",
    "def calculate_vif(df, features=None, vif_thresh=10.0):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) safely:\n",
    "    - Removes constant columns\n",
    "    - Removes perfectly collinear columns\n",
    "    - Returns sorted VIF table\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with numeric features\n",
    "        features (list): Optional list of features to check; defaults to all numeric\n",
    "        vif_thresh (float): Threshold for flagging high VIF\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: VIF table\n",
    "    \"\"\"\n",
    "    # Select numeric columns if features not provided\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # 1. Drop constant columns\n",
    "    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"Dropping constant columns: {constant_cols}\")\n",
    "        X.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    # 2. Drop perfectly collinear columns\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    perfect_corr_cols = [col for col in upper.columns if any(upper[col] == 1.0)]\n",
    "    if perfect_corr_cols:\n",
    "        print(f\"Dropping perfectly collinear columns: {perfect_corr_cols}\")\n",
    "        X.drop(columns=perfect_corr_cols, inplace=True)\n",
    "    \n",
    "    # 3. Calculate VIF\n",
    "    X_const = X.assign(const=1)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_const.values, i) for i in range(len(X.columns))]\n",
    "    })\n",
    "    \n",
    "    # 4. Sort by VIF\n",
    "    vif_data.sort_values(by=\"VIF\", ascending=False, inplace=True)\n",
    "    \n",
    "    # 5. Flag high VIF\n",
    "    vif_data[\"High_VIF\"] = vif_data[\"VIF\"] > vif_thresh\n",
    "    \n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209b52c",
   "metadata": {},
   "source": [
    "## Mendeley Delay Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e8fc9",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d437e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.041\n",
      "Test R²: 0.041\n",
      "Test RMSE: 34.423\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "linreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit\n",
    "linreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = linreg_pipe.predict(X_train)\n",
    "y_pred_test = linreg_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    'model': 'Linear Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3dd709",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0904961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.061\n",
      "Test R²: 0.060\n",
      "Test RMSE: 34.098\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Polynomial regression pipeline\n",
    "poly_reg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),                # your ColumnTransformer\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),  # expand features\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit\n",
    "poly_reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = poly_reg_pipe.predict(X_train)\n",
    "y_pred_test = poly_reg_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    'model': 'Polynomial Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd84e3",
   "metadata": {},
   "source": [
    "### VIF: Variable Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91cb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature           VIF  High_VIF\n",
      "4       nonhubairportorigin           inf      True\n",
      "5     smallhubairportorigin           inf      True\n",
      "14   mediumhubairlineorigin           inf      True\n",
      "15    largehubairlineorigin           inf      True\n",
      "7     largehubairportorigin           inf      True\n",
      "6    mediumhubairportorigin           inf      True\n",
      "8         nonhubairportdest           inf      True\n",
      "9       smallhubairportdest           inf      True\n",
      "11      largehubairportdest           inf      True\n",
      "10     mediumhubairportdest           inf      True\n",
      "12      nonhubairlineorigin           inf      True\n",
      "13    smallhubairlineorigin           inf      True\n",
      "18     mediumhubairlinedest           inf      True\n",
      "16        nonhubairlinedest  9.007199e+15      True\n",
      "17      smallhubairlinedest  9.007199e+15      True\n",
      "19      largehubairlinedest  3.002400e+15      True\n",
      "35               temp_20_30  3.112421e+02      True\n",
      "34               temp_10_20  2.888751e+02      True\n",
      "33                temp_0_10  1.916659e+02      True\n",
      "36               temp_30_40  1.151459e+02      True\n",
      "32               temp_n10_0  7.218545e+01      True\n",
      "30              temperature  1.428852e+01      True\n",
      "31          temp_ninfty_n10  1.244839e+01      True\n",
      "39          windspeedsquare  9.775125e+00     False\n",
      "38                windspeed  8.654598e+00     False\n",
      "37            temp_40_infty  5.987323e+00     False\n",
      "1           marketsharedest  3.613516e+00     False\n",
      "0         marketshareorigin  3.567442e+00     False\n",
      "20                     year  3.074945e+00     False\n",
      "41            windgustspeed  2.711904e+00     False\n",
      "27               numflights  2.588605e+00     False\n",
      "3                   hhidest  2.433162e+00     False\n",
      "2                 hhiorigin  2.371325e+00     False\n",
      "40            windgustdummy  2.178559e+00     False\n",
      "29            monopolyroute  1.597858e+00     False\n",
      "28                 distance  1.548539e+00     False\n",
      "47  originmetrogdppercapita  1.517796e+00     False\n",
      "26               loadfactor  1.450634e+00     False\n",
      "49    destmetrogdppercapita  1.449125e+00     False\n",
      "25                 capacity  1.331635e+00     False\n",
      "48             destmetropop  1.319521e+00     False\n",
      "46           originmetropop  1.319287e+00     False\n",
      "45           snowtracedummy  1.179233e+00     False\n",
      "23                dayofweek  1.165420e+00     False\n",
      "44                snowdummy  1.091662e+00     False\n",
      "24            scheduledhour  1.064159e+00     False\n",
      "21                    month  1.048153e+00     False\n",
      "42                raindummy  1.046193e+00     False\n",
      "43           raintracedummy  1.022820e+00     False\n",
      "22               dayofmonth  1.000520e+00     False\n"
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(df_mend_clean, features=X_mend.columns.tolist(), vif_thresh=10.0)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9f2ac",
   "metadata": {},
   "source": [
    "# Week 2 Notebook - Linear Regression 2\n",
    "\n",
    "For Week 2, include concepts such as linear regression with lasso, ridge, and elastic net regression. This homework will be submitted for peer review and feedback in Week 3 in the assignment titled 3.4 Peer Review: Week 2 Jupyter Notebook. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8fd697",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a18f",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0be88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.041\n",
      "Test R²: 0.041\n",
      "Test RMSE: 34.432\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Lasso\n",
    "lasso_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Lasso(alpha=0.1, max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lasso_pipe.predict(X_train)\n",
    "y_pred_test = lasso_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    'model': 'Lasso Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6ee55",
   "metadata": {},
   "source": [
    "### Lasso Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491616f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + Lasso\n",
    "lasso_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Lasso(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "grid = GridSearchCV(\n",
    "    lasso_pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best alpha:\", grid.best_params_[\"model__alpha\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    'model': 'Lasso Regression (Tuned)',\n",
    "    'best_alpha': grid.best_params_[\"model__alpha\"],\n",
    "    'train_r2': grid.best_score_,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6d279",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha): 0.04376642307060696\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Ridge\n",
    "ridge_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Ridge(alpha=1.0, max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = ridge_pipe.predict(X_train)\n",
    "y_pred_test = ridge_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    'model': 'Ridge Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de903e5",
   "metadata": {},
   "source": [
    "### Ridge Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19aaf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 1000\n",
      "Best CV R^2: 0.043972332094467756\n",
      "Test R^2: 0.04377132510993642\n"
     ]
    }
   ],
   "source": [
    "# Pipeline: preprocessing + Ridge\n",
    "ridge_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Ridge(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "ridge_param_grid = {\n",
    "    \"model__alpha\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge_pipe,\n",
    "    ridge_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Ridge alpha:\", ridge_grid.best_params_[\"model__alpha\"])\n",
    "\n",
    "y_pred_test = ridge_grid.predict(X_test)\n",
    "print(\"Ridge Test R²:\", r2_score(y_test, y_pred_test))\n",
    "print(\"Ridge Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    'model': 'Ridge Regression (Tuned)',\n",
    "    'best_alpha': ridge_grid.best_params_[\"model__alpha\"],\n",
    "    'train_r2': ridge_grid.best_score_,\n",
    "    'test_r2': r2_score(y_test, y_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c99e1",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha, l1_ratio): 0.033545260335402505\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Elastic Net\n",
    "elasticnet_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000, random_state=42))\n",
    "])\n",
    "# Fit\n",
    "elasticnet_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = elasticnet_pipe.predict(X_train)\n",
    "y_pred_test = elasticnet_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    'model': 'Elastic Net Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d70713",
   "metadata": {},
   "source": [
    "### Elastic Net Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a67925",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m param_grid = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.001\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m10\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__l1_ratio\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.1\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.9\u001b[39m]\n\u001b[32m      5\u001b[39m }\n\u001b[32m      7\u001b[39m grid = GridSearchCV(model, param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest CV R^2:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Pipeline: preprocessing + ElasticNet\n",
    "elastic_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", ElasticNet(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"model__l1_ratio\": [0.2, 0.5, 0.8]  # balance between L1 and L2\n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe,\n",
    "    elastic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "y_pred_test = elastic_grid.predict(X_test)\n",
    "print(\"ElasticNet Test R²:\", r2_score(y_test, y_pred_test))\n",
    "print(\"ElasticNet Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    'model': 'Elastic Net Regression (Tuned)',\n",
    "    'best_params': elastic_grid.best_params_,\n",
    "    'train_r2': elastic_grid.best_score_,\n",
    "    'test_r2': r2_score(y_test, y_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472fe2",
   "metadata": {},
   "source": [
    "# Week 3 Notebook - Linear Regression 3\n",
    "\n",
    "For Week 3, include concepts such as linear regression with forward and backward selection, PCR, and PLSR. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04655a18",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e762f",
   "metadata": {},
   "source": [
    "### Forward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908d8d7",
   "metadata": {},
   "source": [
    "#### Using SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47503042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['low_card__uniquecarrier_9E', 'low_card__uniquecarrier_B6', 'low_card__uniquecarrier_CO', 'low_card__uniquecarrier_DL', 'low_card__uniquecarrier_FL', 'low_card__uniquecarrier_MQ', 'low_card__uniquecarrier_UA', 'low_card__uniquecarrier_XE', 'high_card__dest', 'high_card__tailnum', 'high_card__origincityname', 'high_card__originstate', 'num__smallhubairportorigin', 'num__nonhubairportdest', 'num__largehubairportdest', 'num__year', 'num__month', 'num__dayofmonth', 'num__dayofweek', 'num__scheduledhour', 'num__loadfactor', 'num__numflights', 'num__temperature', 'num__temp_ninfty_n10', 'num__temp_n10_0', 'num__windspeedsquare', 'num__windgustdummy', 'num__raindummy', 'num__raintracedummy', 'num__snowdummy', 'num__snowtracedummy', 'num__destmetrogdppercapita']\n",
      "Model Accuracy (R²): -0.0439\n",
      "Model RMSE: 1372.6907\n"
     ]
    }
   ],
   "source": [
    "# Base pipeline: preprocessing + linear regression\n",
    "linreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Forward selection wrapper\n",
    "sfs = SFS(\n",
    "    linreg_pipe,\n",
    "    k_features=\"best\",                # keep adding until performance stops improving\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring=\"neg_mean_squared_error\", # optimize RMSE\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on raw features\n",
    "sfs = sfs.fit(X_mend, y_mend_numeric)\n",
    "\n",
    "# Extract CV scores (negative MSE → convert to RMSE)\n",
    "cv_scores = list(sfs.get_metric_dict().values())\n",
    "num_features = [d[\"feature_idx\"] for d in cv_scores]\n",
    "rmse_scores = [np.sqrt(-d[\"avg_score\"]) for d in cv_scores]\n",
    "\n",
    "# Plot improvement curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(rmse_scores)+1), rmse_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of Features Selected\")\n",
    "plt.ylabel(\"CV RMSE\")\n",
    "plt.title(\"Forward Selection Improvement Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Best subset\n",
    "print(\"Best number of features:\", sfs.k_features_)\n",
    "print(\"Selected features:\", sfs.k_feature_names_)\n",
    "print(\"Best CV RMSE:\", min(rmse_scores))\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    'model': 'Forward Selection Linear Regression',\n",
    "    'num_features': sfs.k_features_,\n",
    "    'selected_features': sfs.k_feature_names_,\n",
    "    'best_cv_rmse': min(rmse_scores)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adce31f",
   "metadata": {},
   "source": [
    "#### Using Stepwise Function\n",
    "\n",
    "This was abandoned due to performance issues. Relying on SFS instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe527202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add high_card__tailnum             with p-value 0.0\n",
      "Add num__scheduledhour             with p-value 1.07019e-156\n",
      "Add num__raindummy                 with p-value 7.40738e-87\n",
      "Add high_card__dest                with p-value 6.44347e-59\n",
      "Add high_card__origincityname      with p-value 6.17954e-47\n",
      "Add num__snowdummy                 with p-value 6.48087e-32\n",
      "Add num__snowtracedummy            with p-value 3.82627e-16\n",
      "Add num__numflights                with p-value 7.88944e-15\n",
      "Add num__windgustdummy             with p-value 1.34108e-14\n",
      "Add num__year                      with p-value 1.63149e-13\n",
      "Add num__largehubairportdest       with p-value 5.15059e-08\n",
      "Add num__raintracedummy            with p-value 3.68001e-07\n",
      "Add high_card__originstate         with p-value 4.61725e-06\n",
      "Add num__temp_n10_0                with p-value 5.76293e-05\n",
      "Add num__windspeedsquare           with p-value 0.000150945\n",
      "Add num__destmetrogdppercapita     with p-value 0.000336916\n",
      "Add num__dayofmonth                with p-value 0.00165799\n",
      "Add low_card__uniquecarrier_B6     with p-value 0.00200597\n",
      "Add num__temp_ninfty_n10           with p-value 0.00287058\n",
      "Add num__loadfactor                with p-value 0.00418016\n",
      "Forward-selected features: ['high_card__tailnum', 'num__scheduledhour', 'num__raindummy', 'high_card__dest', 'high_card__origincityname', 'num__snowdummy', 'num__snowtracedummy', 'num__numflights', 'num__windgustdummy', 'num__year', 'num__largehubairportdest', 'num__raintracedummy', 'high_card__originstate', 'num__temp_n10_0', 'num__windspeedsquare', 'num__destmetrogdppercapita', 'num__dayofmonth', 'low_card__uniquecarrier_B6', 'num__temp_ninfty_n10', 'num__loadfactor']\n",
      "Forward Selection Model Accuracy (R²): -0.0439\n",
      "Forward Selection Model RMSE: 1372.7035\n"
     ]
    }
   ],
   "source": [
    "# def forward_selection(X, y, threshold_in=0.01, verbose=True):\n",
    "#     \"\"\"Forward selection based on p-values from statsmodels OLS\"\"\"\n",
    "#     included = []\n",
    "#     while True:\n",
    "#         changed = False\n",
    "#         excluded = list(set(X.columns) - set(included))\n",
    "#         new_pval = pd.Series(index=excluded, dtype=float)\n",
    "#         for new_column in excluded:\n",
    "#             model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()\n",
    "#             new_pval[new_column] = model.pvalues[new_column]\n",
    "#         best_pval = new_pval.min() if not new_pval.empty else None\n",
    "#         if best_pval is not None and best_pval < threshold_in:\n",
    "#             best_feature = new_pval.idxmin()\n",
    "#             included.append(best_feature)\n",
    "#             changed = True\n",
    "#             if verbose:\n",
    "#                 print(f'Add {best_feature:30} with p-value {best_pval:.6}')\n",
    "#         if not changed:\n",
    "#             break\n",
    "#     return included\n",
    "# # Transform train/test into numeric DataFrames\n",
    "# preprocessor.fit(X_train, y_train)\n",
    "\n",
    "# X_train_trans = pd.DataFrame(\n",
    "#     preprocessor.transform(X_train),\n",
    "#     columns=preprocessor.get_feature_names_out(),\n",
    "#     index=X_train.index\n",
    "# )\n",
    "# X_test_trans = pd.DataFrame(\n",
    "#     preprocessor.transform(X_test),\n",
    "#     columns=preprocessor.get_feature_names_out(),\n",
    "#     index=X_test.index\n",
    "# )\n",
    "\n",
    "# # Run forward selection\n",
    "# forward_features = forward_selection(X_train_trans, y_train)\n",
    "# print(\"Forward-selected features:\", forward_features)\n",
    "\n",
    "# # Fit final model\n",
    "# final_model = sm.OLS(y_train, sm.add_constant(X_train_trans[forward_features])).fit()\n",
    "\n",
    "# # Predict on test set\n",
    "# y_pred = final_model.predict(sm.add_constant(X_test_trans[forward_features]))\n",
    "\n",
    "# # Evaluate accuracy\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print(f\"Forward Selection Model Accuracy (R²): {r2:.4f}\")\n",
    "# print(f\"Forward Selection Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb90c3",
   "metadata": {},
   "source": [
    "### Backward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e30443e",
   "metadata": {},
   "source": [
    "#### Using SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83900a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward-selected features: ['low_card__uniquecarrier_AS', 'low_card__uniquecarrier_B6', 'low_card__uniquecarrier_CO', 'low_card__uniquecarrier_OO', 'low_card__uniquecarrier_US', 'low_card__uniquecarrier_WN', 'low_card__uniquecarrier_XE', 'high_card__dest', 'high_card__tailnum', 'high_card__origincityname', 'high_card__originstate', 'num__smallhubairportorigin', 'num__mediumhubairportdest', 'num__largehubairportdest', 'num__year', 'num__month', 'num__dayofmonth', 'num__dayofweek', 'num__scheduledhour', 'num__loadfactor', 'num__numflights', 'num__temperature', 'num__temp_0_10', 'num__temp_10_20', 'num__temp_20_30', 'num__windspeedsquare', 'num__windgustdummy', 'num__raindummy', 'num__raintracedummy', 'num__snowdummy', 'num__snowtracedummy', 'num__destmetrogdppercapita']\n",
      "Backward Selection Model Accuracy (R²): -0.0439\n",
      "Backward Selection Model RMSE: 1372.7010\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Base pipeline: preprocessing + linear regression\n",
    "linreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Backward selection wrapper\n",
    "sbs = SFS(\n",
    "    linreg_pipe,\n",
    "    k_features=\"best\",                # keep removing until optimal subset\n",
    "    forward=False,                    # backward elimination\n",
    "    floating=False,\n",
    "    scoring=\"neg_mean_squared_error\", # optimize RMSE\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on raw features\n",
    "sbs = sbs.fit(X_mend, y_mend_numeric)\n",
    "\n",
    "# Extract CV scores (negative MSE → RMSE)\n",
    "cv_scores = list(sbs.get_metric_dict().values())\n",
    "rmse_scores = [np.sqrt(-d[\"avg_score\"]) for d in cv_scores]\n",
    "\n",
    "# Plot improvement curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(len(rmse_scores), 0, -1), rmse_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of Features Remaining\")\n",
    "plt.ylabel(\"CV RMSE\")\n",
    "plt.title(\"Backward Selection Improvement Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Best subset\n",
    "print(\"Best number of features:\", sbs.k_features_)\n",
    "print(\"Selected features:\", sbs.k_feature_names_)\n",
    "print(\"Best CV RMSE:\", min(rmse_scores))\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    'model': 'Backward Selection Linear Regression',\n",
    "    'num_features': sbs.k_features_,\n",
    "    'selected_features': sbs.k_feature_names_,\n",
    "    'best_cv_rmse': min(rmse_scores)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980ce9c",
   "metadata": {},
   "source": [
    "#### Using Stepwise\n",
    "\n",
    "Abandoned due to performance issues. Relying on SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedfe564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop num__distance                  with p-value 0.994829\n",
      "Drop num__largehubairlinedest       with p-value 0.966156\n",
      "Drop num__nonhubairlinedest         with p-value 0.999495\n",
      "Drop num__originmetrogdppercapita   with p-value 0.938646\n",
      "Drop num__hhidest                   with p-value 0.934772\n",
      "Drop num__windgustspeed             with p-value 0.929315\n",
      "Drop num__mediumhubairportorigin    with p-value 0.920231\n",
      "Drop high_card__origin              with p-value 0.87493\n",
      "Drop low_card__uniquecarrier_WN     with p-value 0.852824\n",
      "Drop low_card__uniquecarrier_F9     with p-value 0.897424\n",
      "Drop low_card__uniquecarrier_US     with p-value 0.85234\n",
      "Drop num__mediumhubairlinedest      with p-value 0.833208\n",
      "Drop num__nonhubairportorigin       with p-value 0.828218\n",
      "Drop low_card__uniquecarrier_YV     with p-value 0.812314\n",
      "Drop num__hhiorigin                 with p-value 0.666242\n",
      "Drop num__marketshareorigin         with p-value 0.838139\n",
      "Drop low_card__uniquecarrier_AS     with p-value 0.678525\n",
      "Drop num__capacity                  with p-value 0.670199\n",
      "Drop num__smallhubairlineorigin     with p-value 0.637363\n",
      "Drop num__mediumhubairlineorigin    with p-value 0.996565\n",
      "Drop num__largehubairlineorigin     with p-value 0.819745\n",
      "Drop low_card__uniquecarrier_AA     with p-value 0.574367\n",
      "Drop num__destmetropop              with p-value 0.582364\n",
      "Drop num__mediumhubairportdest      with p-value 0.583266\n",
      "Drop num__smallhubairportdest       with p-value 0.577486\n",
      "Drop num__smallhubairlinedest       with p-value 0.551636\n",
      "Drop low_card__uniquecarrier_HA     with p-value 0.545089\n",
      "Drop low_card__uniquecarrier_MQ     with p-value 0.524251\n",
      "Drop low_card__uniquecarrier_OH     with p-value 0.509704\n",
      "Drop num__monopolyroute             with p-value 0.437354\n",
      "Drop num__windspeed                 with p-value 0.338178\n",
      "Drop low_card__uniquecarrier_EV     with p-value 0.303229\n",
      "Drop low_card__uniquecarrier_NW     with p-value 0.299041\n",
      "Drop num__largehubairportorigin     with p-value 0.311789\n",
      "Drop num__nonhubairlineorigin       with p-value 0.441396\n",
      "Drop low_card__uniquecarrier_FL     with p-value 0.301925\n",
      "Drop low_card__uniquecarrier_DL     with p-value 0.356621\n",
      "Drop num__originmetropop            with p-value 0.253863\n",
      "Drop num__temp_40_infty             with p-value 0.15899\n",
      "Drop num__temp_ninfty_n10           with p-value 0.500802\n",
      "Drop num__temp_30_40                with p-value 0.407732\n",
      "Drop num__temp_n10_0                with p-value 0.546831\n",
      "Drop num__marketsharedest           with p-value 0.119029\n",
      "Drop num__nonhubairportdest         with p-value 0.104067\n",
      "Drop low_card__uniquecarrier_OO     with p-value 0.112514\n",
      "Drop num__month                     with p-value 0.0817786\n",
      "Drop num__smallhubairportorigin     with p-value 0.0553531\n",
      "Backward-selected features: ['low_card__uniquecarrier_9E', 'low_card__uniquecarrier_B6', 'low_card__uniquecarrier_CO', 'low_card__uniquecarrier_UA', 'low_card__uniquecarrier_XE', 'high_card__dest', 'high_card__tailnum', 'high_card__origincityname', 'high_card__originstate', 'num__largehubairportdest', 'num__year', 'num__dayofmonth', 'num__dayofweek', 'num__scheduledhour', 'num__loadfactor', 'num__numflights', 'num__temperature', 'num__temp_0_10', 'num__temp_10_20', 'num__temp_20_30', 'num__windspeedsquare', 'num__windgustdummy', 'num__raindummy', 'num__raintracedummy', 'num__snowdummy', 'num__snowtracedummy', 'num__destmetrogdppercapita']\n",
      "Backward Elimination Model Accuracy (R²): -0.0440\n",
      "Backward Elimination Model RMSE: 1372.8699\n"
     ]
    }
   ],
   "source": [
    "# def backward_elimination(X, y, threshold_out=0.05, verbose=True):\n",
    "#     \"\"\"Backward elimination based on p-values from statsmodels OLS.\n",
    "#        Assumes X is a numeric DataFrame (already preprocessed).\"\"\"\n",
    "#     included = list(X.columns)\n",
    "#     while True:\n",
    "#         changed = False\n",
    "#         model = sm.OLS(y, sm.add_constant(X[included])).fit()\n",
    "#         # exclude intercept\n",
    "#         pvalues = model.pvalues.iloc[1:]\n",
    "#         worst_pval = pvalues.max() if not pvalues.empty else None\n",
    "#         if worst_pval is not None and worst_pval > threshold_out:\n",
    "#             worst_feature = pvalues.idxmax()\n",
    "#             included.remove(worst_feature)\n",
    "#             changed = True\n",
    "#             if verbose:\n",
    "#                 print(f'Drop {worst_feature:30} with p-value {worst_pval:.6}')\n",
    "#         if not changed:\n",
    "#             break\n",
    "#     return included\n",
    "\n",
    "# # --- Transform train/test into numeric DataFrames ---\n",
    "# preprocessor.fit(X_train, y_train)\n",
    "\n",
    "# X_train_trans = pd.DataFrame(\n",
    "#     preprocessor.transform(X_train),\n",
    "#     columns=preprocessor.get_feature_names_out(),\n",
    "#     index=X_train.index\n",
    "# )\n",
    "# X_test_trans = pd.DataFrame(\n",
    "#     preprocessor.transform(X_test),\n",
    "#     columns=preprocessor.get_feature_names_out(),\n",
    "#     index=X_test.index\n",
    "# )\n",
    "\n",
    "# # --- Run backward elimination on transformed data ---\n",
    "# backward_features = backward_elimination(X_train_trans, y_train)\n",
    "# print(\"Backward-selected features:\", backward_features)\n",
    "\n",
    "# # --- Fit final model on selected features ---\n",
    "# final_model = sm.OLS(y_train, sm.add_constant(X_train_trans[backward_features])).fit()\n",
    "\n",
    "# # Predict on test set\n",
    "# y_pred = final_model.predict(sm.add_constant(X_test_trans[backward_features]))\n",
    "\n",
    "# # --- Evaluate accuracy ---\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# print(f\"Backward Elimination Model Accuracy (R²): {r2:.4f}\")\n",
    "# print(f\"Backward Elimination Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0461f2",
   "metadata": {},
   "source": [
    "### PCR\n",
    "Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a7788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCR R2: -0.07829785712740689\n",
      "PCR RMSE: 1417.9651431890868\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# PCR pipeline: preprocessing → PCA → Linear Regression\n",
    "pcr_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),   # your ColumnTransformer\n",
    "    (\"pca\", PCA()),                    # dimensionality reduction\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Grid search over number of components\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [5, 10, 20, 40, 60]  # tune based on dataset size\n",
    "}\n",
    "\n",
    "pcr_grid = GridSearchCV(\n",
    "    pcr_pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "pcr_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best number of components\n",
    "print(\"Best n_components:\", pcr_grid.best_params_[\"pca__n_components\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = pcr_grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"PCR Test R²: {test_r2:.3f}\")\n",
    "print(f\"PCR Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "results.append({\n",
    "    \"Model\": \"PCR\",\n",
    "    \"Best Params\": pcr_grid.best_params_,\n",
    "    \"Test R²\": test_r2,\n",
    "    \"Test RMSE\": test_rmse\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a9361",
   "metadata": {},
   "source": [
    "### PLSR\n",
    "Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02303914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSR R2: -0.04399321509843657\n",
      "PLSR RMSE: 1372.854428811666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# PLSR pipeline: preprocessing → PLSRegression\n",
    "pls_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),   # your ColumnTransformer\n",
    "    (\"model\", PLSRegression())\n",
    "])\n",
    "\n",
    "# Grid search over number of components\n",
    "param_grid = {\n",
    "    \"model__n_components\": [2, 5, 10, 20, 40]  # tune based on dataset size\n",
    "}\n",
    "\n",
    "pls_grid = GridSearchCV(\n",
    "    pls_pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "pls_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best number of components\n",
    "print(\"Best n_components:\", pls_grid.best_params_[\"model__n_components\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = pls_grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"PLSR Test R²: {test_r2:.3f}\")\n",
    "print(f\"PLSR Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "results.append({\n",
    "    \"Model\": \"PLSR\",\n",
    "    \"Best Params\": pls_grid.best_params_,\n",
    "    \"Test R²\": test_r2,\n",
    "    \"Test RMSE\": test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd613b8",
   "metadata": {},
   "source": [
    "# Week 4 Notebook - Logistic Regression and Feature Scaling\n",
    "\n",
    "For Week 4, include concepts such as logistic regression and feature scaling. This homework should be submitted for peer review in the assignment titled 4.3 Peer Review: Week 4 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501ecd8",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526a100",
   "metadata": {},
   "source": [
    "### Log Regression: Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results\n",
      "Accuracy: 0.6042\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.63      0.66      6088\n",
      "           1       0.49      0.57      0.53      3912\n",
      "\n",
      "    accuracy                           0.60     10000\n",
      "   macro avg       0.59      0.60      0.59     10000\n",
      "weighted avg       0.62      0.60      0.61     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_binary_15, test_size=0.2, random_state=42, stratify=y_mend_binary_15\n",
    ")\n",
    "\n",
    "# Build pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = logreg_pipe.predict(X_train)\n",
    "y_pred_test = logreg_pipe.predict(X_test)\n",
    "y_pred_proba = logreg_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    \"Model\": \"Logistic Regression\",\n",
    "    \"Train Accuracy\": train_acc,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test AUC\": test_auc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40f62e",
   "metadata": {},
   "source": [
    "### Log Regression: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab6b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'classifier__C': np.float64(0.008632008168602538), 'classifier__class_weight': 'balanced', 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
      "Best CV Score: 0.5688355837791884\n",
      "\n",
      "Random Search Logistic Regression Results\n",
      "Accuracy: 0.6279\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.68      6088\n",
      "           1       0.52      0.61      0.56      3912\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.62      0.62      0.62     10000\n",
      "weighted avg       0.64      0.63      0.63     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_binary_15, test_size=0.2, random_state=42, stratify=y_mend_binary_15\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, solver=\"saga\", penalty=\"l1\"))  \n",
    "    # saga supports both L1 and L2\n",
    "])\n",
    "\n",
    "# Parameter distributions for random search\n",
    "param_distributions = {\n",
    "    \"model__C\": loguniform(1e-3, 1e3),   # inverse regularization strength\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],      # try both penalties\n",
    "    \"model__solver\": [\"saga\"]            # saga works with both l1 and l2\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,                # number of random samples\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = random_search.predict(X_test)\n",
    "y_pred_proba = random_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    \"Model\": \"Logistic Regression (Random Search)\",\n",
    "    \"Best Params\": random_search.best_params_,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test AUC\": test_auc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36167c8",
   "metadata": {},
   "source": [
    "### Log Regession: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_binary_15, test_size=0.2, random_state=42, stratify=y_mend_binary_15\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, solver=\"saga\"))  \n",
    "    # saga supports both L1 and L2\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"model__C\": [0.001, 0.01, 0.1, 1, 10, 100],   # regularization strength\n",
    "    \"model__penalty\": [\"l1\", \"l2\"]                # L1 (Lasso) or L2 (Ridge)\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test)\n",
    "y_pred_proba = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    \"Model\": \"Logistic Regression (Grid Search)\",\n",
    "    \"Best Params\": grid.best_params_,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test AUC\": test_auc\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13b062",
   "metadata": {},
   "source": [
    "# Week 5 - Support Vector Machines\n",
    "\n",
    "For Week 5, include concepts such as support vector machines, the kernel trick, and regularization for support vector machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5fb28",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1e8e1",
   "metadata": {},
   "source": [
    "### SVM Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_binary_15, test_size=0.2, random_state=42, stratify=y_mend_binary_15\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + SVM (RBF kernel by default)\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_test = svm_pipe.predict(X_test)\n",
    "y_pred_proba = svm_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Basic SVM Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Basic SVM AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    \"Model\": \"SVM\",\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7213d9",
   "metadata": {},
   "source": [
    "### SVM Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dea8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline: preprocessing + SVM\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter distributions\n",
    "param_distributions = {\n",
    "    \"model__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"model__C\": loguniform(1e-3, 1e3),   # regularization strength\n",
    "    \"model__gamma\": [\"scale\", \"auto\", 0.01, 0.1, 1]  # only relevant for RBF\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "svm_random = RandomizedSearchCV(\n",
    "    svm_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,                # number of random samples\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "svm_random.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Params:\", svm_random.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = svm_random.predict(X_test)\n",
    "y_pred_proba = svm_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Search SVM Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Search SVM AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "#store results\n",
    "results.append({\n",
    "    \"Model\": \"SVM (Random Search)\",\n",
    "    \"Best Params\": svm_random.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf128055",
   "metadata": {},
   "source": [
    "### Kernel Trick: Linear vs. RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_binary_15, test_size=0.2, random_state=42, stratify=y_mend_binary_15\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + SVM\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters (RBF only)\n",
    "param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"model__gamma\": [0.001, 0.01, 0.1, 1, \"scale\"]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Collect results into DataFrame\n",
    "cv_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "\n",
    "# Pivot table for heatmap (mean test AUC)\n",
    "heatmap_data = cv_results.pivot(\n",
    "    index=\"param_model__C\",\n",
    "    columns=\"param_model__gamma\",\n",
    "    values=\"mean_test_score\"\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "plt.title(\"SVM RBF Kernel: AUC across C and gamma\")\n",
    "plt.ylabel(\"C (Regularization)\")\n",
    "plt.xlabel(\"Gamma (Kernel Width)\")\n",
    "plt.show()\n",
    "\n",
    "# Best params + score\n",
    "print(\"Best Params:\", svm_grid.best_params_)\n",
    "print(\"Best CV AUC:\", svm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de485de",
   "metadata": {},
   "source": [
    "# Week 6 - Decision Trees and Random Forests \n",
    "\n",
    "For Week 6, include concepts such as decision trees and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08548c",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2455e",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ce3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_binary_15, test_size=0.2, random_state=42, stratify=y_mend_binary_15\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + Decision Tree\n",
    "dt_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "dt_param_grid = {\n",
    "    \"model__max_depth\": [3, 5, 10, None],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "dt_grid = GridSearchCV(\n",
    "    dt_pipe,\n",
    "    dt_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_test = dt_grid.predict(X_test)\n",
    "y_pred_proba = dt_grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Decision Tree Best Params:\", dt_grid.best_params_)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Decision Tree AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    \"Model\": \"Decision Tree\",\n",
    "    \"Best Params\": dt_grid.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdb7f7",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980599c",
   "metadata": {},
   "source": [
    "#### RF w/ Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline: preprocessing + Random Forest\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "rf_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__max_depth\": [5, 10, None],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe,\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_test = rf_grid.predict(X_test)\n",
    "y_pred_proba = rf_grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Best Params:\", rf_grid.best_params_)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    \"Model\": \"Random Forest\",\n",
    "    \"Best Params\": rf_grid.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})\n",
    "\n",
    "# Optional: view results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2744387",
   "metadata": {},
   "source": [
    "#### RF Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bf815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "# reg_prep_mend is your ColumnTransformer\n",
    "feature_names = reg_prep_mend.get_feature_names_out()\n",
    "\n",
    "# Extract feature importances from the best Random Forest\n",
    "best_rf = rf_grid.best_estimator_.named_steps[\"model\"]\n",
    "importances = best_rf.feature_importances_\n",
    "\n",
    "# Put into DataFrame for sorting\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_imp_df.head(20), palette=\"viridis\")\n",
    "plt.title(\"Random Forest Feature Importances (Top 20)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(feat_imp_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a890e26",
   "metadata": {},
   "source": [
    "# Week 7 - Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a45a13",
   "metadata": {},
   "source": [
    "## Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of dicts into DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by test_r2 (descending) or test_rmse (ascending)\n",
    "results_df_sorted = results_df.sort_values(by=\"test_r2\", ascending=False)\n",
    "\n",
    "print(results_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=results_df, x=\"model\", y=\"test_r2\", palette=\"viridis\")\n",
    "plt.title(\"Model Comparison (Test R²)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=results_df, x=\"model\", y=\"test_rmse\", palette=\"magma\")\n",
    "plt.title(\"Model Comparison (Test RMSE)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87f20f",
   "metadata": {},
   "source": [
    "# Scrapyard\n",
    "\n",
    "Previously completed work no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eac970",
   "metadata": {},
   "source": [
    "## Week 1: USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8269e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_19324\\508701689.py:5: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_19324\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_19324\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_19324\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_19324\\508701689.py:5: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456aeb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 3779.75 MB\n",
      "Memory usage after optimization: 535.69 MB\n",
      "Reduced by 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ba988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n",
      "R^2 score: 0.06941058350132379\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "usdot_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "usdot_model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", usdot_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f1d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping constant columns: ['year', 'flights']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m vif_table \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_vif\u001b[49m\u001b[43m(\u001b[49m\u001b[43musdot_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(vif_table)\n",
      "\n",
      "Cell \u001b[1;32mIn[11], line 59\u001b[0m, in \u001b[0;36mcalculate_vif\u001b[1;34m(df, features, vif_thresh)\u001b[0m\n",
      "\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 3. Calculate VIF\u001b[39;00m\n",
      "\u001b[0;32m     56\u001b[0m X_const \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39massign(const\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;32m     57\u001b[0m vif_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n",
      "\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: X\u001b[38;5;241m.\u001b[39mcolumns,\n",
      "\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43m[\u001b[49m\u001b[43mvariance_inflation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_const\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;32m     60\u001b[0m })\n",
      "\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 4. Sort by VIF\u001b[39;00m\n",
      "\u001b[0;32m     63\u001b[0m vif_data\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "Cell \u001b[1;32mIn[11], line 59\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 3. Calculate VIF\u001b[39;00m\n",
      "\u001b[0;32m     56\u001b[0m X_const \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39massign(const\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;32m     57\u001b[0m vif_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n",
      "\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: X\u001b[38;5;241m.\u001b[39mcolumns,\n",
      "\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43mvariance_inflation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_const\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))]\n",
      "\u001b[0;32m     60\u001b[0m })\n",
      "\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 4. Sort by VIF\u001b[39;00m\n",
      "\u001b[0;32m     63\u001b[0m vif_data\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:196\u001b[0m, in \u001b[0;36mvariance_inflation_factor\u001b[1;34m(exog, exog_idx)\u001b[0m\n",
      "\u001b[0;32m    194\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(k_vars) \u001b[38;5;241m!=\u001b[39m exog_idx\n",
      "\u001b[0;32m    195\u001b[0m x_noti \u001b[38;5;241m=\u001b[39m exog[:, mask]\n",
      "\u001b[1;32m--> 196\u001b[0m r_squared_i \u001b[38;5;241m=\u001b[39m \u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_noti\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrsquared\n",
      "\u001b[0;32m    197\u001b[0m vif \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m r_squared_i)\n",
      "\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vif\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:333\u001b[0m, in \u001b[0;36mRegressionModel.fit\u001b[1;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpinv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[0;32m    330\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_cov_params\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n",
      "\u001b[0;32m    331\u001b[0m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n",
      "\u001b[1;32m--> 333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, singular_values \u001b[38;5;241m=\u001b[39m \u001b[43mpinv_extended\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwexog\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_cov_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\n",
      "\u001b[0;32m    335\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog, np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog))\n",
      "\u001b[0;32m    337\u001b[0m         \u001b[38;5;66;03m# Cache these singular values for use later.\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\tools\\tools.py:264\u001b[0m, in \u001b[0;36mpinv_extended\u001b[1;34m(x, rcond)\u001b[0m\n",
      "\u001b[0;32m    262\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n",
      "\u001b[0;32m    263\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mconjugate()\n",
      "\u001b[1;32m--> 264\u001b[0m u, s, vt \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    265\u001b[0m s_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(s)\n",
      "\u001b[0;32m    266\u001b[0m m \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:1862\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n",
      "\u001b[0;32m   1858\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_svd_nonconvergence,\n",
      "\u001b[0;32m   1860\u001b[0m               invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m, over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n",
      "\u001b[0;32m   1861\u001b[0m               under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;32m-> 1862\u001b[0m     u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1863\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;32m   1864\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(usdot_df)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a4c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: df\n",
      "Dropped DataFrame: df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n",
      "Dropped DataFrame: combined_df\n",
      "Dropped DataFrame: usdot_df\n",
      "Dropped DataFrame: usdot_df_lin\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6316f",
   "metadata": {},
   "source": [
    "## Week 2: USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ffa656",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n",
      "Memory usage before optimization: 3779.75 MB\n",
      "Memory usage after optimization: 535.69 MB\n",
      "Reduced by 85.8%\n",
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34155af",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.04595741706259315\n"
     ]
    }
   ],
   "source": [
    "# Default Lasso \n",
    "\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lasso)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a497a8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n",
      "\u001b[32m      2\u001b[39m param_grid = {\n",
      "\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.001\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m]\n",
      "\u001b[32m      4\u001b[39m }\n",
      "\u001b[32m      6\u001b[39m grid = GridSearchCV(model, param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest alpha:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_[\u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest CV R^2:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_score_)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1358\u001b[39m     estimator._validate_params()\n",
      "\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n",
      "\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n",
      "\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n",
      "\u001b[32m   1363\u001b[39m     )\n",
      "\u001b[32m   1364\u001b[39m ):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n",
      "\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n",
      "\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n",
      "\u001b[32m   1047\u001b[39m     )\n",
      "\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n",
      "\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n",
      "\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n",
      "\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n",
      "\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n",
      "\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n",
      "\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n",
      "\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n",
      "\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n",
      "\u001b[32m    994\u001b[39m         )\n",
      "\u001b[32m    995\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n",
      "\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1020\u001b[39m     )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n",
      "\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n",
      "\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n",
      "\u001b[32m     75\u001b[39m     (\n",
      "\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n",
      "\u001b[32m     81\u001b[39m )\n",
      "\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n",
      "\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n",
      "\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n",
      "\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n",
      "\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n",
      "\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n",
      "\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n",
      "\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n",
      "\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n",
      "\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n",
      "\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n",
      "\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n",
      "\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n",
      "\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n",
      "\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n",
      "\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n",
      "\u001b[32m   1799\u001b[39m     ):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n",
      "\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n",
      "\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n",
      "\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n",
      "\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Lasso with tuned alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a05178",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha): 0.06941067445046167\n"
     ]
    }
   ],
   "source": [
    "#Ridge with default alpha\n",
    "\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ridge)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default alpha\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha):\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1533b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with tune alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19407c4",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de4aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha, l1_ratio): 0.04155075820659526\n"
     ]
    }
   ],
   "source": [
    "# Replace regressor with ElasticNet\n",
    "elastic = ElasticNet(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', elastic)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default parameters\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha, l1_ratio):\", model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Tune alpha and l1_ratio with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: combined_df\n",
      "Dropped DataFrame: usdot_df\n",
      "Dropped DataFrame: usdot_df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c5f4a",
   "metadata": {},
   "source": [
    "## Week 3: USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e41b6a",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff3783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n",
      "Memory usage before optimization: 3779.75 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\3367681747.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization: 552.31 MB\n",
      "Reduced by 85.4%\n",
      "Sampling the dataset to 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  usdot_df.groupby('depdelay_bin', group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "SAMPLE_SIZE = 50000\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "combined_df.columns = combined_df.columns.str.lower()\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)\n",
    "\n",
    "if SAMPLE_SIZE >= 0:\n",
    "    print(\"Sampling the dataset to\", SAMPLE_SIZE)\n",
    "    bins = [-np.inf, -1, 0, 15, 60, 180, np.inf]\n",
    "    labels = ['early', 'on_time', 'small_delay', 'moderate_delay', 'long_delay', 'extreme_delay']\n",
    "\n",
    "    usdot_df['depdelay_bin'] = pd.cut(usdot_df['dep_delay'], bins=bins, labels=labels)\n",
    "    usdot_df = (\n",
    "        usdot_df.groupby('depdelay_bin', group_keys=False)\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=min(int(SAMPLE_SIZE * len(x) / len(usdot_df)), len(x)), \n",
    "            random_state=42\n",
    "        ))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    usdot_df = usdot_df.drop(columns=['depdelay_bin'])\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f4604",
   "metadata": {},
   "source": [
    "### Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e068ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 out of 107 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 14:42:18] Features: 1/107 -- score: 0.14339819239562326[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 out of 106 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 14:42:18] Features: 2/107 -- score: 0.15745076680487888[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  74 out of 105 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:    0.3s finished\n",
      "\n",
      "[2025-09-28 14:42:19] Features: 3/107 -- score: 0.1637377783872315[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 104 | elapsed:    0.4s finished\n",
      "\n",
      "[2025-09-28 14:42:20] Features: 4/107 -- score: 0.16931774650750123[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of 103 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 103 out of 103 | elapsed:    0.4s finished\n",
      "\n",
      "[2025-09-28 14:42:20] Features: 5/107 -- score: 0.17181008494434805[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 102 out of 102 | elapsed:    0.5s finished\n",
      "\n",
      "[2025-09-28 14:42:21] Features: 6/107 -- score: 0.173528548453462[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of 101 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:    0.5s finished\n",
      "\n",
      "[2025-09-28 14:42:22] Features: 7/107 -- score: 0.17489180135475366[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "\n",
      "[2025-09-28 14:42:23] Features: 8/107 -- score: 0.17614373064544017[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  99 | elapsed:    0.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  99 out of  99 | elapsed:    0.7s finished\n",
      "\n",
      "[2025-09-28 14:42:23] Features: 9/107 -- score: 0.1770294126956441[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  98 out of  98 | elapsed:    0.7s finished\n",
      "\n",
      "[2025-09-28 14:42:24] Features: 10/107 -- score: 0.1778421002218102[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  97 | elapsed:    0.6s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  97 out of  97 | elapsed:    0.8s finished\n",
      "\n",
      "[2025-09-28 14:42:25] Features: 11/107 -- score: 0.17863027688559333[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:    0.9s finished\n",
      "\n",
      "[2025-09-28 14:42:27] Features: 12/107 -- score: 0.17935142037678467[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  95 | elapsed:    0.7s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  95 out of  95 | elapsed:    1.0s finished\n",
      "\n",
      "[2025-09-28 14:42:28] Features: 13/107 -- score: 0.18000184972448852[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  94 | elapsed:    0.7s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  94 out of  94 | elapsed:    1.0s finished\n",
      "\n",
      "[2025-09-28 14:42:29] Features: 14/107 -- score: 0.18045163479743365[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of  93 | elapsed:    1.1s finished\n",
      "\n",
      "[2025-09-28 14:42:30] Features: 15/107 -- score: 0.18076390323995964[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of  92 | elapsed:    1.2s finished\n",
      "\n",
      "[2025-09-28 14:42:32] Features: 16/107 -- score: 0.18105903961729564[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  91 | elapsed:    0.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  91 out of  91 | elapsed:    1.2s finished\n",
      "\n",
      "[2025-09-28 14:42:33] Features: 17/107 -- score: 0.18133191447235533[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    1.3s finished\n",
      "\n",
      "[2025-09-28 14:42:35] Features: 18/107 -- score: 0.18159770191334768[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  89 out of  89 | elapsed:    1.4s finished\n",
      "\n",
      "[2025-09-28 14:42:36] Features: 19/107 -- score: 0.18185296033874898[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  88 out of  88 | elapsed:    1.4s finished\n",
      "\n",
      "[2025-09-28 14:42:38] Features: 20/107 -- score: 0.1820462482122069[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  87 out of  87 | elapsed:    1.5s finished\n",
      "\n",
      "[2025-09-28 14:42:40] Features: 21/107 -- score: 0.1822449298180305[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  86 out of  86 | elapsed:    1.5s finished\n",
      "\n",
      "[2025-09-28 14:42:41] Features: 22/107 -- score: 0.182457142663404[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  85 | elapsed:    1.6s finished\n",
      "\n",
      "[2025-09-28 14:42:43] Features: 23/107 -- score: 0.18271812298850687[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed:    1.6s finished\n",
      "\n",
      "[2025-09-28 14:42:45] Features: 24/107 -- score: 0.18303714073115676[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  83 out of  83 | elapsed:    1.7s finished\n",
      "\n",
      "[2025-09-28 14:42:47] Features: 25/107 -- score: 0.18348973820138859[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  82 out of  82 | elapsed:    1.7s finished\n",
      "\n",
      "[2025-09-28 14:42:49] Features: 26/107 -- score: 0.18366088569604003[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    1.9s finished\n",
      "\n",
      "[2025-09-28 14:42:51] Features: 27/107 -- score: 0.1838053004187663[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    1.8s finished\n",
      "\n",
      "[2025-09-28 14:42:53] Features: 28/107 -- score: 0.18393029278801853[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  79 out of  79 | elapsed:    2.0s finished\n",
      "\n",
      "[2025-09-28 14:42:55] Features: 29/107 -- score: 0.18415340358757892[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  78 out of  78 | elapsed:    2.1s finished\n",
      "\n",
      "[2025-09-28 14:42:58] Features: 30/107 -- score: 0.18458365690942513[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  77 out of  77 | elapsed:    2.2s finished\n",
      "\n",
      "[2025-09-28 14:43:00] Features: 31/107 -- score: 0.1848443920416316[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  76 out of  76 | elapsed:    2.3s finished\n",
      "\n",
      "[2025-09-28 14:43:03] Features: 32/107 -- score: 0.18496062032265323[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    2.4s finished\n",
      "\n",
      "[2025-09-28 14:43:05] Features: 33/107 -- score: 0.185077784818746[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  74 out of  74 | elapsed:    2.4s finished\n",
      "\n",
      "[2025-09-28 14:43:08] Features: 34/107 -- score: 0.18518645322199925[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  73 out of  73 | elapsed:    2.7s finished\n",
      "\n",
      "[2025-09-28 14:43:11] Features: 35/107 -- score: 0.1852917145510241[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    2.6s finished\n",
      "\n",
      "[2025-09-28 14:43:14] Features: 36/107 -- score: 0.185391919005793[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of  71 | elapsed:    3.0s finished\n",
      "\n",
      "[2025-09-28 14:43:17] Features: 37/107 -- score: 0.18547070249011965[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    3.2s finished\n",
      "\n",
      "[2025-09-28 14:43:20] Features: 38/107 -- score: 0.18554626139127128[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  69 | elapsed:    3.3s finished\n",
      "\n",
      "[2025-09-28 14:43:24] Features: 39/107 -- score: 0.1856183870226886[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  68 | elapsed:    3.4s finished\n",
      "\n",
      "[2025-09-28 14:43:27] Features: 40/107 -- score: 0.18567991850034515[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  67 out of  67 | elapsed:    3.7s finished\n",
      "\n",
      "[2025-09-28 14:43:31] Features: 41/107 -- score: 0.18572281087640333[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  66 | elapsed:    3.7s finished\n",
      "\n",
      "[2025-09-28 14:43:35] Features: 42/107 -- score: 0.1857581012364143[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:    4.1s finished\n",
      "\n",
      "[2025-09-28 14:43:39] Features: 43/107 -- score: 0.1857892053160228[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:    4.1s finished\n",
      "\n",
      "[2025-09-28 14:43:44] Features: 44/107 -- score: 0.18585033977537768[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed:    4.5s finished\n",
      "\n",
      "[2025-09-28 14:43:48] Features: 45/107 -- score: 0.1859635771945406[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  62 out of  62 | elapsed:    4.8s finished\n",
      "\n",
      "[2025-09-28 14:43:53] Features: 46/107 -- score: 0.18598674364699655[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  61 out of  61 | elapsed:    5.1s finished\n",
      "\n",
      "[2025-09-28 14:43:59] Features: 47/107 -- score: 0.18600797947547873[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    5.1s finished\n",
      "\n",
      "[2025-09-28 14:44:04] Features: 48/107 -- score: 0.1860280317620513[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  59 out of  59 | elapsed:    5.3s finished\n",
      "\n",
      "[2025-09-28 14:44:10] Features: 49/107 -- score: 0.1861470325495228[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  58 out of  58 | elapsed:    5.8s finished\n",
      "\n",
      "[2025-09-28 14:44:16] Features: 50/107 -- score: 0.18626371125405775[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  57 | elapsed:    6.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:    6.0s finished\n",
      "\n",
      "[2025-09-28 14:44:22] Features: 51/107 -- score: 0.1863354657607231[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  56 | elapsed:    6.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  56 | elapsed:    6.2s finished\n",
      "\n",
      "[2025-09-28 14:44:28] Features: 52/107 -- score: 0.18639236044978694[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  55 | elapsed:    6.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    6.4s finished\n",
      "\n",
      "[2025-09-28 14:44:35] Features: 53/107 -- score: 0.1865280339511047[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  54 | elapsed:    6.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    6.5s finished\n",
      "\n",
      "[2025-09-28 14:44:42] Features: 54/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  53 | elapsed:    6.5s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  53 | elapsed:    6.5s finished\n",
      "\n",
      "[2025-09-28 14:44:48] Features: 55/107 -- score: 0.18657535826972735[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  52 | elapsed:    6.7s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:    6.9s finished\n",
      "\n",
      "[2025-09-28 14:44:55] Features: 56/107 -- score: 0.1865753582697274[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  51 | elapsed:    7.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:    7.2s finished\n",
      "\n",
      "[2025-09-28 14:45:03] Features: 57/107 -- score: 0.18657535826972735[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  50 | elapsed:    7.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    7.2s finished\n",
      "\n",
      "[2025-09-28 14:45:10] Features: 58/107 -- score: 0.1865753582697273[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  49 | elapsed:    7.5s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  49 | elapsed:    7.5s finished\n",
      "\n",
      "[2025-09-28 14:45:18] Features: 59/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  48 | elapsed:    7.4s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    7.5s finished\n",
      "\n",
      "[2025-09-28 14:45:26] Features: 60/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  47 | elapsed:    7.5s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  47 | elapsed:    7.7s finished\n",
      "\n",
      "[2025-09-28 14:45:33] Features: 61/107 -- score: 0.1865753582697273[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  46 | elapsed:    7.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  46 | elapsed:    7.7s finished\n",
      "\n",
      "[2025-09-28 14:45:41] Features: 62/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  45 | elapsed:    7.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    7.8s finished\n",
      "\n",
      "[2025-09-28 14:45:49] Features: 63/107 -- score: 0.18657535826972724[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  44 | elapsed:    7.9s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:    8.0s finished\n",
      "\n",
      "[2025-09-28 14:45:58] Features: 64/107 -- score: 0.18657535826972735[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  43 | elapsed:    7.6s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  43 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:46:06] Features: 65/107 -- score: 0.18657535826972724[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  42 | elapsed:    7.6s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:    8.0s finished\n",
      "\n",
      "[2025-09-28 14:46:14] Features: 66/107 -- score: 0.1865753582697273[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  41 | elapsed:    7.1s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  41 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:46:22] Features: 67/107 -- score: 0.18657424173907375[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  40 | elapsed:    7.7s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:46:31] Features: 68/107 -- score: 0.18656952852940564[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  39 | elapsed:    7.4s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  39 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:46:39] Features: 69/107 -- score: 0.18656952852940564[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  38 | elapsed:    7.4s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  38 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:46:48] Features: 70/107 -- score: 0.18656623923423743[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  37 | elapsed:    7.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  37 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:46:56] Features: 71/107 -- score: 0.1865662392342374[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  36 | elapsed:    7.6s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    8.2s finished\n",
      "\n",
      "[2025-09-28 14:47:04] Features: 72/107 -- score: 0.18655851694591766[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  35 | elapsed:    7.2s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:13] Features: 73/107 -- score: 0.18655851694591766[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  34 | elapsed:    7.4s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:21] Features: 74/107 -- score: 0.18654603701549632[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  33 | elapsed:    7.3s remaining:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  33 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:30] Features: 75/107 -- score: 0.18686420251397814[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  32 | elapsed:    6.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:47:38] Features: 76/107 -- score: 0.18683118237503216[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  31 | elapsed:    6.1s remaining:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:47] Features: 77/107 -- score: 0.18679131707303367[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    5.3s remaining:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:55] Features: 78/107 -- score: 0.18675356585575462[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  29 | elapsed:    5.6s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:48:04] Features: 79/107 -- score: 0.18674660501820298[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:    5.0s remaining:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:48:12] Features: 80/107 -- score: 0.1866979038393903[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  27 | elapsed:    4.9s remaining:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:    7.9s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:48:20] Features: 81/107 -- score: 0.18667223359013577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  26 | elapsed:    4.9s remaining:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  26 | elapsed:    7.7s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    7.7s finished\n",
      "\n",
      "[2025-09-28 14:48:28] Features: 82/107 -- score: 0.18663897753087552[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  25 | elapsed:    4.8s remaining:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  25 | elapsed:    7.3s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    7.5s finished\n",
      "\n",
      "[2025-09-28 14:48:35] Features: 83/107 -- score: 0.18666137928981708[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:    4.7s remaining:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:    7.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    7.2s finished\n",
      "\n",
      "[2025-09-28 14:48:43] Features: 84/107 -- score: 0.1866702554612374[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  23 | elapsed:    4.6s remaining:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  23 | elapsed:    6.2s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    7.1s finished\n",
      "\n",
      "[2025-09-28 14:48:50] Features: 85/107 -- score: 0.18665796449059988[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  22 | elapsed:    4.4s remaining:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  22 | elapsed:    6.3s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    6.9s finished\n",
      "\n",
      "[2025-09-28 14:48:57] Features: 86/107 -- score: 0.18660783894367003[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  21 | elapsed:    5.9s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:    6.6s finished\n",
      "\n",
      "[2025-09-28 14:49:04] Features: 87/107 -- score: 0.18660783894367003[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:    5.7s remaining:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    6.7s finished\n",
      "\n",
      "[2025-09-28 14:49:11] Features: 88/107 -- score: 0.18660783894366986[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  19 | elapsed:    5.6s remaining:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    6.4s finished\n",
      "\n",
      "[2025-09-28 14:49:18] Features: 89/107 -- score: 0.18655545030285048[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:    5.2s remaining:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    6.3s finished\n",
      "\n",
      "[2025-09-28 14:49:24] Features: 90/107 -- score: 0.18648147274912033[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  17 | elapsed:    5.1s remaining:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  17 | elapsed:    6.2s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:    6.4s finished\n",
      "\n",
      "[2025-09-28 14:49:31] Features: 91/107 -- score: 0.18640155043274506[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  16 | elapsed:    5.3s remaining:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  16 | elapsed:    6.0s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    6.1s finished\n",
      "\n",
      "[2025-09-28 14:49:37] Features: 92/107 -- score: 0.18631408804032104[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    5.8s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    5.8s finished\n",
      "\n",
      "[2025-09-28 14:49:43] Features: 93/107 -- score: 0.1863140880403211[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  14 | elapsed:    5.5s remaining:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:    5.6s finished\n",
      "\n",
      "[2025-09-28 14:49:49] Features: 94/107 -- score: 0.18620896090507955[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  13 | elapsed:    5.3s remaining:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  13 | elapsed:    5.3s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    5.3s finished\n",
      "\n",
      "[2025-09-28 14:49:55] Features: 95/107 -- score: 0.1862089609050794[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    5.0s remaining:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    5.0s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    5.0s finished\n",
      "\n",
      "[2025-09-28 14:50:00] Features: 96/107 -- score: 0.18607865249380606[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    4.5s remaining:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    4.6s finished\n",
      "\n",
      "[2025-09-28 14:50:05] Features: 97/107 -- score: 0.1859285772805774[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    4.2s remaining:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.2s finished\n",
      "\n",
      "[2025-09-28 14:50:09] Features: 98/107 -- score: 0.1857508578648969[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    3.8s remaining:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    3.8s finished\n",
      "\n",
      "[2025-09-28 14:50:13] Features: 99/107 -- score: 0.18556131567349568[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    3.4s remaining:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    3.4s finished\n",
      "\n",
      "[2025-09-28 14:50:17] Features: 100/107 -- score: 0.18525879218816046[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    2.8s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    2.8s finished\n",
      "\n",
      "[2025-09-28 14:50:20] Features: 101/107 -- score: 0.18544038721852327[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.3s finished\n",
      "\n",
      "[2025-09-28 14:50:22] Features: 102/107 -- score: 0.18512974870671342[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.6s finished\n",
      "\n",
      "[2025-09-28 14:50:24] Features: 103/107 -- score: 0.18512974870671342[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.2s finished\n",
      "\n",
      "[2025-09-28 14:50:26] Features: 104/107 -- score: 0.1851297487067134[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.9s finished\n",
      "\n",
      "[2025-09-28 14:50:27] Features: 105/107 -- score: 0.1851297487067133[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    0.5s finished\n",
      "\n",
      "[2025-09-28 14:50:28] Features: 106/107 -- score: 0.18481102822461862[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['low_card__op_unique_carrier_AA', 'low_card__op_unique_carrier_AS', 'low_card__op_unique_carrier_DL', 'low_card__op_unique_carrier_F9', 'low_card__op_unique_carrier_HA', 'low_card__op_unique_carrier_NK', 'low_card__op_unique_carrier_OO', 'low_card__op_unique_carrier_UA', 'low_card__op_unique_carrier_WN', 'low_card__op_unique_carrier_YX', 'low_card__op_carrier_AA', 'low_card__op_carrier_AS', 'low_card__op_carrier_DL', 'low_card__op_carrier_F9', 'low_card__op_carrier_HA', 'low_card__op_carrier_NK', 'low_card__op_carrier_OO', 'low_card__op_carrier_UA', 'low_card__op_carrier_WN', 'low_card__op_carrier_YX', 'low_card__dep_time_blk_0001-0559', 'low_card__dep_time_blk_0600-0659', 'low_card__dep_time_blk_0700-0759', 'low_card__dep_time_blk_0800-0859', 'low_card__dep_time_blk_0900-0959', 'low_card__dep_time_blk_1400-1459', 'low_card__dep_time_blk_1700-1759', 'low_card__dep_time_blk_1800-1859', 'low_card__dep_time_blk_1900-1959', 'low_card__dep_time_blk_2000-2059', 'low_card__dep_time_blk_2100-2159', 'low_card__dep_time_blk_2200-2259', 'low_card__arr_time_blk_0001-0559', 'low_card__arr_time_blk_0600-0659', 'low_card__arr_time_blk_0700-0759', 'low_card__arr_time_blk_0800-0859', 'low_card__arr_time_blk_0900-0959', 'low_card__arr_time_blk_1000-1059', 'low_card__arr_time_blk_1100-1159', 'low_card__arr_time_blk_1200-1259', 'low_card__arr_time_blk_1300-1359', 'low_card__arr_time_blk_1400-1459', 'low_card__arr_time_blk_1500-1559', 'low_card__arr_time_blk_1600-1659', 'low_card__arr_time_blk_1700-1759', 'low_card__arr_time_blk_1800-1859', 'low_card__arr_time_blk_1900-1959', 'low_card__arr_time_blk_2000-2059', 'low_card__arr_time_blk_2100-2159', 'low_card__arr_time_blk_2200-2259', 'low_card__arr_time_blk_2300-2359', 'low_card__cancellation_code_B', 'high_card__tail_num', 'high_card__origin', 'high_card__origin_state_abr', 'high_card__origin_state_nm', 'high_card__dest_city_name', 'high_card__dest_state_abr', 'high_card__dest_state_nm', 'num__year', 'num__quarter', 'num__month', 'num__op_carrier_fl_num', 'num__dep_time', 'num__taxi_out', 'num__wheels_off', 'num__wheels_on', 'num__taxi_in', 'num__arr_time', 'num__flights', 'num__first_dep_time', 'num__total_add_gtime', 'num__div_airport_landings', 'num__div_actual_elapsed_time', 'num__div_arr_delay']\n",
      "Model Accuracy (R²): -0.0641\n",
      "Model RMSE: 3364.6948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2025-09-28 14:50:28] Features: 107/107 -- score: 0.18481102822461867"
     ]
    }
   ],
   "source": [
    "# Fit preprocessor with y (important for supervised encoders)\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "X_train_trans = preprocessor.transform(X_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "sfs_forward = SFS(\n",
    "    lin_reg,\n",
    "    k_features='best',\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "sfs_forward = sfs_forward.fit(X_train_trans, y_train)\n",
    "\n",
    "# Map indices back to feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "selected_features = [feature_names[i] for i in sfs_forward.k_feature_idx_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# --- Evaluate model accuracy ---\n",
    "# Restrict to selected features\n",
    "X_train_sel = X_train_trans[:, sfs_forward.k_feature_idx_]\n",
    "X_test_sel  = X_test_trans[:, sfs_forward.k_feature_idx_]\n",
    "\n",
    "# Fit final model\n",
    "lin_reg.fit(X_train_sel, y_train)\n",
    "y_pred = lin_reg.predict(X_test_sel)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf3cf5",
   "metadata": {},
   "source": [
    "### Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36908552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:   55.2s finished\n",
      "\n",
      "[2025-09-28 14:52:27] Features: 106/1 -- score: 0.18499935318473468[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 106 out of 106 | elapsed:   53.9s finished\n",
      "\n",
      "[2025-09-28 14:53:21] Features: 105/1 -- score: 0.18516577826394912[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:   51.9s finished\n",
      "\n",
      "[2025-09-28 14:54:13] Features: 104/1 -- score: 0.18531519681955885[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 104 | elapsed:   50.5s finished\n",
      "\n",
      "[2025-09-28 14:55:04] Features: 103/1 -- score: 0.18541494981583076[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 103 out of 103 | elapsed:   49.5s finished\n",
      "\n",
      "[2025-09-28 14:55:54] Features: 102/1 -- score: 0.18549382862594602[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 102 out of 102 | elapsed:   48.0s finished\n",
      "\n",
      "[2025-09-28 14:56:42] Features: 101/1 -- score: 0.18557007509780235[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:   46.8s finished\n",
      "\n",
      "[2025-09-28 14:57:29] Features: 100/1 -- score: 0.1856332803752098[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   45.3s finished\n",
      "\n",
      "[2025-09-28 14:58:15] Features: 99/1 -- score: 0.18563493637579034[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  99 out of  99 | elapsed:   44.0s finished\n",
      "\n",
      "[2025-09-28 14:58:59] Features: 98/1 -- score: 0.18563493637579057[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  98 out of  98 | elapsed:   42.7s finished\n",
      "\n",
      "[2025-09-28 14:59:42] Features: 97/1 -- score: 0.18563493637579054[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 out of  97 | elapsed:   41.6s finished\n",
      "\n",
      "[2025-09-28 15:00:24] Features: 96/1 -- score: 0.18563493637579057[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:   40.6s finished\n",
      "\n",
      "[2025-09-28 15:01:04] Features: 95/1 -- score: 0.18563493637579057[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  95 out of  95 | elapsed:   39.6s finished\n",
      "\n",
      "[2025-09-28 15:01:44] Features: 94/1 -- score: 0.18563493637579065[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  94 out of  94 | elapsed:   37.7s finished\n",
      "\n",
      "[2025-09-28 15:02:22] Features: 93/1 -- score: 0.18589816652691923[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of  93 | elapsed:   34.7s finished\n",
      "\n",
      "[2025-09-28 15:02:57] Features: 92/1 -- score: 0.18598731464990514[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of  92 | elapsed:   33.2s finished\n",
      "\n",
      "[2025-09-28 15:03:30] Features: 91/1 -- score: 0.18599039132048115[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  91 out of  91 | elapsed:   31.7s finished\n",
      "\n",
      "[2025-09-28 15:04:02] Features: 90/1 -- score: 0.18599039132048126[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   30.0s finished\n",
      "\n",
      "[2025-09-28 15:04:33] Features: 89/1 -- score: 0.1859903913204813[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  89 out of  89 | elapsed:   28.9s finished\n",
      "\n",
      "[2025-09-28 15:05:02] Features: 88/1 -- score: 0.18599039132048137[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  88 out of  88 | elapsed:   27.9s finished\n",
      "\n",
      "[2025-09-28 15:05:30] Features: 87/1 -- score: 0.18599039132048145[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  87 out of  87 | elapsed:   26.9s finished\n",
      "\n",
      "[2025-09-28 15:05:57] Features: 86/1 -- score: 0.18606043572686368[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  86 out of  86 | elapsed:   26.0s finished\n",
      "\n",
      "[2025-09-28 15:06:23] Features: 85/1 -- score: 0.186102158005397[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  85 | elapsed:   24.9s finished\n",
      "\n",
      "[2025-09-28 15:06:48] Features: 84/1 -- score: 0.18610215800539698[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed:   24.2s finished\n",
      "\n",
      "[2025-09-28 15:07:13] Features: 83/1 -- score: 0.18610215800539706[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  83 out of  83 | elapsed:   23.5s finished\n",
      "\n",
      "[2025-09-28 15:07:36] Features: 82/1 -- score: 0.18610215800539695[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  82 out of  82 | elapsed:   23.1s finished\n",
      "\n",
      "[2025-09-28 15:08:00] Features: 81/1 -- score: 0.18613402900770337[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:   22.3s finished\n",
      "\n",
      "[2025-09-28 15:08:22] Features: 80/1 -- score: 0.18613402900770368[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   21.4s finished\n",
      "\n",
      "[2025-09-28 15:08:44] Features: 79/1 -- score: 0.1861340290077036[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  79 out of  79 | elapsed:   20.0s finished\n",
      "\n",
      "[2025-09-28 15:09:04] Features: 78/1 -- score: 0.18613402900770348[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  78 out of  78 | elapsed:   19.4s finished\n",
      "\n",
      "[2025-09-28 15:09:24] Features: 77/1 -- score: 0.18613402900770346[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  77 out of  77 | elapsed:   18.5s finished\n",
      "\n",
      "[2025-09-28 15:09:43] Features: 76/1 -- score: 0.18613402900770357[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 out of  76 | elapsed:   17.8s finished\n",
      "\n",
      "[2025-09-28 15:10:01] Features: 75/1 -- score: 0.18613402900770348[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:   17.0s finished\n",
      "\n",
      "[2025-09-28 15:10:18] Features: 74/1 -- score: 0.18613402900770346[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  74 out of  74 | elapsed:   16.3s finished\n",
      "\n",
      "[2025-09-28 15:10:34] Features: 73/1 -- score: 0.18613402900770362[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  73 out of  73 | elapsed:   15.3s finished\n",
      "\n",
      "[2025-09-28 15:10:50] Features: 72/1 -- score: 0.18613402900770346[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   14.8s finished\n",
      "\n",
      "[2025-09-28 15:11:05] Features: 71/1 -- score: 0.1861340290077035[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of  71 | elapsed:   14.1s finished\n",
      "\n",
      "[2025-09-28 15:11:19] Features: 70/1 -- score: 0.18613402900770337[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   13.6s finished\n",
      "\n",
      "[2025-09-28 15:11:33] Features: 69/1 -- score: 0.18613256860914953[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  69 | elapsed:   13.0s finished\n",
      "\n",
      "[2025-09-28 15:11:46] Features: 68/1 -- score: 0.18615759389065667[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  68 | elapsed:   12.4s finished\n",
      "\n",
      "[2025-09-28 15:11:59] Features: 67/1 -- score: 0.1861330467236765[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  67 out of  67 | elapsed:   11.9s finished\n",
      "\n",
      "[2025-09-28 15:12:11] Features: 66/1 -- score: 0.18610821099403577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  66 | elapsed:   11.5s finished\n",
      "\n",
      "[2025-09-28 15:12:23] Features: 65/1 -- score: 0.18611050677894628[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:   11.0s finished\n",
      "\n",
      "[2025-09-28 15:12:34] Features: 64/1 -- score: 0.18608219271424245[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:   10.4s finished\n",
      "\n",
      "[2025-09-28 15:12:45] Features: 63/1 -- score: 0.18603054581937853[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed:    9.8s finished\n",
      "\n",
      "[2025-09-28 15:12:55] Features: 62/1 -- score: 0.18598814412586703[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  62 out of  62 | elapsed:    9.3s finished\n",
      "\n",
      "[2025-09-28 15:13:04] Features: 61/1 -- score: 0.1859370314079706[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  61 out of  61 | elapsed:    8.8s finished\n",
      "\n",
      "[2025-09-28 15:13:13] Features: 60/1 -- score: 0.18592090984571877[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 15:13:22] Features: 59/1 -- score: 0.1859050065151784[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  59 out of  59 | elapsed:    7.8s finished\n",
      "\n",
      "[2025-09-28 15:13:30] Features: 58/1 -- score: 0.18586805641794915[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  58 out of  58 | elapsed:    7.5s finished\n",
      "\n",
      "[2025-09-28 15:13:38] Features: 57/1 -- score: 0.18581388690177555[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  57 | elapsed:    7.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:    7.0s finished\n",
      "\n",
      "[2025-09-28 15:13:45] Features: 56/1 -- score: 0.18573079061602302[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  56 | elapsed:    6.7s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  56 | elapsed:    6.8s finished\n",
      "\n",
      "[2025-09-28 15:13:52] Features: 55/1 -- score: 0.1856259974568312[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  55 | elapsed:    6.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    6.2s finished\n",
      "\n",
      "[2025-09-28 15:13:58] Features: 54/1 -- score: 0.185538859436197[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  54 | elapsed:    5.8s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    5.8s finished\n",
      "\n",
      "[2025-09-28 15:14:04] Features: 53/1 -- score: 0.18543934344976518[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  53 | elapsed:    5.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  53 | elapsed:    5.3s finished\n",
      "\n",
      "[2025-09-28 15:14:10] Features: 52/1 -- score: 0.185418601153418[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  52 | elapsed:    5.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:    5.2s finished\n",
      "\n",
      "[2025-09-28 15:14:15] Features: 51/1 -- score: 0.1853640516507528[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  51 | elapsed:    4.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:    4.7s finished\n",
      "\n",
      "[2025-09-28 15:14:20] Features: 50/1 -- score: 0.18532336575538885[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  50 | elapsed:    4.1s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    4.2s finished\n",
      "\n",
      "[2025-09-28 15:14:24] Features: 49/1 -- score: 0.1853249746783168[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  49 | elapsed:    3.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  49 | elapsed:    4.0s finished\n",
      "\n",
      "[2025-09-28 15:14:29] Features: 48/1 -- score: 0.18533010291134236[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  48 | elapsed:    3.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    3.6s finished\n",
      "\n",
      "[2025-09-28 15:14:32] Features: 47/1 -- score: 0.18529398123391413[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  47 | elapsed:    3.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  47 | elapsed:    3.2s finished\n",
      "\n",
      "[2025-09-28 15:14:36] Features: 46/1 -- score: 0.18528286632061744[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  46 | elapsed:    2.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  46 | elapsed:    3.0s finished\n",
      "\n",
      "[2025-09-28 15:14:39] Features: 45/1 -- score: 0.18527065848517357[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  45 | elapsed:    2.8s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    2.9s finished\n",
      "\n",
      "[2025-09-28 15:14:42] Features: 44/1 -- score: 0.18528120983306226[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  44 | elapsed:    2.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:    2.6s finished\n",
      "\n",
      "[2025-09-28 15:14:45] Features: 43/1 -- score: 0.18531695504146906[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  43 | elapsed:    2.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  43 | elapsed:    2.4s finished\n",
      "\n",
      "[2025-09-28 15:14:48] Features: 42/1 -- score: 0.1852371225912781[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  42 | elapsed:    2.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:    2.2s finished\n",
      "\n",
      "[2025-09-28 15:14:50] Features: 41/1 -- score: 0.18513960828210582[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  41 | elapsed:    1.6s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  41 | elapsed:    1.8s finished\n",
      "\n",
      "[2025-09-28 15:14:52] Features: 40/1 -- score: 0.18503484486257282[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  40 | elapsed:    1.7s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    1.8s finished\n",
      "\n",
      "[2025-09-28 15:14:54] Features: 39/1 -- score: 0.18490868604921865[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  39 | elapsed:    1.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  39 | elapsed:    1.6s finished\n",
      "\n",
      "[2025-09-28 15:14:56] Features: 38/1 -- score: 0.18477230577639112[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  38 | elapsed:    1.4s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  38 | elapsed:    1.5s finished\n",
      "\n",
      "[2025-09-28 15:14:58] Features: 37/1 -- score: 0.18460497192743683[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  37 | elapsed:    1.3s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  37 | elapsed:    1.4s finished\n",
      "\n",
      "[2025-09-28 15:14:59] Features: 36/1 -- score: 0.18440434269854142[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  36 | elapsed:    1.2s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    1.3s finished\n",
      "\n",
      "[2025-09-28 15:15:01] Features: 35/1 -- score: 0.18417819833966795[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  35 | elapsed:    1.0s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:    1.2s finished\n",
      "\n",
      "[2025-09-28 15:15:02] Features: 34/1 -- score: 0.18394508553028366[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  34 | elapsed:    0.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    1.0s finished\n",
      "\n",
      "[2025-09-28 15:15:03] Features: 33/1 -- score: 0.1836945648679484[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  33 | elapsed:    0.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  33 | elapsed:    1.0s finished\n",
      "\n",
      "[2025-09-28 15:15:05] Features: 32/1 -- score: 0.18356157114895963[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  32 | elapsed:    0.8s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:    0.9s finished\n",
      "\n",
      "[2025-09-28 15:15:06] Features: 31/1 -- score: 0.1833332890083371[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  31 | elapsed:    0.5s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    0.8s finished\n",
      "\n",
      "[2025-09-28 15:15:07] Features: 30/1 -- score: 0.18306762046895184[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    0.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.7s finished\n",
      "\n",
      "[2025-09-28 15:15:08] Features: 29/1 -- score: 0.1833677364997365[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  29 | elapsed:    0.4s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    0.6s finished\n",
      "\n",
      "[2025-09-28 15:15:08] Features: 28/1 -- score: 0.1831004615096368[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:    0.4s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    0.6s finished\n",
      "\n",
      "[2025-09-28 15:15:09] Features: 27/1 -- score: 0.1828240191014856[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  27 | elapsed:    0.3s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    0.5s finished\n",
      "\n",
      "[2025-09-28 15:15:10] Features: 26/1 -- score: 0.1825268681578656[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  26 | elapsed:    0.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  26 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    0.4s finished\n",
      "\n",
      "[2025-09-28 15:15:11] Features: 25/1 -- score: 0.18254382271102862[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  25 | elapsed:    0.2s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  25 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    0.4s finished\n",
      "\n",
      "[2025-09-28 15:15:11] Features: 24/1 -- score: 0.1822404563435557[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:    0.2s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    0.3s finished\n",
      "\n",
      "[2025-09-28 15:15:12] Features: 23/1 -- score: 0.18189784100321146[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  23 | elapsed:    0.2s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  23 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    0.3s finished\n",
      "\n",
      "[2025-09-28 15:15:12] Features: 22/1 -- score: 0.18175246071947773[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  22 | elapsed:    0.2s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  22 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    0.3s finished\n",
      "\n",
      "[2025-09-28 15:15:13] Features: 21/1 -- score: 0.18161299187406976[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  21 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:    0.3s finished\n",
      "\n",
      "[2025-09-28 15:15:13] Features: 20/1 -- score: 0.18125759536934524[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 15:15:14] Features: 19/1 -- score: 0.1808869513131749[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  19 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 15:15:14] Features: 18/1 -- score: 0.1806196661142252[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 15:15:15] Features: 17/1 -- score: 0.18038952242629513[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  17 | elapsed:    0.1s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  17 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 15:15:15] Features: 16/1 -- score: 0.18022501271792413[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  16 | elapsed:    0.1s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  16 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    0.1s finished\n",
      "\n",
      "[2025-09-28 15:15:16] Features: 15/1 -- score: 0.17981407630713495[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.1s finished\n",
      "\n",
      "[2025-09-28 15:15:16] Features: 14/1 -- score: 0.17967795673235507[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  14 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:    0.1s finished\n",
      "\n",
      "[2025-09-28 15:15:16] Features: 13/1 -- score: 0.17940685167990658[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  13 | elapsed:    0.0s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  13 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    0.1s finished\n",
      "\n",
      "[2025-09-28 15:15:16] Features: 12/1 -- score: 0.17902910208738906[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:17] Features: 11/1 -- score: 0.17863330809082537[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:17] Features: 10/1 -- score: 0.1779559104292702[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:17] Features: 9/1 -- score: 0.17710846783228282[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:17] Features: 8/1 -- score: 0.1761376266403564[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:17] Features: 7/1 -- score: 0.174865193774005[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:18] Features: 6/1 -- score: 0.17352176415675283[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:18] Features: 5/1 -- score: 0.1717977746777414[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:18] Features: 4/1 -- score: 0.16923298214780713[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:18] Features: 3/1 -- score: 0.1637377783872315[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.0s finished\n",
      "\n",
      "[2025-09-28 15:15:18] Features: 2/1 -- score: 0.15745076680487888[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward-selected features: ['low_card__op_unique_carrier_AA', 'low_card__op_unique_carrier_AS', 'low_card__op_unique_carrier_B6', 'low_card__op_unique_carrier_F9', 'low_card__op_unique_carrier_G4', 'low_card__op_unique_carrier_NK', 'low_card__op_unique_carrier_YX', 'low_card__op_carrier_DL', 'low_card__op_carrier_HA', 'low_card__op_carrier_MQ', 'low_card__op_carrier_OH', 'low_card__op_carrier_UA', 'low_card__dep_time_blk_0001-0559', 'low_card__dep_time_blk_0600-0659', 'low_card__dep_time_blk_0700-0759', 'low_card__dep_time_blk_0800-0859', 'low_card__dep_time_blk_0900-0959', 'low_card__dep_time_blk_1000-1059', 'low_card__dep_time_blk_1100-1159', 'low_card__dep_time_blk_1200-1259', 'low_card__dep_time_blk_1300-1359', 'low_card__dep_time_blk_1400-1459', 'low_card__dep_time_blk_1500-1559', 'low_card__dep_time_blk_1600-1659', 'low_card__dep_time_blk_1700-1759', 'low_card__dep_time_blk_1900-1959', 'low_card__dep_time_blk_2200-2259', 'low_card__arr_time_blk_0001-0559', 'low_card__arr_time_blk_0600-0659', 'low_card__arr_time_blk_0700-0759', 'low_card__arr_time_blk_0800-0859', 'low_card__arr_time_blk_0900-0959', 'low_card__arr_time_blk_1000-1059', 'low_card__arr_time_blk_1100-1159', 'low_card__arr_time_blk_1200-1259', 'low_card__arr_time_blk_1300-1359', 'low_card__arr_time_blk_1400-1459', 'low_card__arr_time_blk_1500-1559', 'low_card__arr_time_blk_1600-1659', 'low_card__arr_time_blk_1800-1859', 'low_card__arr_time_blk_1900-1959', 'low_card__arr_time_blk_2000-2059', 'low_card__arr_time_blk_2100-2159', 'low_card__arr_time_blk_2200-2259', 'low_card__arr_time_blk_2300-2359', 'low_card__cancellation_code_B', 'high_card__tail_num', 'high_card__origin', 'high_card__origin_state_abr', 'high_card__dest', 'high_card__dest_state_abr', 'num__quarter', 'num__month', 'num__op_carrier_fl_num', 'num__dep_time', 'num__wheels_off', 'num__wheels_on', 'num__taxi_in', 'num__arr_time', 'num__actual_elapsed_time', 'num__air_time', 'num__first_dep_time', 'num__total_add_gtime', 'num__div_airport_landings', 'num__div_reached_dest', 'num__div_actual_elapsed_time', 'num__div_arr_delay', 'num__div_distance']\n",
      "Backward Selection Model Accuracy (R²): -0.0631\n",
      "Backward Selection Model RMSE: 3361.7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2025-09-28 15:15:18] Features: 1/1 -- score: 0.14339819239562326"
     ]
    }
   ],
   "source": [
    "# --- Backward selection ---\n",
    "sfs_backward = SFS(\n",
    "    lin_reg,\n",
    "    k_features='best',\n",
    "    forward=False,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Important: fit preprocessor with y\n",
    "preprocessor.fit(X_train, y_train)\n",
    "X_train_trans = preprocessor.transform(X_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "sfs_backward = sfs_backward.fit(X_train_trans, y_train)\n",
    "\n",
    "# Map indices back to feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "selected_features = [feature_names[i] for i in sfs_backward.k_feature_idx_]\n",
    "print(\"Backward-selected features:\", selected_features)\n",
    "\n",
    "# --- Evaluate model accuracy ---\n",
    "# Restrict to selected features\n",
    "X_train_sel = X_train_trans[:, sfs_backward.k_feature_idx_]\n",
    "X_test_sel  = X_test_trans[:, sfs_backward.k_feature_idx_]\n",
    "\n",
    "# Fit final model\n",
    "lin_reg.fit(X_train_sel, y_train)\n",
    "y_pred = lin_reg.predict(X_test_sel)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Backward Selection Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Backward Selection Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b4571",
   "metadata": {},
   "source": [
    "### PCR\n",
    "Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCR R2: -0.08136605278516584\n",
      "PCR RMSE: 3419.3802488901792\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline: preprocessing → PCA → Linear Regression\n",
    "pcr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=20)),   # choose number of components\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pcr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pcr = pcr.predict(X_test)\n",
    "\n",
    "print(\"PCR R2:\", r2_score(y_test, y_pred_pcr))\n",
    "print(\"PCR RMSE:\", mean_squared_error(y_test, y_pred_pcr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6974ff5",
   "metadata": {},
   "source": [
    "### PLSR\n",
    "Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb8545",
   "metadata": {},
   "source": [
    "## Week 4: USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b5bb0",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e5c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\2341748188.py:6: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\2341748188.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\2341748188.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\2341748188.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\2341748188.py:6: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n",
      "Memory usage before optimization: 3779.75 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\3367681747.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization: 552.31 MB\n",
      "Reduced by 85.4%\n",
      "Sampling the dataset to 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\2341748188.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  usdot_df.groupby('depdelay_bin', group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\2341748188.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "SAMPLE_SIZE = 50000\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "combined_df.columns = combined_df.columns.str.lower()\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)\n",
    "\n",
    "if SAMPLE_SIZE >= 0:\n",
    "    print(\"Sampling the dataset to\", SAMPLE_SIZE)\n",
    "    bins = [-np.inf, -1, 0, 15, 60, 180, np.inf]\n",
    "    labels = ['early', 'on_time', 'small_delay', 'moderate_delay', 'long_delay', 'extreme_delay']\n",
    "\n",
    "    usdot_df['depdelay_bin'] = pd.cut(usdot_df['dep_delay'], bins=bins, labels=labels)\n",
    "    usdot_df = (\n",
    "        usdot_df.groupby('depdelay_bin', group_keys=False)\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=min(int(SAMPLE_SIZE * len(x) / len(usdot_df)), len(x)), \n",
    "            random_state=42\n",
    "        ))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    usdot_df = usdot_df.drop(columns=['depdelay_bin'])\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "\n",
    "# --- Create binary target: 1 if depdelay > 0, else 0 ---\n",
    "usdot_df_lin['depdelay_binary'] = (usdot_df_lin['dep_delay'] > 0).astype(int)\n",
    "\n",
    "# Update target variable\n",
    "X = usdot_df_lin.drop(columns=['dep_delay', 'depdelay_binary'])\n",
    "y = usdot_df_lin['depdelay_binary']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def461c9",
   "metadata": {},
   "source": [
    "### Log Regression: Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c82aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results\n",
      "Accuracy: 0.6157\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67      6230\n",
      "           1       0.49      0.59      0.54      3770\n",
      "\n",
      "    accuracy                           0.62     10000\n",
      "   macro avg       0.60      0.61      0.60     10000\n",
      "weighted avg       0.63      0.62      0.62     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Logistic Regression Pipeline ---\n",
    "log_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000,       # increase iterations for convergence\n",
    "        solver='lbfgs',      # robust solver\n",
    "        n_jobs=-1,            # parallelize\n",
    "        class_weight='balanced'  # handle class imbalance\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Fit model ---\n",
    "log_reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# --- Predict ---\n",
    "y_pred = log_reg_pipe.predict(X_test)\n",
    "\n",
    "# --- Evaluate ---\n",
    "print(\"\\nLogistic Regression Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec7b94",
   "metadata": {},
   "source": [
    "### Log Regression: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dcdc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters: {'classifier__C': np.float64(0.008632008168602538), 'classifier__class_weight': 'balanced', 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
      "Best CV Score: 0.5898257289075901\n",
      "\n",
      "Random Search Logistic Regression Results\n",
      "Accuracy: 0.6348\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.62      0.68      6230\n",
      "           1       0.51      0.66      0.58      3770\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.63      0.64      0.63     10000\n",
      "weighted avg       0.66      0.63      0.64     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Define parameter distributions ---\n",
    "param_dist = {\n",
    "    # Regularization strength (inverse of penalty)\n",
    "    'classifier__C': loguniform(1e-3, 1e3),\n",
    "    \n",
    "    # Penalty type (note: lbfgs only supports l2, saga supports l1/l2/elasticnet)\n",
    "    'classifier__penalty': ['l2'],\n",
    "    \n",
    "    # Try different solvers (must be compatible with penalty)\n",
    "    'classifier__solver': ['lbfgs', 'saga'],\n",
    "    \n",
    "    # Optionally explore class weights\n",
    "    'classifier__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# --- Randomized Search ---\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=log_reg_pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,              # number of random combinations to try\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='f1',           # optimize for F1 (better for imbalance than accuracy)\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- Fit random search ---\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Best parameters and score ---\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best CV Score:\", random_search.best_score_)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Search Logistic Regression Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSR R2: -0.06250654912747011\n",
      "PLSR RMSE: 3359.7447405025227\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess first\n",
    "X_train_trans = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "# Fit PLSR with, say, 10 components\n",
    "plsr = PLSRegression(n_components=10)\n",
    "plsr.fit(X_train_trans, y_train)\n",
    "\n",
    "y_pred_plsr = plsr.predict(X_test_trans)\n",
    "\n",
    "print(\"PLSR R2:\", r2_score(y_test, y_pred_plsr))\n",
    "print(\"PLSR RMSE:\", mean_squared_error(y_test, y_pred_plsr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
