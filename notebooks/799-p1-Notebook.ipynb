{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d58ef2",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa4899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "& was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "# Load Virtual Environment\n",
    "\n",
    "!& \"c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Scripts\\Activate.ps1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bda5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9a73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = 'C:/Users/tbran/Python/repos/Semester 3 Repos/capstone/'\n",
    "data_path = project_path + 'data/'\n",
    "src_path = project_path + 'src/'\n",
    "model_path = project_path + 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc5532",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7983db",
   "metadata": {},
   "source": [
    "## Week 1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7713e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(df, datetime_cols=None, fillna=False):\n",
    "    \"\"\"\n",
    "    Cleans and optimizes a DataFrame:\n",
    "    - Converts object datetime columns to datetime64\n",
    "    - Converts object columns with repeated values to category\n",
    "    - Downcasts numeric columns to smallest safe type\n",
    "    - Optionally fills NaNs before downcasting\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to optimize\n",
    "        datetime_cols (list): List of column names to convert to datetime\n",
    "        fillna (bool): If True, fills NaNs before downcasting\n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Convert datetime columns\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # 2. Convert object columns to category if appropriate\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < 0.5:  # heuristic: less than 50% unique\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # 3. Downcast numeric columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "    \n",
    "    for col in int_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in float_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    print(f\"Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_column_names(df, remove_accents=True):\n",
    "    \"\"\"\n",
    "    Cleans DataFrame column names:\n",
    "    - Strips whitespace\n",
    "    - Converts to lowercase\n",
    "    - Replaces spaces & special chars with underscores\n",
    "    - Removes duplicate underscores\n",
    "    - Optionally removes accents\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame whose columns to clean\n",
    "        remove_accents (bool): If True, strips accents from characters\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    def _clean(col):\n",
    "        col = col.strip().lower()\n",
    "        if remove_accents:\n",
    "            col = ''.join(\n",
    "                c for c in unicodedata.normalize('NFKD', col)\n",
    "                if not unicodedata.combining(c)\n",
    "            )\n",
    "        col = re.sub(r'[^0-9a-zA-Z]+', '_', col)  # replace non-alphanumeric with _\n",
    "        col = re.sub(r'_+', '_', col)             # collapse multiple underscores\n",
    "        col = col.strip('_')                      # remove leading/trailing underscores\n",
    "        return col\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.columns = [_clean(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline(df, target, \n",
    "                                  high_card_threshold=20, \n",
    "                                  scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for linear regression:\n",
    "    - One-hot encodes low-cardinality categorical columns\n",
    "    - Target encodes high-cardinality categorical columns\n",
    "    - Optionally scales numeric columns\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame (including target column)\n",
    "        target (str): Name of target column\n",
    "        high_card_threshold (int): Unique value cutoff for high-cardinality\n",
    "        scale_numeric (bool): Whether to scale numeric features\n",
    "        \n",
    "    Returns:\n",
    "        pipeline (ColumnTransformer): Preprocessing transformer\n",
    "        low_card_cols (list): Low-cardinality categorical columns\n",
    "        high_card_cols (list): High-cardinality categorical columns\n",
    "        num_cols (list): Numeric columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target])\n",
    "    \n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [col for col in cat_cols if X[col].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [col for col in cat_cols if X[col].nunique() > high_card_threshold]\n",
    "    \n",
    "    # Transformers\n",
    "    low_card_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    high_card_transformer = TargetEncoder()\n",
    "    num_transformer = StandardScaler() if scale_numeric else 'passthrough'\n",
    "    \n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', low_card_transformer, low_card_cols),\n",
    "            ('high_card', high_card_transformer, high_card_cols),\n",
    "            ('num', num_transformer, num_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor, low_card_cols, high_card_cols, num_cols\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df, features=None, vif_thresh=10.0):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) safely:\n",
    "    - Removes constant columns\n",
    "    - Removes perfectly collinear columns\n",
    "    - Returns sorted VIF table\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with numeric features\n",
    "        features (list): Optional list of features to check; defaults to all numeric\n",
    "        vif_thresh (float): Threshold for flagging high VIF\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: VIF table\n",
    "    \"\"\"\n",
    "    # Select numeric columns if features not provided\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # 1. Drop constant columns\n",
    "    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"Dropping constant columns: {constant_cols}\")\n",
    "        X.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    # 2. Drop perfectly collinear columns\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    perfect_corr_cols = [col for col in upper.columns if any(upper[col] == 1.0)]\n",
    "    if perfect_corr_cols:\n",
    "        print(f\"Dropping perfectly collinear columns: {perfect_corr_cols}\")\n",
    "        X.drop(columns=perfect_corr_cols, inplace=True)\n",
    "    \n",
    "    # 3. Calculate VIF\n",
    "    X_const = X.assign(const=1)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_const.values, i) for i in range(len(X.columns))]\n",
    "    })\n",
    "    \n",
    "    # 4. Sort by VIF\n",
    "    vif_data.sort_values(by=\"VIF\", ascending=False, inplace=True)\n",
    "    \n",
    "    # 5. Flag high VIF\n",
    "    vif_data[\"High_VIF\"] = vif_data[\"VIF\"] > vif_thresh\n",
    "    \n",
    "    return vif_data\n",
    "\n",
    "def fit_polynomial_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and returns the fitted model and transformed features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "def add_interaction_terms(df, features):\n",
    "    \"\"\"\n",
    "    Adds pairwise interaction terms between given features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    interaction_array = poly.fit_transform(df[features])\n",
    "    interaction_df = pd.DataFrame(interaction_array, columns=poly.get_feature_names_out(features))\n",
    "    return pd.concat([df.reset_index(drop=True), interaction_df], axis=1)\n",
    "\n",
    "def preprocess_features(df, categorical_cols, numeric_cols):\n",
    "    \"\"\"\n",
    "    Returns a ColumnTransformer that one-hot encodes categorical columns\n",
    "    and passes numeric columns through unchanged.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "            ('num', 'passthrough', numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def regression_summary(X, y):\n",
    "    \"\"\"\n",
    "    Fits an OLS regression model using statsmodels and prints the summary.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32311e0",
   "metadata": {},
   "source": [
    "# Week 1 Notebook – Linear Regression 1\n",
    "Each week, you will apply the concepts of that week to your Integrated Capstone Project’s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data, using the methods you have learned about in this course and in this program, and draw interesting conclusions. \n",
    "\n",
    "For Week 1, include concepts such as linear regression with polynomial terms, interaction terms, multicollinearity, variance inflation factor and regression, and categorical and continuous features. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209b52c",
   "metadata": {},
   "source": [
    "## Mendeley Delay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438b27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df = pd.read_csv(data_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1404dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n"
     ]
    }
   ],
   "source": [
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2046a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d437e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n",
      "R^2 score: 0.043766421823172585\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91cb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature           VIF  High_VIF\n",
      "13      largehubairportdest           inf      True\n",
      "12     mediumhubairportdest           inf      True\n",
      "10        nonhubairportdest           inf      True\n",
      "11      smallhubairportdest           inf      True\n",
      "7     smallhubairportorigin           inf      True\n",
      "6       nonhubairportorigin           inf      True\n",
      "18        nonhubairlinedest           inf      True\n",
      "19      smallhubairlinedest           inf      True\n",
      "16   mediumhubairlineorigin           inf      True\n",
      "17    largehubairlineorigin           inf      True\n",
      "20     mediumhubairlinedest           inf      True\n",
      "21      largehubairlinedest           inf      True\n",
      "15    smallhubairlineorigin           inf      True\n",
      "14      nonhubairlineorigin           inf      True\n",
      "8    mediumhubairportorigin  9.007199e+15      True\n",
      "9     largehubairportorigin  9.007199e+15      True\n",
      "39               temp_20_30  3.112541e+02      True\n",
      "38               temp_10_20  2.888918e+02      True\n",
      "37                temp_0_10  1.916812e+02      True\n",
      "40               temp_30_40  1.151481e+02      True\n",
      "36               temp_n10_0  7.219096e+01      True\n",
      "34              temperature  1.429181e+01      True\n",
      "35          temp_ninfty_n10  1.244962e+01      True\n",
      "43          windspeedsquare  9.783338e+00     False\n",
      "42                windspeed  8.658978e+00     False\n",
      "1                  arrdelay  8.096892e+00     False\n",
      "0                  depdelay  8.012084e+00     False\n",
      "41            temp_40_infty  5.988440e+00     False\n",
      "3           marketsharedest  3.643296e+00     False\n",
      "2         marketshareorigin  3.596286e+00     False\n",
      "22                     year  3.081408e+00     False\n",
      "45            windgustspeed  2.712213e+00     False\n",
      "31               numflights  2.603665e+00     False\n",
      "5                   hhidest  2.493102e+00     False\n",
      "4                 hhiorigin  2.430416e+00     False\n",
      "44            windgustdummy  2.181984e+00     False\n",
      "33            monopolyroute  1.598653e+00     False\n",
      "32                 distance  1.564135e+00     False\n",
      "51  originmetrogdppercapita  1.519503e+00     False\n",
      "30               loadfactor  1.452184e+00     False\n",
      "53    destmetrogdppercapita  1.450741e+00     False\n",
      "29                 capacity  1.332612e+00     False\n",
      "50           originmetropop  1.326248e+00     False\n",
      "52             destmetropop  1.326226e+00     False\n",
      "49           snowtracedummy  1.181472e+00     False\n",
      "27          originairportid  1.173991e+00     False\n",
      "25                dayofweek  1.165972e+00     False\n",
      "28            destairportid  1.160076e+00     False\n",
      "48                snowdummy  1.105665e+00     False\n",
      "26            scheduledhour  1.084384e+00     False\n",
      "46                raindummy  1.061760e+00     False\n",
      "23                    month  1.048324e+00     False\n",
      "47           raintracedummy  1.024877e+00     False\n",
      "24               dayofmonth  1.000590e+00     False\n"
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(df)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f78e22",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96001c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18530e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 3779.75 MB\n",
      "Memory usage after optimization: 535.69 MB\n",
      "Reduced by 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d649ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4434d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n",
      "R^2 score: 0.06941058350132379\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "usdot_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "usdot_model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", usdot_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d909dbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping constant columns: ['year', 'flights']\n",
      "                    feature           VIF  High_VIF\n",
      "11          dest_airport_id  5.861101e+09      True\n",
      "12      dest_airport_seq_id  5.861097e+09      True\n",
      "6         origin_airport_id  5.727668e+09      True\n",
      "7     origin_airport_seq_id  5.727665e+09      True\n",
      "35      actual_elapsed_time  5.031438e+03      True\n",
      "36                 air_time  4.704893e+03      True\n",
      "29            arr_delay_new  7.782086e+02      True\n",
      "18                dep_delay  4.491302e+02      True\n",
      "19            dep_delay_new  3.941995e+02      True\n",
      "39            carrier_delay  2.318702e+02      True\n",
      "28                arr_delay  1.658624e+02      True\n",
      "43      late_aircraft_delay  1.573233e+02      True\n",
      "30                arr_del15  1.516176e+02      True\n",
      "34         crs_elapsed_time  1.389059e+02      True\n",
      "22                 taxi_out  1.003038e+02      True\n",
      "37                 distance  8.611603e+01      True\n",
      "41                nas_delay  5.612771e+01      True\n",
      "40            weather_delay  4.864794e+01      True\n",
      "25                  taxi_in  4.525602e+01      True\n",
      "38           distance_group  3.630979e+01      True\n",
      "31          arr_delay_group  3.452216e+01      True\n",
      "21          dep_delay_group  3.050542e+01      True\n",
      "45          total_add_gtime  2.539939e+01      True\n",
      "46        longest_add_gtime  2.533810e+01      True\n",
      "17                 dep_time  2.056231e+01      True\n",
      "23               wheels_off  1.390820e+01      True\n",
      "24                wheels_on  1.338470e+01      True\n",
      "27                 arr_time  1.226734e+01      True\n",
      "16             crs_dep_time  8.976904e+00     False\n",
      "50            div_arr_delay  8.089041e+00     False\n",
      "49  div_actual_elapsed_time  7.906063e+00     False\n",
      "20                dep_del15  4.196562e+00     False\n",
      "1                     month  3.894604e+00     False\n",
      "0                   quarter  3.862039e+00     False\n",
      "26             crs_arr_time  3.665930e+00     False\n",
      "10               origin_wac  1.894548e+00     False\n",
      "13      dest_city_market_id  1.824195e+00     False\n",
      "15                 dest_wac  1.780173e+00     False\n",
      "8     origin_city_market_id  1.757761e+00     False\n",
      "42           security_delay  1.619792e+00     False\n",
      "4     op_carrier_airline_id  1.436409e+00     False\n",
      "33                 diverted  1.398590e+00     False\n",
      "5         op_carrier_fl_num  1.397663e+00     False\n",
      "48         div_reached_dest  1.339445e+00     False\n",
      "47     div_airport_landings  1.337987e+00     False\n",
      "51             div_distance  1.335226e+00     False\n",
      "32                cancelled  1.113056e+00     False\n",
      "9         origin_state_fips  1.079831e+00     False\n",
      "14          dest_state_fips  1.054488e+00     False\n",
      "44           first_dep_time  1.030265e+00     False\n",
      "2              day_of_month  1.002750e+00     False\n",
      "3               day_of_week  1.002279e+00     False\n"
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(usdot_df)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68d61f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: df\n",
      "Dropped DataFrame: df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n",
      "Dropped DataFrame: combined_df\n",
      "Dropped DataFrame: usdot_df\n",
      "Dropped DataFrame: usdot_df_lin\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9f2ac",
   "metadata": {},
   "source": [
    "# Week 2 Notebook - Linear Regression 2\n",
    "\n",
    "For Week 2, include concepts such as linear regression with lasso, ridge, and elastic net regression. This homework will be submitted for peer review and feedback in Week 3 in the assignment titled 3.4 Peer Review: Week 2 Jupyter Notebook. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8fd697",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4b684",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada4666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n",
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n"
     ]
    }
   ],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df = pd.read_csv(data_path + file_name)\n",
    "\n",
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a18f",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0be88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.0337074407107556\n"
     ]
    }
   ],
   "source": [
    "# Default Lasso \n",
    "\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lasso)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491616f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.01\n",
      "Best CV R^2: 0.04403471123674032\n",
      "Test R^2: 0.04379222831327789\n"
     ]
    }
   ],
   "source": [
    "# Lasso with tuned alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6d279",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c167ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha): 0.04376642307060696\n"
     ]
    }
   ],
   "source": [
    "#Ridge with default alpha\n",
    "\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ridge)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default alpha\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha):\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c19aaf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 1000\n",
      "Best CV R^2: 0.043972332094467756\n",
      "Test R^2: 0.04377132510993642\n"
     ]
    }
   ],
   "source": [
    "# Ridge with tune alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c99e1",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a201c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha, l1_ratio): 0.033545260335402505\n"
     ]
    }
   ],
   "source": [
    "# Replace regressor with ElasticNet\n",
    "elastic = ElasticNet(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', elastic)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default parameters\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha, l1_ratio):\", model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8a67925",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m param_grid = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.001\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__l1_ratio\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.1\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m0.9\u001b[39m]\n\u001b[32m      5\u001b[39m }\n\u001b[32m      7\u001b[39m grid = GridSearchCV(model, param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest CV R^2:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Option 2: Tune alpha and l1_ratio with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19e44f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: df\n",
      "Dropped DataFrame: df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b555ac",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b697bc",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3be53614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n",
      "Memory usage before optimization: 3779.75 MB\n",
      "Memory usage after optimization: 535.69 MB\n",
      "Reduced by 85.8%\n",
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ced8a3",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaf2fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.04595741706259315\n"
     ]
    }
   ],
   "source": [
    "# Default Lasso \n",
    "\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lasso)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f56dd8ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m param_grid = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.001\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m]\n\u001b[32m      4\u001b[39m }\n\u001b[32m      6\u001b[39m grid = GridSearchCV(model, param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest alpha:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_[\u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest CV R^2:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Lasso with tuned alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a743c98",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e5da7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha): 0.06941067445046167\n"
     ]
    }
   ],
   "source": [
    "#Ridge with default alpha\n",
    "\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ridge)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default alpha\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha):\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fa346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with tune alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba5f6e",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddfe0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha, l1_ratio): 0.04155075820659526\n"
     ]
    }
   ],
   "source": [
    "# Replace regressor with ElasticNet\n",
    "elastic = ElasticNet(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', elastic)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default parameters\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha, l1_ratio):\", model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19977e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Tune alpha and l1_ratio with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f361c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: combined_df\n",
      "Dropped DataFrame: usdot_df\n",
      "Dropped DataFrame: usdot_df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472fe2",
   "metadata": {},
   "source": [
    "# Week 3 Notebook - Linear Regression 3\n",
    "\n",
    "For Week 3, include concepts such as linear regression with forward and backward selection, PCR, and PLSR. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b99d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd613b8",
   "metadata": {},
   "source": [
    "# Week 4 Notebook - Logistic Regression and Feature Scaling\n",
    "\n",
    "For Week 4, include concepts such as logistic regression and feature scaling. This homework should be submitted for peer review in the assignment titled 4.3 Peer Review: Week 4 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57cfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
