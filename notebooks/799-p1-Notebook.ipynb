{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d58ef2",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa4899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "& was unexpected at this time.\n"
     ]
    }
   ],
   "source": [
    "# Load Virtual Environment\n",
    "\n",
    "!& \"c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Scripts\\Activate.ps1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bda5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File handling / utilities\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Scikit-learn: preprocessing & pipelines\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Scikit-learn: models\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scikit-learn: model selection & metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, classification_report, confusion_matrix\n",
    "\n",
    "# Feature selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9a73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = 'C:/Users/tbran/Python/repos/Semester 3 Repos/capstone/'\n",
    "data_path = project_path + 'data/'\n",
    "src_path = project_path + 'src/'\n",
    "model_path = project_path + 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc5532",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7983db",
   "metadata": {},
   "source": [
    "## Week 1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(df, datetime_cols=None, fillna=False):\n",
    "    \"\"\"\n",
    "    Cleans and optimizes a DataFrame:\n",
    "    - Converts object datetime columns to datetime64\n",
    "    - Converts object columns with repeated values to category\n",
    "    - Downcasts numeric columns to smallest safe type\n",
    "    - Optionally fills NaNs before downcasting\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to optimize\n",
    "        datetime_cols (list): List of column names to convert to datetime\n",
    "        fillna (bool): If True, fills NaNs before downcasting\n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Convert datetime columns\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # 2. Convert object columns to category if appropriate\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < 0.5:  # heuristic: less than 50% unique\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # 3. Downcast numeric columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "    \n",
    "    for col in int_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in float_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    print(f\"Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_column_names(df, remove_accents=True):\n",
    "    \"\"\"\n",
    "    Cleans DataFrame column names:\n",
    "    - Strips whitespace\n",
    "    - Converts to lowercase\n",
    "    - Replaces spaces & special chars with underscores\n",
    "    - Removes duplicate underscores\n",
    "    - Optionally removes accents\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame whose columns to clean\n",
    "        remove_accents (bool): If True, strips accents from characters\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    def _clean(col):\n",
    "        col = col.strip().lower()\n",
    "        if remove_accents:\n",
    "            col = ''.join(\n",
    "                c for c in unicodedata.normalize('NFKD', col)\n",
    "                if not unicodedata.combining(c)\n",
    "            )\n",
    "        col = re.sub(r'[^0-9a-zA-Z]+', '_', col)  # replace non-alphanumeric with _\n",
    "        col = re.sub(r'_+', '_', col)             # collapse multiple underscores\n",
    "        col = col.strip('_')                      # remove leading/trailing underscores\n",
    "        return col\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.columns = [_clean(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline(df, target, \n",
    "                                  high_card_threshold=20, \n",
    "                                  scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for linear regression:\n",
    "    - One-hot encodes low-cardinality categorical columns\n",
    "    - Target encodes high-cardinality categorical columns\n",
    "    - Optionally scales numeric columns\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame (including target column)\n",
    "        target (str): Name of target column\n",
    "        high_card_threshold (int): Unique value cutoff for high-cardinality\n",
    "        scale_numeric (bool): Whether to scale numeric features\n",
    "        \n",
    "    Returns:\n",
    "        pipeline (ColumnTransformer): Preprocessing transformer\n",
    "        low_card_cols (list): Low-cardinality categorical columns\n",
    "        high_card_cols (list): High-cardinality categorical columns\n",
    "        num_cols (list): Numeric columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target])\n",
    "    \n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [col for col in cat_cols if X[col].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [col for col in cat_cols if X[col].nunique() > high_card_threshold]\n",
    "    \n",
    "    # Transformers\n",
    "    low_card_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    high_card_transformer = TargetEncoder()\n",
    "    num_transformer = StandardScaler() if scale_numeric else 'passthrough'\n",
    "    \n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', low_card_transformer, low_card_cols),\n",
    "            ('high_card', high_card_transformer, high_card_cols),\n",
    "            ('num', num_transformer, num_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor, low_card_cols, high_card_cols, num_cols\n",
    "\n",
    "\n",
    "\n",
    "def calculate_vif(df, features=None, vif_thresh=10.0):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) safely:\n",
    "    - Removes constant columns\n",
    "    - Removes perfectly collinear columns\n",
    "    - Returns sorted VIF table\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with numeric features\n",
    "        features (list): Optional list of features to check; defaults to all numeric\n",
    "        vif_thresh (float): Threshold for flagging high VIF\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: VIF table\n",
    "    \"\"\"\n",
    "    # Select numeric columns if features not provided\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # 1. Drop constant columns\n",
    "    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"Dropping constant columns: {constant_cols}\")\n",
    "        X.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    # 2. Drop perfectly collinear columns\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    perfect_corr_cols = [col for col in upper.columns if any(upper[col] == 1.0)]\n",
    "    if perfect_corr_cols:\n",
    "        print(f\"Dropping perfectly collinear columns: {perfect_corr_cols}\")\n",
    "        X.drop(columns=perfect_corr_cols, inplace=True)\n",
    "    \n",
    "    # 3. Calculate VIF\n",
    "    X_const = X.assign(const=1)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_const.values, i) for i in range(len(X.columns))]\n",
    "    })\n",
    "    \n",
    "    # 4. Sort by VIF\n",
    "    vif_data.sort_values(by=\"VIF\", ascending=False, inplace=True)\n",
    "    \n",
    "    # 5. Flag high VIF\n",
    "    vif_data[\"High_VIF\"] = vif_data[\"VIF\"] > vif_thresh\n",
    "    \n",
    "    return vif_data\n",
    "\n",
    "def fit_polynomial_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and returns the fitted model and transformed features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "def add_interaction_terms(df, features):\n",
    "    \"\"\"\n",
    "    Adds pairwise interaction terms between given features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    interaction_array = poly.fit_transform(df[features])\n",
    "    interaction_df = pd.DataFrame(interaction_array, columns=poly.get_feature_names_out(features))\n",
    "    return pd.concat([df.reset_index(drop=True), interaction_df], axis=1)\n",
    "\n",
    "def preprocess_features(df, categorical_cols, numeric_cols):\n",
    "    \"\"\"\n",
    "    Returns a ColumnTransformer that one-hot encodes categorical columns\n",
    "    and passes numeric columns through unchanged.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "            ('num', 'passthrough', numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def regression_summary(X, y):\n",
    "    \"\"\"\n",
    "    Fits an OLS regression model using statsmodels and prints the summary.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32311e0",
   "metadata": {},
   "source": [
    "# Week 1 Notebook – Linear Regression 1\n",
    "Each week, you will apply the concepts of that week to your Integrated Capstone Project’s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data, using the methods you have learned about in this course and in this program, and draw interesting conclusions. \n",
    "\n",
    "For Week 1, include concepts such as linear regression with polynomial terms, interaction terms, multicollinearity, variance inflation factor and regression, and categorical and continuous features. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209b52c",
   "metadata": {},
   "source": [
    "## Mendeley Delay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438b27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df = pd.read_csv(data_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1404dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n"
     ]
    }
   ],
   "source": [
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2046a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d437e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n",
      "R^2 score: 0.043766421823172585\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91cb0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature           VIF  High_VIF\n",
      "13      largehubairportdest           inf      True\n",
      "12     mediumhubairportdest           inf      True\n",
      "10        nonhubairportdest           inf      True\n",
      "11      smallhubairportdest           inf      True\n",
      "7     smallhubairportorigin           inf      True\n",
      "6       nonhubairportorigin           inf      True\n",
      "18        nonhubairlinedest           inf      True\n",
      "19      smallhubairlinedest           inf      True\n",
      "16   mediumhubairlineorigin           inf      True\n",
      "17    largehubairlineorigin           inf      True\n",
      "20     mediumhubairlinedest           inf      True\n",
      "21      largehubairlinedest           inf      True\n",
      "15    smallhubairlineorigin           inf      True\n",
      "14      nonhubairlineorigin           inf      True\n",
      "8    mediumhubairportorigin  9.007199e+15      True\n",
      "9     largehubairportorigin  9.007199e+15      True\n",
      "39               temp_20_30  3.112541e+02      True\n",
      "38               temp_10_20  2.888918e+02      True\n",
      "37                temp_0_10  1.916812e+02      True\n",
      "40               temp_30_40  1.151481e+02      True\n",
      "36               temp_n10_0  7.219096e+01      True\n",
      "34              temperature  1.429181e+01      True\n",
      "35          temp_ninfty_n10  1.244962e+01      True\n",
      "43          windspeedsquare  9.783338e+00     False\n",
      "42                windspeed  8.658978e+00     False\n",
      "1                  arrdelay  8.096892e+00     False\n",
      "0                  depdelay  8.012084e+00     False\n",
      "41            temp_40_infty  5.988440e+00     False\n",
      "3           marketsharedest  3.643296e+00     False\n",
      "2         marketshareorigin  3.596286e+00     False\n",
      "22                     year  3.081408e+00     False\n",
      "45            windgustspeed  2.712213e+00     False\n",
      "31               numflights  2.603665e+00     False\n",
      "5                   hhidest  2.493102e+00     False\n",
      "4                 hhiorigin  2.430416e+00     False\n",
      "44            windgustdummy  2.181984e+00     False\n",
      "33            monopolyroute  1.598653e+00     False\n",
      "32                 distance  1.564135e+00     False\n",
      "51  originmetrogdppercapita  1.519503e+00     False\n",
      "30               loadfactor  1.452184e+00     False\n",
      "53    destmetrogdppercapita  1.450741e+00     False\n",
      "29                 capacity  1.332612e+00     False\n",
      "50           originmetropop  1.326248e+00     False\n",
      "52             destmetropop  1.326226e+00     False\n",
      "49           snowtracedummy  1.181472e+00     False\n",
      "27          originairportid  1.173991e+00     False\n",
      "25                dayofweek  1.165972e+00     False\n",
      "28            destairportid  1.160076e+00     False\n",
      "48                snowdummy  1.105665e+00     False\n",
      "26            scheduledhour  1.084384e+00     False\n",
      "46                raindummy  1.061760e+00     False\n",
      "23                    month  1.048324e+00     False\n",
      "47           raintracedummy  1.024877e+00     False\n",
      "24               dayofmonth  1.000590e+00     False\n"
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(df)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f78e22",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96001c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\508701689.py:5: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18530e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 3779.75 MB\n",
      "Memory usage after optimization: 535.69 MB\n",
      "Reduced by 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d649ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4434d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n",
      "R^2 score: 0.06941058350132379\n"
     ]
    }
   ],
   "source": [
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "usdot_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "usdot_model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", usdot_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d909dbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping constant columns: ['year', 'flights']\n",
      "                    feature           VIF  High_VIF\n",
      "11          dest_airport_id  5.861101e+09      True\n",
      "12      dest_airport_seq_id  5.861097e+09      True\n",
      "6         origin_airport_id  5.727668e+09      True\n",
      "7     origin_airport_seq_id  5.727665e+09      True\n",
      "35      actual_elapsed_time  5.031438e+03      True\n",
      "36                 air_time  4.704893e+03      True\n",
      "29            arr_delay_new  7.782086e+02      True\n",
      "18                dep_delay  4.491302e+02      True\n",
      "19            dep_delay_new  3.941995e+02      True\n",
      "39            carrier_delay  2.318702e+02      True\n",
      "28                arr_delay  1.658624e+02      True\n",
      "43      late_aircraft_delay  1.573233e+02      True\n",
      "30                arr_del15  1.516176e+02      True\n",
      "34         crs_elapsed_time  1.389059e+02      True\n",
      "22                 taxi_out  1.003038e+02      True\n",
      "37                 distance  8.611603e+01      True\n",
      "41                nas_delay  5.612771e+01      True\n",
      "40            weather_delay  4.864794e+01      True\n",
      "25                  taxi_in  4.525602e+01      True\n",
      "38           distance_group  3.630979e+01      True\n",
      "31          arr_delay_group  3.452216e+01      True\n",
      "21          dep_delay_group  3.050542e+01      True\n",
      "45          total_add_gtime  2.539939e+01      True\n",
      "46        longest_add_gtime  2.533810e+01      True\n",
      "17                 dep_time  2.056231e+01      True\n",
      "23               wheels_off  1.390820e+01      True\n",
      "24                wheels_on  1.338470e+01      True\n",
      "27                 arr_time  1.226734e+01      True\n",
      "16             crs_dep_time  8.976904e+00     False\n",
      "50            div_arr_delay  8.089041e+00     False\n",
      "49  div_actual_elapsed_time  7.906063e+00     False\n",
      "20                dep_del15  4.196562e+00     False\n",
      "1                     month  3.894604e+00     False\n",
      "0                   quarter  3.862039e+00     False\n",
      "26             crs_arr_time  3.665930e+00     False\n",
      "10               origin_wac  1.894548e+00     False\n",
      "13      dest_city_market_id  1.824195e+00     False\n",
      "15                 dest_wac  1.780173e+00     False\n",
      "8     origin_city_market_id  1.757761e+00     False\n",
      "42           security_delay  1.619792e+00     False\n",
      "4     op_carrier_airline_id  1.436409e+00     False\n",
      "33                 diverted  1.398590e+00     False\n",
      "5         op_carrier_fl_num  1.397663e+00     False\n",
      "48         div_reached_dest  1.339445e+00     False\n",
      "47     div_airport_landings  1.337987e+00     False\n",
      "51             div_distance  1.335226e+00     False\n",
      "32                cancelled  1.113056e+00     False\n",
      "9         origin_state_fips  1.079831e+00     False\n",
      "14          dest_state_fips  1.054488e+00     False\n",
      "44           first_dep_time  1.030265e+00     False\n",
      "2              day_of_month  1.002750e+00     False\n",
      "3               day_of_week  1.002279e+00     False\n"
     ]
    }
   ],
   "source": [
    "vif_table = calculate_vif(usdot_df)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68d61f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: df\n",
      "Dropped DataFrame: df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n",
      "Dropped DataFrame: combined_df\n",
      "Dropped DataFrame: usdot_df\n",
      "Dropped DataFrame: usdot_df_lin\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9f2ac",
   "metadata": {},
   "source": [
    "# Week 2 Notebook - Linear Regression 2\n",
    "\n",
    "For Week 2, include concepts such as linear regression with lasso, ridge, and elastic net regression. This homework will be submitted for peer review and feedback in Week 3 in the assignment titled 3.4 Peer Review: Week 2 Jupyter Notebook. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8fd697",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4b684",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada4666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n",
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n"
     ]
    }
   ],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "df = pd.read_csv(data_path + file_name)\n",
    "\n",
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a18f",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0be88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.0337074407107556\n"
     ]
    }
   ],
   "source": [
    "# Default Lasso \n",
    "\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lasso)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "491616f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.01\n",
      "Best CV R^2: 0.04403471123674032\n",
      "Test R^2: 0.04379222831327789\n"
     ]
    }
   ],
   "source": [
    "# Lasso with tuned alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6d279",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c167ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha): 0.04376642307060696\n"
     ]
    }
   ],
   "source": [
    "#Ridge with default alpha\n",
    "\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ridge)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default alpha\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha):\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c19aaf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 1000\n",
      "Best CV R^2: 0.043972332094467756\n",
      "Test R^2: 0.04377132510993642\n"
     ]
    }
   ],
   "source": [
    "# Ridge with tune alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c99e1",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a201c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha, l1_ratio): 0.033545260335402505\n"
     ]
    }
   ],
   "source": [
    "# Replace regressor with ElasticNet\n",
    "elastic = ElasticNet(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', elastic)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default parameters\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha, l1_ratio):\", model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a67925",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m param_grid = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.001\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m10\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__l1_ratio\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.1\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.9\u001b[39m]\n\u001b[32m      5\u001b[39m }\n\u001b[32m      7\u001b[39m grid = GridSearchCV(model, param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest CV R^2:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Option 2: Tune alpha and l1_ratio with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19e44f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: df\n",
      "Dropped DataFrame: df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b555ac",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b697bc",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3be53614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_15444\\1136470253.py:5: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n",
      "Memory usage before optimization: 3779.75 MB\n",
      "Memory usage after optimization: 535.69 MB\n",
      "Reduced by 85.8%\n",
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ced8a3",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaf2fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score: 0.04595741706259315\n"
     ]
    }
   ],
   "source": [
    "# Default Lasso \n",
    "\n",
    "lasso = Lasso(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lasso)\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f56dd8ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m param_grid = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.001\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m100\u001b[39m]\n\u001b[32m      4\u001b[39m }\n\u001b[32m      6\u001b[39m grid = GridSearchCV(model, param_grid, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest alpha:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_params_[\u001b[33m'\u001b[39m\u001b[33mregressor__alpha\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest CV R^2:\u001b[39m\u001b[33m\"\u001b[39m, grid.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Lasso with tuned alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a743c98",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e5da7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha): 0.06941067445046167\n"
     ]
    }
   ],
   "source": [
    "#Ridge with default alpha\n",
    "\n",
    "ridge = Ridge(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ridge)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default alpha\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha):\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fa346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with tune alpha with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_['regressor__alpha'])\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba5f6e",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddfe0b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score (default alpha, l1_ratio): 0.04155075820659526\n"
     ]
    }
   ],
   "source": [
    "# Replace regressor with ElasticNet\n",
    "elastic = ElasticNet(max_iter=10000, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', elastic)\n",
    "])\n",
    "\n",
    "# Option 1: Fit with default parameters\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R^2 score (default alpha, l1_ratio):\", model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19977e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Tune alpha and l1_ratio with cross-validation\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'regressor__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "print(\"Test R^2:\", grid.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f361c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped DataFrame: combined_df\n",
      "Dropped DataFrame: usdot_df\n",
      "Dropped DataFrame: usdot_df_lin\n",
      "Dropped DataFrame: X\n",
      "Dropped DataFrame: X_train\n",
      "Dropped DataFrame: X_test\n"
     ]
    }
   ],
   "source": [
    "# Remove large datasets\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and len(obj) > 10000:\n",
    "        del globals()[name]\n",
    "        print(f\"Dropped DataFrame: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472fe2",
   "metadata": {},
   "source": [
    "# Week 3 Notebook - Linear Regression 3\n",
    "\n",
    "For Week 3, include concepts such as linear regression with forward and backward selection, PCR, and PLSR. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04655a18",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d428c74",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "Added a sampling step because this is taking way too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b99d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n",
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\3100718249.py:20: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby('depdelay_bin', group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['uniquecarrier']\n",
      "High-cardinality categorical: ['origin', 'dest', 'tailnum', 'origincityname', 'originstate']\n",
      "Numeric columns: ['marketshareorigin', 'marketsharedest', 'hhiorigin', 'hhidest', 'nonhubairportorigin', 'smallhubairportorigin', 'mediumhubairportorigin', 'largehubairportorigin', 'nonhubairportdest', 'smallhubairportdest', 'mediumhubairportdest', 'largehubairportdest', 'nonhubairlineorigin', 'smallhubairlineorigin', 'mediumhubairlineorigin', 'largehubairlineorigin', 'nonhubairlinedest', 'smallhubairlinedest', 'mediumhubairlinedest', 'largehubairlinedest', 'year', 'month', 'dayofmonth', 'dayofweek', 'scheduledhour', 'capacity', 'loadfactor', 'numflights', 'distance', 'monopolyroute', 'temperature', 'temp_ninfty_n10', 'temp_n10_0', 'temp_0_10', 'temp_10_20', 'temp_20_30', 'temp_30_40', 'temp_40_infty', 'windspeed', 'windspeedsquare', 'windgustdummy', 'windgustspeed', 'raindummy', 'raintracedummy', 'snowdummy', 'snowtracedummy', 'originmetropop', 'originmetrogdppercapita', 'destmetropop', 'destmetrogdppercapita']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\3100718249.py:21: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(\n"
     ]
    }
   ],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "SAMPLE_SIZE = 50000\n",
    "df = pd.read_csv(data_path + file_name)\n",
    "\n",
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)\n",
    "\n",
    "if SAMPLE_SIZE:\n",
    "    bins = [-np.inf, -1, 0, 15, 60, 180, np.inf]\n",
    "    labels = ['early', 'on_time', 'small_delay', 'moderate_delay', 'long_delay', 'extreme_delay']\n",
    "\n",
    "    df['depdelay_bin'] = pd.cut(df['depdelay'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "    df = (\n",
    "        df.groupby('depdelay_bin', group_keys=False)\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=min(int(SAMPLE_SIZE * len(x) / len(df)), len(x)), \n",
    "            random_state=42\n",
    "        ))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=['depdelay_bin'])\n",
    "\n",
    "\n",
    "# Get column categories\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = df_lin.drop(columns=['depdelay'])\n",
    "y = df_lin['depdelay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e762f",
   "metadata": {},
   "source": [
    "### Forward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908d8d7",
   "metadata": {},
   "source": [
    "#### Using SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47503042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['low_card__uniquecarrier_9E', 'low_card__uniquecarrier_B6', 'low_card__uniquecarrier_CO', 'low_card__uniquecarrier_DL', 'low_card__uniquecarrier_FL', 'low_card__uniquecarrier_MQ', 'low_card__uniquecarrier_UA', 'low_card__uniquecarrier_XE', 'high_card__dest', 'high_card__tailnum', 'high_card__origincityname', 'high_card__originstate', 'num__smallhubairportorigin', 'num__nonhubairportdest', 'num__largehubairportdest', 'num__year', 'num__month', 'num__dayofmonth', 'num__dayofweek', 'num__scheduledhour', 'num__loadfactor', 'num__numflights', 'num__temperature', 'num__temp_ninfty_n10', 'num__temp_n10_0', 'num__windspeedsquare', 'num__windgustdummy', 'num__raindummy', 'num__raintracedummy', 'num__snowdummy', 'num__snowtracedummy', 'num__destmetrogdppercapita']\n",
      "Model Accuracy (R²): -0.0439\n",
      "Model RMSE: 1372.6907\n"
     ]
    }
   ],
   "source": [
    "# Fit preprocessor with y (important for supervised encoders)\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "X_train_trans = preprocessor.transform(X_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "sfs_forward = SFS(\n",
    "    lin_reg,\n",
    "    k_features='best',\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs_forward = sfs_forward.fit(X_train_trans, y_train)\n",
    "\n",
    "# Map indices back to feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "selected_features = [feature_names[i] for i in sfs_forward.k_feature_idx_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# --- Evaluate model accuracy ---\n",
    "# Restrict to selected features\n",
    "X_train_sel = X_train_trans[:, sfs_forward.k_feature_idx_]\n",
    "X_test_sel  = X_test_trans[:, sfs_forward.k_feature_idx_]\n",
    "\n",
    "# Fit final model\n",
    "lin_reg.fit(X_train_sel, y_train)\n",
    "y_pred = lin_reg.predict(X_test_sel)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adce31f",
   "metadata": {},
   "source": [
    "#### Using Stepwise Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe527202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add high_card__tailnum             with p-value 0.0\n",
      "Add num__scheduledhour             with p-value 1.07019e-156\n",
      "Add num__raindummy                 with p-value 7.40738e-87\n",
      "Add high_card__dest                with p-value 6.44347e-59\n",
      "Add high_card__origincityname      with p-value 6.17954e-47\n",
      "Add num__snowdummy                 with p-value 6.48087e-32\n",
      "Add num__snowtracedummy            with p-value 3.82627e-16\n",
      "Add num__numflights                with p-value 7.88944e-15\n",
      "Add num__windgustdummy             with p-value 1.34108e-14\n",
      "Add num__year                      with p-value 1.63149e-13\n",
      "Add num__largehubairportdest       with p-value 5.15059e-08\n",
      "Add num__raintracedummy            with p-value 3.68001e-07\n",
      "Add high_card__originstate         with p-value 4.61725e-06\n",
      "Add num__temp_n10_0                with p-value 5.76293e-05\n",
      "Add num__windspeedsquare           with p-value 0.000150945\n",
      "Add num__destmetrogdppercapita     with p-value 0.000336916\n",
      "Add num__dayofmonth                with p-value 0.00165799\n",
      "Add low_card__uniquecarrier_B6     with p-value 0.00200597\n",
      "Add num__temp_ninfty_n10           with p-value 0.00287058\n",
      "Add num__loadfactor                with p-value 0.00418016\n",
      "Forward-selected features: ['high_card__tailnum', 'num__scheduledhour', 'num__raindummy', 'high_card__dest', 'high_card__origincityname', 'num__snowdummy', 'num__snowtracedummy', 'num__numflights', 'num__windgustdummy', 'num__year', 'num__largehubairportdest', 'num__raintracedummy', 'high_card__originstate', 'num__temp_n10_0', 'num__windspeedsquare', 'num__destmetrogdppercapita', 'num__dayofmonth', 'low_card__uniquecarrier_B6', 'num__temp_ninfty_n10', 'num__loadfactor']\n",
      "Forward Selection Model Accuracy (R²): -0.0439\n",
      "Forward Selection Model RMSE: 1372.7035\n"
     ]
    }
   ],
   "source": [
    "def forward_selection(X, y, threshold_in=0.01, verbose=True):\n",
    "    \"\"\"Forward selection based on p-values from statsmodels OLS\"\"\"\n",
    "    included = []\n",
    "    while True:\n",
    "        changed = False\n",
    "        excluded = list(set(X.columns) - set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype=float)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min() if not new_pval.empty else None\n",
    "        if best_pval is not None and best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f'Add {best_feature:30} with p-value {best_pval:.6}')\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "# Transform train/test into numeric DataFrames\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "X_train_trans = pd.DataFrame(\n",
    "    preprocessor.transform(X_train),\n",
    "    columns=preprocessor.get_feature_names_out(),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_trans = pd.DataFrame(\n",
    "    preprocessor.transform(X_test),\n",
    "    columns=preprocessor.get_feature_names_out(),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Run forward selection\n",
    "forward_features = forward_selection(X_train_trans, y_train)\n",
    "print(\"Forward-selected features:\", forward_features)\n",
    "\n",
    "# Fit final model\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train_trans[forward_features])).fit()\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = final_model.predict(sm.add_constant(X_test_trans[forward_features]))\n",
    "\n",
    "# Evaluate accuracy\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Forward Selection Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Forward Selection Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb90c3",
   "metadata": {},
   "source": [
    "### Backward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e30443e",
   "metadata": {},
   "source": [
    "#### Using SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83900a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward-selected features: ['low_card__uniquecarrier_AS', 'low_card__uniquecarrier_B6', 'low_card__uniquecarrier_CO', 'low_card__uniquecarrier_OO', 'low_card__uniquecarrier_US', 'low_card__uniquecarrier_WN', 'low_card__uniquecarrier_XE', 'high_card__dest', 'high_card__tailnum', 'high_card__origincityname', 'high_card__originstate', 'num__smallhubairportorigin', 'num__mediumhubairportdest', 'num__largehubairportdest', 'num__year', 'num__month', 'num__dayofmonth', 'num__dayofweek', 'num__scheduledhour', 'num__loadfactor', 'num__numflights', 'num__temperature', 'num__temp_0_10', 'num__temp_10_20', 'num__temp_20_30', 'num__windspeedsquare', 'num__windgustdummy', 'num__raindummy', 'num__raintracedummy', 'num__snowdummy', 'num__snowtracedummy', 'num__destmetrogdppercapita']\n",
      "Backward Selection Model Accuracy (R²): -0.0439\n",
      "Backward Selection Model RMSE: 1372.7010\n"
     ]
    }
   ],
   "source": [
    "# --- Backward selection ---\n",
    "sfs_backward = SFS(\n",
    "    lin_reg,\n",
    "    k_features='best',\n",
    "    forward=False,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Important: fit preprocessor with y\n",
    "preprocessor.fit(X_train, y_train)\n",
    "X_train_trans = preprocessor.transform(X_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "sfs_backward = sfs_backward.fit(X_train_trans, y_train)\n",
    "\n",
    "# Map indices back to feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "selected_features = [feature_names[i] for i in sfs_backward.k_feature_idx_]\n",
    "print(\"Backward-selected features:\", selected_features)\n",
    "\n",
    "# --- Evaluate model accuracy ---\n",
    "# Restrict to selected features\n",
    "X_train_sel = X_train_trans[:, sfs_backward.k_feature_idx_]\n",
    "X_test_sel  = X_test_trans[:, sfs_backward.k_feature_idx_]\n",
    "\n",
    "# Fit final model\n",
    "lin_reg.fit(X_train_sel, y_train)\n",
    "y_pred = lin_reg.predict(X_test_sel)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Backward Selection Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Backward Selection Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980ce9c",
   "metadata": {},
   "source": [
    "#### Using Stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eedfe564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop num__distance                  with p-value 0.994829\n",
      "Drop num__largehubairlinedest       with p-value 0.966156\n",
      "Drop num__nonhubairlinedest         with p-value 0.999495\n",
      "Drop num__originmetrogdppercapita   with p-value 0.938646\n",
      "Drop num__hhidest                   with p-value 0.934772\n",
      "Drop num__windgustspeed             with p-value 0.929315\n",
      "Drop num__mediumhubairportorigin    with p-value 0.920231\n",
      "Drop high_card__origin              with p-value 0.87493\n",
      "Drop low_card__uniquecarrier_WN     with p-value 0.852824\n",
      "Drop low_card__uniquecarrier_F9     with p-value 0.897424\n",
      "Drop low_card__uniquecarrier_US     with p-value 0.85234\n",
      "Drop num__mediumhubairlinedest      with p-value 0.833208\n",
      "Drop num__nonhubairportorigin       with p-value 0.828218\n",
      "Drop low_card__uniquecarrier_YV     with p-value 0.812314\n",
      "Drop num__hhiorigin                 with p-value 0.666242\n",
      "Drop num__marketshareorigin         with p-value 0.838139\n",
      "Drop low_card__uniquecarrier_AS     with p-value 0.678525\n",
      "Drop num__capacity                  with p-value 0.670199\n",
      "Drop num__smallhubairlineorigin     with p-value 0.637363\n",
      "Drop num__mediumhubairlineorigin    with p-value 0.996565\n",
      "Drop num__largehubairlineorigin     with p-value 0.819745\n",
      "Drop low_card__uniquecarrier_AA     with p-value 0.574367\n",
      "Drop num__destmetropop              with p-value 0.582364\n",
      "Drop num__mediumhubairportdest      with p-value 0.583266\n",
      "Drop num__smallhubairportdest       with p-value 0.577486\n",
      "Drop num__smallhubairlinedest       with p-value 0.551636\n",
      "Drop low_card__uniquecarrier_HA     with p-value 0.545089\n",
      "Drop low_card__uniquecarrier_MQ     with p-value 0.524251\n",
      "Drop low_card__uniquecarrier_OH     with p-value 0.509704\n",
      "Drop num__monopolyroute             with p-value 0.437354\n",
      "Drop num__windspeed                 with p-value 0.338178\n",
      "Drop low_card__uniquecarrier_EV     with p-value 0.303229\n",
      "Drop low_card__uniquecarrier_NW     with p-value 0.299041\n",
      "Drop num__largehubairportorigin     with p-value 0.311789\n",
      "Drop num__nonhubairlineorigin       with p-value 0.441396\n",
      "Drop low_card__uniquecarrier_FL     with p-value 0.301925\n",
      "Drop low_card__uniquecarrier_DL     with p-value 0.356621\n",
      "Drop num__originmetropop            with p-value 0.253863\n",
      "Drop num__temp_40_infty             with p-value 0.15899\n",
      "Drop num__temp_ninfty_n10           with p-value 0.500802\n",
      "Drop num__temp_30_40                with p-value 0.407732\n",
      "Drop num__temp_n10_0                with p-value 0.546831\n",
      "Drop num__marketsharedest           with p-value 0.119029\n",
      "Drop num__nonhubairportdest         with p-value 0.104067\n",
      "Drop low_card__uniquecarrier_OO     with p-value 0.112514\n",
      "Drop num__month                     with p-value 0.0817786\n",
      "Drop num__smallhubairportorigin     with p-value 0.0553531\n",
      "Backward-selected features: ['low_card__uniquecarrier_9E', 'low_card__uniquecarrier_B6', 'low_card__uniquecarrier_CO', 'low_card__uniquecarrier_UA', 'low_card__uniquecarrier_XE', 'high_card__dest', 'high_card__tailnum', 'high_card__origincityname', 'high_card__originstate', 'num__largehubairportdest', 'num__year', 'num__dayofmonth', 'num__dayofweek', 'num__scheduledhour', 'num__loadfactor', 'num__numflights', 'num__temperature', 'num__temp_0_10', 'num__temp_10_20', 'num__temp_20_30', 'num__windspeedsquare', 'num__windgustdummy', 'num__raindummy', 'num__raintracedummy', 'num__snowdummy', 'num__snowtracedummy', 'num__destmetrogdppercapita']\n",
      "Backward Elimination Model Accuracy (R²): -0.0440\n",
      "Backward Elimination Model RMSE: 1372.8699\n"
     ]
    }
   ],
   "source": [
    "def backward_elimination(X, y, threshold_out=0.05, verbose=True):\n",
    "    \"\"\"Backward elimination based on p-values from statsmodels OLS.\n",
    "       Assumes X is a numeric DataFrame (already preprocessed).\"\"\"\n",
    "    included = list(X.columns)\n",
    "    while True:\n",
    "        changed = False\n",
    "        model = sm.OLS(y, sm.add_constant(X[included])).fit()\n",
    "        # exclude intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() if not pvalues.empty else None\n",
    "        if worst_pval is not None and worst_pval > threshold_out:\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f'Drop {worst_feature:30} with p-value {worst_pval:.6}')\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "# --- Transform train/test into numeric DataFrames ---\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "X_train_trans = pd.DataFrame(\n",
    "    preprocessor.transform(X_train),\n",
    "    columns=preprocessor.get_feature_names_out(),\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_trans = pd.DataFrame(\n",
    "    preprocessor.transform(X_test),\n",
    "    columns=preprocessor.get_feature_names_out(),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# --- Run backward elimination on transformed data ---\n",
    "backward_features = backward_elimination(X_train_trans, y_train)\n",
    "print(\"Backward-selected features:\", backward_features)\n",
    "\n",
    "# --- Fit final model on selected features ---\n",
    "final_model = sm.OLS(y_train, sm.add_constant(X_train_trans[backward_features])).fit()\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = final_model.predict(sm.add_constant(X_test_trans[backward_features]))\n",
    "\n",
    "# --- Evaluate accuracy ---\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Backward Elimination Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Backward Elimination Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0461f2",
   "metadata": {},
   "source": [
    "### PCR\n",
    "Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f9a7788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCR R2: -0.07829785712740689\n",
      "PCR RMSE: 1417.9651431890868\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline: preprocessing → PCA → Linear Regression\n",
    "pcr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=20)),   # choose number of components\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pcr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pcr = pcr.predict(X_test)\n",
    "\n",
    "print(\"PCR R2:\", r2_score(y_test, y_pred_pcr))\n",
    "print(\"PCR RMSE:\", mean_squared_error(y_test, y_pred_pcr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a9361",
   "metadata": {},
   "source": [
    "### PLSR\n",
    "Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02303914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLSR R2: -0.04399321509843657\n",
      "PLSR RMSE: 1372.854428811666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess first\n",
    "X_train_trans = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "# Fit PLSR with, say, 10 components\n",
    "plsr = PLSRegression(n_components=10)\n",
    "plsr.fit(X_train_trans, y_train)\n",
    "\n",
    "y_pred_plsr = plsr.predict(X_test_trans)\n",
    "\n",
    "print(\"PLSR R2:\", r2_score(y_test, y_pred_plsr))\n",
    "print(\"PLSR RMSE:\", mean_squared_error(y_test, y_pred_plsr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931253e",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436fe92",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ec6a890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n",
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:6: DtypeWarning: Columns (77,84,85,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs = [pd.read_csv(f) for f in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined: 5\n",
      "Final shape: (2906929, 109)\n",
      "Memory usage before optimization: 3779.75 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\3367681747.py:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization: 552.31 MB\n",
      "Reduced by 85.4%\n",
      "Sampling the dataset to 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  usdot_df.groupby('depdelay_bin', group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality categorical: ['op_unique_carrier', 'op_carrier', 'dep_time_blk', 'arr_time_blk', 'cancellation_code']\n",
      "High-cardinality categorical: ['tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm']\n",
      "Numeric columns: ['year', 'quarter', 'month', 'day_of_month', 'day_of_week', 'op_carrier_fl_num', 'dep_time', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled', 'diverted', 'crs_elapsed_time', 'actual_elapsed_time', 'air_time', 'flights', 'distance', 'distance_group', 'first_dep_time', 'total_add_gtime', 'longest_add_gtime', 'div_airport_landings', 'div_reached_dest', 'div_actual_elapsed_time', 'div_arr_delay', 'div_distance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_39576\\426218808.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(\n"
     ]
    }
   ],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "SAMPLE_SIZE = 50000\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "combined_df.columns = combined_df.columns.str.lower()\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)\n",
    "\n",
    "if SAMPLE_SIZE >= 0:\n",
    "    print(\"Sampling the dataset to\", SAMPLE_SIZE)\n",
    "    bins = [-np.inf, -1, 0, 15, 60, 180, np.inf]\n",
    "    labels = ['early', 'on_time', 'small_delay', 'moderate_delay', 'long_delay', 'extreme_delay']\n",
    "\n",
    "    usdot_df['depdelay_bin'] = pd.cut(usdot_df['dep_delay'], bins=bins, labels=labels)\n",
    "    usdot_df = (\n",
    "        usdot_df.groupby('depdelay_bin', group_keys=False)\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=min(int(SAMPLE_SIZE * len(x) / len(usdot_df)), len(x)), \n",
    "            random_state=42\n",
    "        ))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    usdot_df = usdot_df.drop(columns=['depdelay_bin'])\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2984b",
   "metadata": {},
   "source": [
    "### Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e9b7158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 out of 107 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 14:42:18] Features: 1/107 -- score: 0.14339819239562326[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 106 out of 106 | elapsed:    0.2s finished\n",
      "\n",
      "[2025-09-28 14:42:18] Features: 2/107 -- score: 0.15745076680487888[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  74 out of 105 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:    0.3s finished\n",
      "\n",
      "[2025-09-28 14:42:19] Features: 3/107 -- score: 0.1637377783872315[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 104 | elapsed:    0.4s finished\n",
      "\n",
      "[2025-09-28 14:42:20] Features: 4/107 -- score: 0.16931774650750123[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of 103 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 103 out of 103 | elapsed:    0.4s finished\n",
      "\n",
      "[2025-09-28 14:42:20] Features: 5/107 -- score: 0.17181008494434805[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 102 out of 102 | elapsed:    0.5s finished\n",
      "\n",
      "[2025-09-28 14:42:21] Features: 6/107 -- score: 0.173528548453462[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of 101 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:    0.5s finished\n",
      "\n",
      "[2025-09-28 14:42:22] Features: 7/107 -- score: 0.17489180135475366[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "\n",
      "[2025-09-28 14:42:23] Features: 8/107 -- score: 0.17614373064544017[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  99 | elapsed:    0.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  99 out of  99 | elapsed:    0.7s finished\n",
      "\n",
      "[2025-09-28 14:42:23] Features: 9/107 -- score: 0.1770294126956441[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  98 out of  98 | elapsed:    0.7s finished\n",
      "\n",
      "[2025-09-28 14:42:24] Features: 10/107 -- score: 0.1778421002218102[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  97 | elapsed:    0.6s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  97 out of  97 | elapsed:    0.8s finished\n",
      "\n",
      "[2025-09-28 14:42:25] Features: 11/107 -- score: 0.17863027688559333[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:    0.9s finished\n",
      "\n",
      "[2025-09-28 14:42:27] Features: 12/107 -- score: 0.17935142037678467[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  95 | elapsed:    0.7s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  95 out of  95 | elapsed:    1.0s finished\n",
      "\n",
      "[2025-09-28 14:42:28] Features: 13/107 -- score: 0.18000184972448852[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  94 | elapsed:    0.7s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  94 out of  94 | elapsed:    1.0s finished\n",
      "\n",
      "[2025-09-28 14:42:29] Features: 14/107 -- score: 0.18045163479743365[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of  93 | elapsed:    1.1s finished\n",
      "\n",
      "[2025-09-28 14:42:30] Features: 15/107 -- score: 0.18076390323995964[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of  92 | elapsed:    1.2s finished\n",
      "\n",
      "[2025-09-28 14:42:32] Features: 16/107 -- score: 0.18105903961729564[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  91 | elapsed:    0.9s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  91 out of  91 | elapsed:    1.2s finished\n",
      "\n",
      "[2025-09-28 14:42:33] Features: 17/107 -- score: 0.18133191447235533[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    1.3s finished\n",
      "\n",
      "[2025-09-28 14:42:35] Features: 18/107 -- score: 0.18159770191334768[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  89 out of  89 | elapsed:    1.4s finished\n",
      "\n",
      "[2025-09-28 14:42:36] Features: 19/107 -- score: 0.18185296033874898[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  88 out of  88 | elapsed:    1.4s finished\n",
      "\n",
      "[2025-09-28 14:42:38] Features: 20/107 -- score: 0.1820462482122069[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  87 out of  87 | elapsed:    1.5s finished\n",
      "\n",
      "[2025-09-28 14:42:40] Features: 21/107 -- score: 0.1822449298180305[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  86 out of  86 | elapsed:    1.5s finished\n",
      "\n",
      "[2025-09-28 14:42:41] Features: 22/107 -- score: 0.182457142663404[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  85 | elapsed:    1.6s finished\n",
      "\n",
      "[2025-09-28 14:42:43] Features: 23/107 -- score: 0.18271812298850687[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed:    1.6s finished\n",
      "\n",
      "[2025-09-28 14:42:45] Features: 24/107 -- score: 0.18303714073115676[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  83 out of  83 | elapsed:    1.7s finished\n",
      "\n",
      "[2025-09-28 14:42:47] Features: 25/107 -- score: 0.18348973820138859[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  82 out of  82 | elapsed:    1.7s finished\n",
      "\n",
      "[2025-09-28 14:42:49] Features: 26/107 -- score: 0.18366088569604003[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    1.9s finished\n",
      "\n",
      "[2025-09-28 14:42:51] Features: 27/107 -- score: 0.1838053004187663[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    1.8s finished\n",
      "\n",
      "[2025-09-28 14:42:53] Features: 28/107 -- score: 0.18393029278801853[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  79 out of  79 | elapsed:    2.0s finished\n",
      "\n",
      "[2025-09-28 14:42:55] Features: 29/107 -- score: 0.18415340358757892[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  78 out of  78 | elapsed:    2.1s finished\n",
      "\n",
      "[2025-09-28 14:42:58] Features: 30/107 -- score: 0.18458365690942513[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  77 out of  77 | elapsed:    2.2s finished\n",
      "\n",
      "[2025-09-28 14:43:00] Features: 31/107 -- score: 0.1848443920416316[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  76 out of  76 | elapsed:    2.3s finished\n",
      "\n",
      "[2025-09-28 14:43:03] Features: 32/107 -- score: 0.18496062032265323[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    2.4s finished\n",
      "\n",
      "[2025-09-28 14:43:05] Features: 33/107 -- score: 0.185077784818746[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  74 out of  74 | elapsed:    2.4s finished\n",
      "\n",
      "[2025-09-28 14:43:08] Features: 34/107 -- score: 0.18518645322199925[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  73 out of  73 | elapsed:    2.7s finished\n",
      "\n",
      "[2025-09-28 14:43:11] Features: 35/107 -- score: 0.1852917145510241[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    2.6s finished\n",
      "\n",
      "[2025-09-28 14:43:14] Features: 36/107 -- score: 0.185391919005793[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of  71 | elapsed:    3.0s finished\n",
      "\n",
      "[2025-09-28 14:43:17] Features: 37/107 -- score: 0.18547070249011965[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    3.2s finished\n",
      "\n",
      "[2025-09-28 14:43:20] Features: 38/107 -- score: 0.18554626139127128[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  69 | elapsed:    3.3s finished\n",
      "\n",
      "[2025-09-28 14:43:24] Features: 39/107 -- score: 0.1856183870226886[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  68 | elapsed:    3.4s finished\n",
      "\n",
      "[2025-09-28 14:43:27] Features: 40/107 -- score: 0.18567991850034515[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  67 out of  67 | elapsed:    3.7s finished\n",
      "\n",
      "[2025-09-28 14:43:31] Features: 41/107 -- score: 0.18572281087640333[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  66 | elapsed:    3.7s finished\n",
      "\n",
      "[2025-09-28 14:43:35] Features: 42/107 -- score: 0.1857581012364143[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:    4.1s finished\n",
      "\n",
      "[2025-09-28 14:43:39] Features: 43/107 -- score: 0.1857892053160228[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:    4.1s finished\n",
      "\n",
      "[2025-09-28 14:43:44] Features: 44/107 -- score: 0.18585033977537768[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed:    4.5s finished\n",
      "\n",
      "[2025-09-28 14:43:48] Features: 45/107 -- score: 0.1859635771945406[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  62 out of  62 | elapsed:    4.8s finished\n",
      "\n",
      "[2025-09-28 14:43:53] Features: 46/107 -- score: 0.18598674364699655[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  61 out of  61 | elapsed:    5.1s finished\n",
      "\n",
      "[2025-09-28 14:43:59] Features: 47/107 -- score: 0.18600797947547873[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    5.1s finished\n",
      "\n",
      "[2025-09-28 14:44:04] Features: 48/107 -- score: 0.1860280317620513[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  59 out of  59 | elapsed:    5.3s finished\n",
      "\n",
      "[2025-09-28 14:44:10] Features: 49/107 -- score: 0.1861470325495228[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  58 out of  58 | elapsed:    5.8s finished\n",
      "\n",
      "[2025-09-28 14:44:16] Features: 50/107 -- score: 0.18626371125405775[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  57 | elapsed:    6.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:    6.0s finished\n",
      "\n",
      "[2025-09-28 14:44:22] Features: 51/107 -- score: 0.1863354657607231[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  56 | elapsed:    6.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  56 | elapsed:    6.2s finished\n",
      "\n",
      "[2025-09-28 14:44:28] Features: 52/107 -- score: 0.18639236044978694[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  55 | elapsed:    6.3s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    6.4s finished\n",
      "\n",
      "[2025-09-28 14:44:35] Features: 53/107 -- score: 0.1865280339511047[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  54 | elapsed:    6.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    6.5s finished\n",
      "\n",
      "[2025-09-28 14:44:42] Features: 54/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  53 | elapsed:    6.5s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  53 | elapsed:    6.5s finished\n",
      "\n",
      "[2025-09-28 14:44:48] Features: 55/107 -- score: 0.18657535826972735[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  52 | elapsed:    6.7s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:    6.9s finished\n",
      "\n",
      "[2025-09-28 14:44:55] Features: 56/107 -- score: 0.1865753582697274[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  51 | elapsed:    7.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:    7.2s finished\n",
      "\n",
      "[2025-09-28 14:45:03] Features: 57/107 -- score: 0.18657535826972735[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  50 | elapsed:    7.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    7.2s finished\n",
      "\n",
      "[2025-09-28 14:45:10] Features: 58/107 -- score: 0.1865753582697273[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  49 | elapsed:    7.5s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  49 | elapsed:    7.5s finished\n",
      "\n",
      "[2025-09-28 14:45:18] Features: 59/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  48 | elapsed:    7.4s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    7.5s finished\n",
      "\n",
      "[2025-09-28 14:45:26] Features: 60/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  47 | elapsed:    7.5s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  47 | elapsed:    7.7s finished\n",
      "\n",
      "[2025-09-28 14:45:33] Features: 61/107 -- score: 0.1865753582697273[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  46 | elapsed:    7.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  46 | elapsed:    7.7s finished\n",
      "\n",
      "[2025-09-28 14:45:41] Features: 62/107 -- score: 0.18657535826972727[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  45 | elapsed:    7.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    7.8s finished\n",
      "\n",
      "[2025-09-28 14:45:49] Features: 63/107 -- score: 0.18657535826972724[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  44 | elapsed:    7.9s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:    8.0s finished\n",
      "\n",
      "[2025-09-28 14:45:58] Features: 64/107 -- score: 0.18657535826972735[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  43 | elapsed:    7.6s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  43 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:46:06] Features: 65/107 -- score: 0.18657535826972724[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  42 | elapsed:    7.6s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:    8.0s finished\n",
      "\n",
      "[2025-09-28 14:46:14] Features: 66/107 -- score: 0.1865753582697273[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  41 | elapsed:    7.1s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  41 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:46:22] Features: 67/107 -- score: 0.18657424173907375[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  40 | elapsed:    7.7s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:46:31] Features: 68/107 -- score: 0.18656952852940564[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  39 | elapsed:    7.4s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  39 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:46:39] Features: 69/107 -- score: 0.18656952852940564[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  38 | elapsed:    7.4s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  38 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:46:48] Features: 70/107 -- score: 0.18656623923423743[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  37 | elapsed:    7.1s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  37 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:46:56] Features: 71/107 -- score: 0.1865662392342374[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  36 | elapsed:    7.6s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    8.2s finished\n",
      "\n",
      "[2025-09-28 14:47:04] Features: 72/107 -- score: 0.18655851694591766[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  35 | elapsed:    7.2s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:13] Features: 73/107 -- score: 0.18655851694591766[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  34 | elapsed:    7.4s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:21] Features: 74/107 -- score: 0.18654603701549632[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  33 | elapsed:    7.3s remaining:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  33 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:30] Features: 75/107 -- score: 0.18686420251397814[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  32 | elapsed:    6.7s remaining:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:47:38] Features: 76/107 -- score: 0.18683118237503216[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  31 | elapsed:    6.1s remaining:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:47] Features: 77/107 -- score: 0.18679131707303367[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    5.3s remaining:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    8.3s finished\n",
      "\n",
      "[2025-09-28 14:47:55] Features: 78/107 -- score: 0.18675356585575462[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  29 | elapsed:    5.6s remaining:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    8.1s finished\n",
      "\n",
      "[2025-09-28 14:48:04] Features: 79/107 -- score: 0.18674660501820298[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  28 | elapsed:    5.0s remaining:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:48:12] Features: 80/107 -- score: 0.1866979038393903[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  27 | elapsed:    4.9s remaining:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:    7.9s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    7.9s finished\n",
      "\n",
      "[2025-09-28 14:48:20] Features: 81/107 -- score: 0.18667223359013577[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  26 | elapsed:    4.9s remaining:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  26 | elapsed:    7.7s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    7.7s finished\n",
      "\n",
      "[2025-09-28 14:48:28] Features: 82/107 -- score: 0.18663897753087552[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  25 | elapsed:    4.8s remaining:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  25 | elapsed:    7.3s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    7.5s finished\n",
      "\n",
      "[2025-09-28 14:48:35] Features: 83/107 -- score: 0.18666137928981708[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  24 | elapsed:    4.7s remaining:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:    7.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    7.2s finished\n",
      "\n",
      "[2025-09-28 14:48:43] Features: 84/107 -- score: 0.1866702554612374[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  23 | elapsed:    4.6s remaining:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  23 | elapsed:    6.2s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    7.1s finished\n",
      "\n",
      "[2025-09-28 14:48:50] Features: 85/107 -- score: 0.18665796449059988[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  22 | elapsed:    4.4s remaining:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  22 | elapsed:    6.3s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    6.9s finished\n",
      "\n",
      "[2025-09-28 14:48:57] Features: 86/107 -- score: 0.18660783894367003[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  21 | elapsed:    5.9s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:    6.6s finished\n",
      "\n",
      "[2025-09-28 14:49:04] Features: 87/107 -- score: 0.18660783894367003[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:    5.7s remaining:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    6.7s finished\n",
      "\n",
      "[2025-09-28 14:49:11] Features: 88/107 -- score: 0.18660783894366986[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  19 | elapsed:    5.6s remaining:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    6.4s finished\n",
      "\n",
      "[2025-09-28 14:49:18] Features: 89/107 -- score: 0.18655545030285048[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  18 | elapsed:    5.2s remaining:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    6.3s finished\n",
      "\n",
      "[2025-09-28 14:49:24] Features: 90/107 -- score: 0.18648147274912033[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  17 | elapsed:    5.1s remaining:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  17 | elapsed:    6.2s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:    6.4s finished\n",
      "\n",
      "[2025-09-28 14:49:31] Features: 91/107 -- score: 0.18640155043274506[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  16 | elapsed:    5.3s remaining:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  16 | elapsed:    6.0s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    6.1s finished\n",
      "\n",
      "[2025-09-28 14:49:37] Features: 92/107 -- score: 0.18631408804032104[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    5.8s remaining:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    5.8s finished\n",
      "\n",
      "[2025-09-28 14:49:43] Features: 93/107 -- score: 0.1863140880403211[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  14 | elapsed:    5.5s remaining:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:    5.6s finished\n",
      "\n",
      "[2025-09-28 14:49:49] Features: 94/107 -- score: 0.18620896090507955[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  13 | elapsed:    5.3s remaining:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  13 | elapsed:    5.3s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    5.3s finished\n",
      "\n",
      "[2025-09-28 14:49:55] Features: 95/107 -- score: 0.1862089609050794[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    5.0s remaining:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    5.0s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    5.0s finished\n",
      "\n",
      "[2025-09-28 14:50:00] Features: 96/107 -- score: 0.18607865249380606[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  11 | elapsed:    4.5s remaining:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    4.6s finished\n",
      "\n",
      "[2025-09-28 14:50:05] Features: 97/107 -- score: 0.1859285772805774[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:    4.2s remaining:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.2s finished\n",
      "\n",
      "[2025-09-28 14:50:09] Features: 98/107 -- score: 0.1857508578648969[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    3.8s remaining:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    3.8s finished\n",
      "\n",
      "[2025-09-28 14:50:13] Features: 99/107 -- score: 0.18556131567349568[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:    3.4s remaining:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    3.4s finished\n",
      "\n",
      "[2025-09-28 14:50:17] Features: 100/107 -- score: 0.18525879218816046[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    2.8s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    2.8s finished\n",
      "\n",
      "[2025-09-28 14:50:20] Features: 101/107 -- score: 0.18544038721852327[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.2s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.3s finished\n",
      "\n",
      "[2025-09-28 14:50:22] Features: 102/107 -- score: 0.18512974870671342[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.6s finished\n",
      "\n",
      "[2025-09-28 14:50:24] Features: 103/107 -- score: 0.18512974870671342[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.2s finished\n",
      "\n",
      "[2025-09-28 14:50:26] Features: 104/107 -- score: 0.1851297487067134[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.9s finished\n",
      "\n",
      "[2025-09-28 14:50:27] Features: 105/107 -- score: 0.1851297487067133[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    0.5s finished\n",
      "\n",
      "[2025-09-28 14:50:28] Features: 106/107 -- score: 0.18481102822461862[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['low_card__op_unique_carrier_AA', 'low_card__op_unique_carrier_AS', 'low_card__op_unique_carrier_DL', 'low_card__op_unique_carrier_F9', 'low_card__op_unique_carrier_HA', 'low_card__op_unique_carrier_NK', 'low_card__op_unique_carrier_OO', 'low_card__op_unique_carrier_UA', 'low_card__op_unique_carrier_WN', 'low_card__op_unique_carrier_YX', 'low_card__op_carrier_AA', 'low_card__op_carrier_AS', 'low_card__op_carrier_DL', 'low_card__op_carrier_F9', 'low_card__op_carrier_HA', 'low_card__op_carrier_NK', 'low_card__op_carrier_OO', 'low_card__op_carrier_UA', 'low_card__op_carrier_WN', 'low_card__op_carrier_YX', 'low_card__dep_time_blk_0001-0559', 'low_card__dep_time_blk_0600-0659', 'low_card__dep_time_blk_0700-0759', 'low_card__dep_time_blk_0800-0859', 'low_card__dep_time_blk_0900-0959', 'low_card__dep_time_blk_1400-1459', 'low_card__dep_time_blk_1700-1759', 'low_card__dep_time_blk_1800-1859', 'low_card__dep_time_blk_1900-1959', 'low_card__dep_time_blk_2000-2059', 'low_card__dep_time_blk_2100-2159', 'low_card__dep_time_blk_2200-2259', 'low_card__arr_time_blk_0001-0559', 'low_card__arr_time_blk_0600-0659', 'low_card__arr_time_blk_0700-0759', 'low_card__arr_time_blk_0800-0859', 'low_card__arr_time_blk_0900-0959', 'low_card__arr_time_blk_1000-1059', 'low_card__arr_time_blk_1100-1159', 'low_card__arr_time_blk_1200-1259', 'low_card__arr_time_blk_1300-1359', 'low_card__arr_time_blk_1400-1459', 'low_card__arr_time_blk_1500-1559', 'low_card__arr_time_blk_1600-1659', 'low_card__arr_time_blk_1700-1759', 'low_card__arr_time_blk_1800-1859', 'low_card__arr_time_blk_1900-1959', 'low_card__arr_time_blk_2000-2059', 'low_card__arr_time_blk_2100-2159', 'low_card__arr_time_blk_2200-2259', 'low_card__arr_time_blk_2300-2359', 'low_card__cancellation_code_B', 'high_card__tail_num', 'high_card__origin', 'high_card__origin_state_abr', 'high_card__origin_state_nm', 'high_card__dest_city_name', 'high_card__dest_state_abr', 'high_card__dest_state_nm', 'num__year', 'num__quarter', 'num__month', 'num__op_carrier_fl_num', 'num__dep_time', 'num__taxi_out', 'num__wheels_off', 'num__wheels_on', 'num__taxi_in', 'num__arr_time', 'num__flights', 'num__first_dep_time', 'num__total_add_gtime', 'num__div_airport_landings', 'num__div_actual_elapsed_time', 'num__div_arr_delay']\n",
      "Model Accuracy (R²): -0.0641\n",
      "Model RMSE: 3364.6948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2025-09-28 14:50:28] Features: 107/107 -- score: 0.18481102822461867"
     ]
    }
   ],
   "source": [
    "# Fit preprocessor with y (important for supervised encoders)\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "X_train_trans = preprocessor.transform(X_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "sfs_forward = SFS(\n",
    "    lin_reg,\n",
    "    k_features='best',\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "sfs_forward = sfs_forward.fit(X_train_trans, y_train)\n",
    "\n",
    "# Map indices back to feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "selected_features = [feature_names[i] for i in sfs_forward.k_feature_idx_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# --- Evaluate model accuracy ---\n",
    "# Restrict to selected features\n",
    "X_train_sel = X_train_trans[:, sfs_forward.k_feature_idx_]\n",
    "X_test_sel  = X_test_trans[:, sfs_forward.k_feature_idx_]\n",
    "\n",
    "# Fit final model\n",
    "lin_reg.fit(X_train_sel, y_train)\n",
    "y_pred = lin_reg.predict(X_test_sel)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353947e",
   "metadata": {},
   "source": [
    "### Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f900a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:   55.2s finished\n",
      "\n",
      "[2025-09-28 14:52:27] Features: 106/1 -- score: 0.18499935318473468[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 106 out of 106 | elapsed:   53.9s finished\n",
      "\n",
      "[2025-09-28 14:53:21] Features: 105/1 -- score: 0.18516577826394912[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:   51.9s finished\n",
      "\n",
      "[2025-09-28 14:54:13] Features: 104/1 -- score: 0.18531519681955885[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 104 | elapsed:   50.5s finished\n",
      "\n",
      "[2025-09-28 14:55:04] Features: 103/1 -- score: 0.18541494981583076[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 103 out of 103 | elapsed:   49.5s finished\n",
      "\n",
      "[2025-09-28 14:55:54] Features: 102/1 -- score: 0.18549382862594602[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 102 out of 102 | elapsed:   48.0s finished\n",
      "\n",
      "[2025-09-28 14:56:42] Features: 101/1 -- score: 0.18557007509780235[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:   46.8s finished\n",
      "\n",
      "[2025-09-28 14:57:29] Features: 100/1 -- score: 0.1856332803752098[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   45.3s finished\n",
      "\n",
      "[2025-09-28 14:58:15] Features: 99/1 -- score: 0.18563493637579034[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  99 out of  99 | elapsed:   44.0s finished\n",
      "\n",
      "[2025-09-28 14:58:59] Features: 98/1 -- score: 0.18563493637579057[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  98 out of  98 | elapsed:   42.7s finished\n",
      "\n",
      "[2025-09-28 14:59:42] Features: 97/1 -- score: 0.18563493637579054[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 out of  97 | elapsed:   41.6s finished\n",
      "\n",
      "[2025-09-28 15:00:24] Features: 96/1 -- score: 0.18563493637579057[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:   40.6s finished\n",
      "\n",
      "[2025-09-28 15:01:04] Features: 95/1 -- score: 0.18563493637579057[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.7s\n"
     ]
    }
   ],
   "source": [
    "# --- Backward selection ---\n",
    "sfs_backward = SFS(\n",
    "    lin_reg,\n",
    "    k_features='best',\n",
    "    forward=False,\n",
    "    floating=False,\n",
    "    scoring='r2',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Important: fit preprocessor with y\n",
    "preprocessor.fit(X_train, y_train)\n",
    "X_train_trans = preprocessor.transform(X_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "sfs_backward = sfs_backward.fit(X_train_trans, y_train)\n",
    "\n",
    "# Map indices back to feature names\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "selected_features = [feature_names[i] for i in sfs_backward.k_feature_idx_]\n",
    "print(\"Backward-selected features:\", selected_features)\n",
    "\n",
    "# --- Evaluate model accuracy ---\n",
    "# Restrict to selected features\n",
    "X_train_sel = X_train_trans[:, sfs_backward.k_feature_idx_]\n",
    "X_test_sel  = X_test_trans[:, sfs_backward.k_feature_idx_]\n",
    "\n",
    "# Fit final model\n",
    "lin_reg.fit(X_train_sel, y_train)\n",
    "y_pred = lin_reg.predict(X_test_sel)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Backward Selection Model Accuracy (R²): {r2:.4f}\")\n",
    "print(f\"Backward Selection Model RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd3d1b",
   "metadata": {},
   "source": [
    "### PCR\n",
    "Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline: preprocessing → PCA → Linear Regression\n",
    "pcr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=20)),   # choose number of components\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pcr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pcr = pcr.predict(X_test)\n",
    "\n",
    "print(\"PCR R2:\", r2_score(y_test, y_pred_pcr))\n",
    "print(\"PCR RMSE:\", mean_squared_error(y_test, y_pred_pcr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045ae19",
   "metadata": {},
   "source": [
    "### PLSR\n",
    "Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocess first\n",
    "X_train_trans = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_trans  = preprocessor.transform(X_test)\n",
    "\n",
    "# Fit PLSR with, say, 10 components\n",
    "plsr = PLSRegression(n_components=10)\n",
    "plsr.fit(X_train_trans, y_train)\n",
    "\n",
    "y_pred_plsr = plsr.predict(X_test_trans)\n",
    "\n",
    "print(\"PLSR R2:\", r2_score(y_test, y_pred_plsr))\n",
    "print(\"PLSR RMSE:\", mean_squared_error(y_test, y_pred_plsr, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd613b8",
   "metadata": {},
   "source": [
    "# Week 4 Notebook - Logistic Regression and Feature Scaling\n",
    "\n",
    "For Week 4, include concepts such as logistic regression and feature scaling. This homework should be submitted for peer review in the assignment titled 4.3 Peer Review: Week 4 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501ecd8",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4ee49",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'MendeleyDelayData.csv'\n",
    "SAMPLE_SIZE = 50000\n",
    "df = pd.read_csv(data_path + file_name)\n",
    "\n",
    "df = optimize_dataframe(\n",
    "    df,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df = clean_column_names(df)\n",
    "\n",
    "if SAMPLE_SIZE:\n",
    "    bins = [-np.inf, -1, 0, 15, 60, 180, np.inf]\n",
    "    labels = ['early', 'on_time', 'small_delay', 'moderate_delay', 'long_delay', 'extreme_delay']\n",
    "\n",
    "    df['depdelay_bin'] = pd.cut(df['depdelay'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "    df = (\n",
    "        df.groupby('depdelay_bin', group_keys=False)\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=min(int(SAMPLE_SIZE * len(x) / len(df)), len(x)), \n",
    "            random_state=42\n",
    "        ))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=['depdelay_bin'])\n",
    "\n",
    "\n",
    "# Get column categories\n",
    "id_cols = ['originairportid', 'destairportid', ]\n",
    "cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "date_cols = ['scheduleddepartdatetime', ]\n",
    "target_cols = ['depdelay','arrdelay',]\n",
    "feature_cols = [col for col in df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "df_lin = df.drop(columns=['arrdelay'] + id_cols + date_cols).copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    df_lin, \n",
    "    target='depdelay', \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "# --- Create binary target: 1 if depdelay > 0, else 0 ---\n",
    "df_lin['depdelay_binary'] = (df_lin['depdelay'] > 0).astype(int)\n",
    "\n",
    "# Update target variable\n",
    "X = df_lin.drop(columns=['depdelay', 'depdelay_binary'])\n",
    "y = df_lin['depdelay_binary']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526a100",
   "metadata": {},
   "source": [
    "### Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Logistic Regression Pipeline ---\n",
    "log_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000,       # increase iterations for convergence\n",
    "        solver='lbfgs',      # robust solver\n",
    "        n_jobs=-1            # parallelize\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Fit model ---\n",
    "log_reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# --- Predict ---\n",
    "y_pred = log_reg_pipe.predict(X_test)\n",
    "\n",
    "# --- Evaluate ---\n",
    "print(\"\\nLogistic Regression Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932c148",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4e545",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "SAMPLE_SIZE = 50000\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "combined_df.columns = combined_df.columns.str.lower()\n",
    "\n",
    "usdot_df = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "usdot_df = clean_column_names(usdot_df)\n",
    "\n",
    "if SAMPLE_SIZE >= 0:\n",
    "    print(\"Sampling the dataset to\", SAMPLE_SIZE)\n",
    "    bins = [-np.inf, -1, 0, 15, 60, 180, np.inf]\n",
    "    labels = ['early', 'on_time', 'small_delay', 'moderate_delay', 'long_delay', 'extreme_delay']\n",
    "\n",
    "    usdot_df['depdelay_bin'] = pd.cut(usdot_df['dep_delay'], bins=bins, labels=labels)\n",
    "    usdot_df = (\n",
    "        usdot_df.groupby('depdelay_bin', group_keys=False)\n",
    "        .apply(lambda x: x.sample(\n",
    "            n=min(int(SAMPLE_SIZE * len(x) / len(usdot_df)), len(x)), \n",
    "            random_state=42\n",
    "        ))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    usdot_df = usdot_df.drop(columns=['depdelay_bin'])\n",
    "\n",
    "# Get column categories\n",
    "\n",
    "id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "date_cols = ['fl_date', ]\n",
    "target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', 'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "feature_cols = [col for col in usdot_df.columns if col not in id_cols + cat_cols + date_cols + target_cols]\n",
    "\n",
    "# drop leakage columns for linear regression\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "leakage_cols = [x for x in target_cols if x != TARGET_COLUMN]\n",
    "usdot_df_lin = usdot_df.drop(leakage_cols + id_cols + date_cols, axis=1, errors=\"ignore\").copy()\n",
    "\n",
    "preprocessor, low_card, high_card, num_cols = build_preprocessing_pipeline(\n",
    "    usdot_df_lin, \n",
    "    target=TARGET_COLUMN, \n",
    "    high_card_threshold=20, \n",
    "    scale_numeric=True\n",
    ")\n",
    "\n",
    "print(\"Low-cardinality categorical:\", low_card)\n",
    "print(\"High-cardinality categorical:\", high_card)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "X = usdot_df_lin.drop(columns=TARGET_COLUMN)\n",
    "y = usdot_df_lin[TARGET_COLUMN]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55163788",
   "metadata": {},
   "source": [
    "### Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Logistic Regression Pipeline ---\n",
    "log_reg_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000,       # increase iterations for convergence\n",
    "        solver='lbfgs',      # robust solver\n",
    "        n_jobs=-1            # parallelize\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Fit model ---\n",
    "log_reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# --- Predict ---\n",
    "y_pred = log_reg_pipe.predict(X_test)\n",
    "\n",
    "# --- Evaluate ---\n",
    "print(\"\\nLogistic Regression Results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13b062",
   "metadata": {},
   "source": [
    "# Week 5 - Support Vector Machines\n",
    "\n",
    "For Week 5, include concepts such as support vector machines, the kernel trick, and regularization for support vector machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5fb28",
   "metadata": {},
   "source": [
    "## Mendeley Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325dfff",
   "metadata": {},
   "source": [
    "## USDOT On Time Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
