{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d58ef2",
   "metadata": {
    "papermill": {
     "duration": 0.016002,
     "end_time": "2025-10-19T04:43:02.280075",
     "exception": false,
     "start_time": "2025-10-19T04:43:02.264073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa4899f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:02.316074Z",
     "iopub.status.busy": "2025-10-19T04:43:02.316074Z",
     "iopub.status.idle": "2025-10-19T04:43:02.319531Z",
     "shell.execute_reply": "2025-10-19T04:43:02.319531Z"
    },
    "papermill": {
     "duration": 0.021461,
     "end_time": "2025-10-19T04:43:02.320537",
     "exception": false,
     "start_time": "2025-10-19T04:43:02.299076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Virtual Environment\n",
    "\n",
    "# !& \"c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Scripts\\Activate.ps1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d627b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:02.344539Z",
     "iopub.status.busy": "2025-10-19T04:43:02.343539Z",
     "iopub.status.idle": "2025-10-19T04:43:02.348547Z",
     "shell.execute_reply": "2025-10-19T04:43:02.348547Z"
    },
    "papermill": {
     "duration": 0.017016,
     "end_time": "2025-10-19T04:43:02.349553",
     "exception": false,
     "start_time": "2025-10-19T04:43:02.332537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize results list\n",
    "mend_results = []\n",
    "usdot_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bda5898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:02.444552Z",
     "iopub.status.busy": "2025-10-19T04:43:02.443553Z",
     "iopub.status.idle": "2025-10-19T04:43:03.935583Z",
     "shell.execute_reply": "2025-10-19T04:43:03.935583Z"
    },
    "papermill": {
     "duration": 1.504035,
     "end_time": "2025-10-19T04:43:03.936587",
     "exception": false,
     "start_time": "2025-10-19T04:43:02.432552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Core ---\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Dates & holidays ---\n",
    "import holidays\n",
    "\n",
    "# --- Statsmodels ---\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# --- Scikit-learn: preprocessing & pipelines ---\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import category_encoders as ce\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# --- Scikit-learn: models ---\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    "    Lasso,\n",
    "    Ridge,\n",
    "    ElasticNet,\n",
    "    LogisticRegression\n",
    ")\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- Scikit-learn: model selection & metrics ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# --- Feature selection ---\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# --- Distributions ---\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "# --- Utilities ---\n",
    "from sklearn.utils import resample\n",
    "from sklearn.inspection import permutation_importance\n",
    "from joblib import parallel_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9a73e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:03.959589Z",
     "iopub.status.busy": "2025-10-19T04:43:03.959589Z",
     "iopub.status.idle": "2025-10-19T04:43:03.962511Z",
     "shell.execute_reply": "2025-10-19T04:43:03.962511Z"
    },
    "papermill": {
     "duration": 0.014929,
     "end_time": "2025-10-19T04:43:03.963516",
     "exception": false,
     "start_time": "2025-10-19T04:43:03.948587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_path = 'C:/Users/tbran/Python/repos/Semester 3 Repos/capstone/'\n",
    "data_path = project_path + 'data/'\n",
    "src_path = project_path + 'src/'\n",
    "model_path = project_path + 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539aae89",
   "metadata": {
    "papermill": {
     "duration": 0.010999,
     "end_time": "2025-10-19T04:43:03.985516",
     "exception": false,
     "start_time": "2025-10-19T04:43:03.974517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mendeley Carrier Delay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836f709",
   "metadata": {
    "papermill": {
     "duration": 0.011,
     "end_time": "2025-10-19T04:43:04.006517",
     "exception": false,
     "start_time": "2025-10-19T04:43:03.995517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f15d71ab",
   "metadata": {
    "papermill": {
     "duration": 0.01,
     "end_time": "2025-10-19T04:43:04.027517",
     "exception": false,
     "start_time": "2025-10-19T04:43:04.017517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7983db",
   "metadata": {
    "papermill": {
     "duration": 0.009999,
     "end_time": "2025-10-19T04:43:04.048515",
     "exception": false,
     "start_time": "2025-10-19T04:43:04.038516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Prep Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7713e4ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:04.071518Z",
     "iopub.status.busy": "2025-10-19T04:43:04.071518Z",
     "iopub.status.idle": "2025-10-19T04:43:04.165298Z",
     "shell.execute_reply": "2025-10-19T04:43:04.165298Z"
    },
    "papermill": {
     "duration": 0.106786,
     "end_time": "2025-10-19T04:43:04.166302",
     "exception": false,
     "start_time": "2025-10-19T04:43:04.059516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_dataframe(df, datetime_cols=None, fillna=False):\n",
    "    \"\"\"\n",
    "    Cleans and optimizes a DataFrame:\n",
    "    - Converts object datetime columns to datetime64\n",
    "    - Converts object columns with repeated values to category\n",
    "    - Downcasts numeric columns to smallest safe type\n",
    "    - Optionally fills NaNs before downcasting\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to optimize\n",
    "        datetime_cols (list): List of column names to convert to datetime\n",
    "        fillna (bool): If True, fills NaNs before downcasting\n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage before optimization: {start_mem:.2f} MB\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Convert datetime columns\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # 2. Convert object columns to category if appropriate\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df[col])\n",
    "        if num_unique / num_total < 0.5:  # heuristic: less than 50% unique\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    # 3. Downcast numeric columns\n",
    "    int_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "    float_cols = df.select_dtypes(include=['float64', 'float32']).columns\n",
    "    \n",
    "    for col in int_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(0)\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in float_cols:\n",
    "        if fillna and df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization: {end_mem:.2f} MB\")\n",
    "    print(f\"Reduced by {100 * (start_mem - end_mem) / start_mem:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_column_names(df, remove_accents=True):\n",
    "    \"\"\"\n",
    "    Cleans DataFrame column names:\n",
    "    - Strips whitespace\n",
    "    - Converts to lowercase\n",
    "    - Replaces spaces & special chars with underscores\n",
    "    - Removes duplicate underscores\n",
    "    - Optionally removes accents\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame whose columns to clean\n",
    "        remove_accents (bool): If True, strips accents from characters\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    def _clean(col):\n",
    "        col = col.strip().lower()\n",
    "        if remove_accents:\n",
    "            col = ''.join(\n",
    "                c for c in unicodedata.normalize('NFKD', col)\n",
    "                if not unicodedata.combining(c)\n",
    "            )\n",
    "        col = re.sub(r'[^0-9a-zA-Z]+', '_', col)  # replace non-alphanumeric with _\n",
    "        col = re.sub(r'_+', '_', col)             # collapse multiple underscores\n",
    "        col = col.strip('_')                      # remove leading/trailing underscores\n",
    "        return col\n",
    "    \n",
    "    df = df.copy()\n",
    "    df.columns = [_clean(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_preprocessing_pipeline(df, target, \n",
    "                                  high_card_threshold=20, \n",
    "                                  scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Builds a preprocessing pipeline for linear regression:\n",
    "    - One-hot encodes low-cardinality categorical columns\n",
    "    - Target encodes high-cardinality categorical columns\n",
    "    - Optionally scales numeric columns\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame (including target column)\n",
    "        target (str): Name of target column\n",
    "        high_card_threshold (int): Unique value cutoff for high-cardinality\n",
    "        scale_numeric (bool): Whether to scale numeric features\n",
    "        \n",
    "    Returns:\n",
    "        pipeline (ColumnTransformer): Preprocessing transformer\n",
    "        low_card_cols (list): Low-cardinality categorical columns\n",
    "        high_card_cols (list): High-cardinality categorical columns\n",
    "        num_cols (list): Numeric columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target])\n",
    "    \n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [col for col in cat_cols if X[col].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [col for col in cat_cols if X[col].nunique() > high_card_threshold]\n",
    "    \n",
    "    # Transformers\n",
    "    low_card_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    high_card_transformer = TargetEncoder()\n",
    "    num_transformer = StandardScaler() if scale_numeric else 'passthrough'\n",
    "    \n",
    "    # Column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', low_card_transformer, low_card_cols),\n",
    "            ('high_card', high_card_transformer, high_card_cols),\n",
    "            ('num', num_transformer, num_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor, low_card_cols, high_card_cols, num_cols\n",
    "\n",
    "def add_interaction_terms(df, features):\n",
    "    \"\"\"\n",
    "    Adds pairwise interaction terms between given features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    interaction_array = poly.fit_transform(df[features])\n",
    "    interaction_df = pd.DataFrame(interaction_array, columns=poly.get_feature_names_out(features))\n",
    "    return pd.concat([df.reset_index(drop=True), interaction_df], axis=1)\n",
    "\n",
    "def preprocess_features(df, categorical_cols, numeric_cols):\n",
    "    \"\"\"\n",
    "    Returns a ColumnTransformer that one-hot encodes categorical columns\n",
    "    and passes numeric columns through unchanged.\n",
    "    \"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "            ('num', 'passthrough', numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def transform_with_names(preprocessor, X, y=None):\n",
    "    \"\"\"Fit/transform and return a DataFrame with feature names preserved.\"\"\"\n",
    "    Xt = preprocessor.fit_transform(X, y)\n",
    "    cols = preprocessor.get_feature_names_out()\n",
    "    return pd.DataFrame(Xt, columns=cols, index=X.index)\n",
    "\n",
    "\n",
    "def create_features_mend(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Build U.S. holiday calendar for relevant years\n",
    "    us_holidays = holidays.US(years=range(2015, 2027))\n",
    "\n",
    "    # Build a list of (holiday_date, holiday_name)\n",
    "    holiday_items = list(us_holidays.items())\n",
    "\n",
    "    # Create a set of all holiday dates ±3 days\n",
    "    holiday_buffer = {}\n",
    "    for h_date, h_name in holiday_items:\n",
    "        for offset in range(-3, 4):  # -3, -2, -1, 0, +1, +2, +3\n",
    "            holiday_buffer[h_date + pd.Timedelta(days=offset)] = h_name\n",
    "\n",
    "    # Binary flag: within ±3 days of a holiday\n",
    "    if \"scheduleddepartdatetime\" in df.columns:\n",
    "        print(\"Adding holiday features...\")\n",
    "        df['is_holiday_period'] = df['scheduleddepartdatetime'].dt.date.isin(holiday_buffer.keys())\n",
    "\n",
    "        # Categorical holiday name (or \"None\")\n",
    "        def get_holiday_name(d):\n",
    "            return holiday_buffer.get(d, \"None\")\n",
    "\n",
    "        df['holiday_name'] = df['scheduleddepartdatetime'].dt.date.apply(get_holiday_name)\n",
    "    else:\n",
    "        print(\"Column 'scheduleddepartdatetime' not found, skipping holiday features.\")\n",
    "\n",
    "    # --- Existing engineered features ---\n",
    "    if \"scheduleddepartdatetime\" in df.columns:\n",
    "        print(\"Adding time-based features...\")\n",
    "        df[\"dayofweek\"] = df[\"scheduleddepartdatetime\"].dt.dayofweek\n",
    "        df[\"month\"] = df[\"scheduleddepartdatetime\"].dt.month\n",
    "    else:\n",
    "        print(\"Skipping time-based features.\")\n",
    "\n",
    "    if {\"origin\",\"dest\"}.issubset(df.columns):\n",
    "        print(\"Adding route feature...\")\n",
    "        df[\"route\"] = df[\"origin\"].astype(str) + \"_\" + df[\"dest\"].astype(str)\n",
    "    else:\n",
    "        print(\"Skipping route feature.\")\n",
    "\n",
    "    if {\"marketshareorigin\",\"marketsharedest\"}.issubset(df.columns):\n",
    "        print(\"Adding marketshare_diff...\")\n",
    "        df[\"marketshare_diff\"] = df[\"marketshareorigin\"] - df[\"marketsharedest\"]\n",
    "    else:\n",
    "        print(\"Skipping marketshare_diff.\")\n",
    "\n",
    "    if {\"hhiorigin\",\"hhidest\"}.issubset(df.columns):\n",
    "        print(\"Adding hhi_diff...\")\n",
    "        df[\"hhi_diff\"] = df[\"hhiorigin\"] - df[\"hhidest\"]\n",
    "    else:\n",
    "        print(\"Skipping hhi_diff.\")\n",
    "\n",
    "    if {\"temperature\",\"windspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding temp_wind_interaction...\")\n",
    "        df[\"temp_wind_interaction\"] = df[\"temperature\"] * df[\"windspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping temp_wind_interaction.\")\n",
    "\n",
    "    if {\"temperature\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding temp_windgust_interaction...\")\n",
    "        df[\"temp_windgust_interaction\"] = df[\"temperature\"] * df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping temp_windgust_interaction.\")\n",
    "\n",
    "    if {\"windspeed\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding wind_gust_diff...\")\n",
    "        df[\"wind_gust_diff\"] = df[\"windspeed\"] - df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping wind_gust_diff.\")\n",
    "\n",
    "    if {\"raindummy\",\"windspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding rain_wind_interaction...\")\n",
    "        df[\"rain_wind_interaction\"] = df[\"raindummy\"] * df[\"windspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping rain_wind_interaction.\")\n",
    "\n",
    "    if {\"snowdummy\",\"windspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding snow_wind_interaction...\")\n",
    "        df[\"snow_wind_interaction\"] = df[\"snowdummy\"] * df[\"windspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping snow_wind_interaction.\")\n",
    "\n",
    "    if {\"raindummy\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding rain_wind_gust_interaction...\")\n",
    "        df[\"rain_wind_gust_interaction\"] = df[\"raindummy\"] * df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping rain_wind_gust_interaction.\")\n",
    "\n",
    "    if {\"snowdummy\",\"windgustspeed\"}.issubset(df.columns):\n",
    "        print(\"Adding snow_wind_gust_interaction...\")\n",
    "        df[\"snow_wind_gust_interaction\"] = df[\"snowdummy\"] * df[\"windgustspeed\"]\n",
    "    else:\n",
    "        print(\"Skipping snow_wind_gust_interaction.\")\n",
    "\n",
    "    if {\"originmetropop\",\"destmetropop\"}.issubset(df.columns):\n",
    "        print(\"Adding metropop_diff...\")\n",
    "        df[\"metropop_diff\"] = df[\"originmetropop\"] - df[\"destmetropop\"]\n",
    "    else:\n",
    "        print(\"Skipping metropop_diff.\")\n",
    "\n",
    "    if {\"originmetrogdppercapita\",\"destmetrogdppercapita\"}.issubset(df.columns):\n",
    "        print(\"Adding metrogdp_diff...\")\n",
    "        df[\"metrogdp_diff\"] = df[\"originmetrogdppercapita\"] - df[\"destmetrogdppercapita\"]\n",
    "    else:\n",
    "        print(\"Skipping metrogdp_diff.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def engineer_flight_features_light(\n",
    "    df,\n",
    "    datetime_col=\"scheduleddepartdatetime\",\n",
    "    origin_col=\"origin\",\n",
    "    dest_col=\"dest\",\n",
    "    carrier_col=\"uniquecarrier\",\n",
    "    delay_col=\"depdelay\",\n",
    "    distance_col=\"distance\"\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Datetime parts ---\n",
    "    dt = pd.to_datetime(df[datetime_col])\n",
    "    df[\"year\"] = dt.dt.year\n",
    "    df[\"month\"] = dt.dt.month\n",
    "    df[\"day\"] = dt.dt.day\n",
    "    df[\"hour\"] = dt.dt.hour\n",
    "    df[\"date\"] = dt.dt.floor(\"D\")\n",
    "    df[\"is_weekend\"] = dt.dt.dayofweek >= 5\n",
    "\n",
    "    # --- Route & distance ---\n",
    "    df[\"route\"] = df[origin_col].astype(str) + \"_\" + df[dest_col].astype(str)\n",
    "    if distance_col in df.columns:\n",
    "        df[\"distance_bin\"] = pd.cut(\n",
    "            df[distance_col],\n",
    "            bins=[0, 500, 1500, 3000, 10000],\n",
    "            labels=[\"short\", \"medium\", \"long\", \"ultra\"]\n",
    "        )\n",
    "\n",
    "    # --- Congestion (lighter via merge instead of transform) ---\n",
    "    # Hourly origin flights\n",
    "    hourly_counts = (\n",
    "        df.groupby([origin_col, \"date\", \"hour\"], observed=True)\n",
    "          .size()\n",
    "          .reset_index(name=\"hourly_origin_flights\")\n",
    "    )\n",
    "    df = df.merge(hourly_counts, on=[origin_col, \"date\", \"hour\"], how=\"left\")\n",
    "\n",
    "    # Daily route flights\n",
    "    daily_counts = (\n",
    "        df.groupby([origin_col, dest_col, \"date\"], observed=True)\n",
    "          .size()\n",
    "          .reset_index(name=\"daily_route_flights\")\n",
    "    )\n",
    "    df = df.merge(daily_counts, on=[origin_col, dest_col, \"date\"], how=\"left\")\n",
    "\n",
    "    # --- Weather (light) ---\n",
    "    if \"temperature\" in df.columns:\n",
    "        df[\"is_extreme_temp\"] = (df[\"temperature\"] < 0) | (df[\"temperature\"] > 35)\n",
    "    if {\"raindummy\", \"snowdummy\", \"windgustdummy\"}.issubset(df.columns):\n",
    "        df[\"stormy\"] = (df[\"raindummy\"] | df[\"snowdummy\"] | df[\"windgustdummy\"]).astype(int)\n",
    "\n",
    "    # --- Market/demand ---\n",
    "    if {\"capacity\", \"numflights\"}.issubset(df.columns):\n",
    "        df[\"capacity_utilization\"] = df[\"numflights\"] / df[\"capacity\"].replace(0, np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_flight_features_heavy(\n",
    "    df,\n",
    "    datetime_col=\"scheduleddepartdatetime\",\n",
    "    origin_col=\"origin\",\n",
    "    dest_col=\"dest\",\n",
    "    carrier_col=\"uniquecarrier\",\n",
    "    delay_col=\"depdelay\",\n",
    "    distance_col=\"distance\",\n",
    "    window=7\n",
    "):\n",
    "    \"\"\"\n",
    "    Engineer advanced features for flight delay prediction.\n",
    "    Optimized for speed, with print statements and timing checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    df = df.copy()\n",
    "    print(\"Starting feature engineering...\")\n",
    "\n",
    "    # --- Precompute datetime parts once ---\n",
    "    t0 = time.time()\n",
    "    if datetime_col in df.columns:\n",
    "        dt = pd.to_datetime(df[datetime_col])\n",
    "        df[\"year\"] = dt.dt.year\n",
    "        df[\"month\"] = dt.dt.month\n",
    "        df[\"day\"] = dt.dt.day\n",
    "        df[\"hour\"] = dt.dt.hour\n",
    "        df[\"date\"] = dt.dt.floor(\"D\")\n",
    "        df[\"quarter\"] = dt.dt.quarter\n",
    "        df[\"is_weekend\"] = dt.dt.dayofweek >= 5\n",
    "        df[\"part_of_day\"] = pd.cut(\n",
    "            df[\"hour\"],\n",
    "            bins=[0,5,11,16,21,24],\n",
    "            labels=[\"late_night\",\"morning\",\"midday\",\"evening\",\"night\"],\n",
    "            right=False\n",
    "        )\n",
    "        df[\"days_since_year_start\"] = (\n",
    "            dt - pd.to_datetime(df[\"year\"].astype(str) + \"-01-01\")\n",
    "        ).dt.days\n",
    "    print(f\"Datetime features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Route & distance features ---\n",
    "    t0 = time.time()\n",
    "    if {origin_col, dest_col}.issubset(df.columns):\n",
    "        if f\"largehubairport{origin_col}\" in df.columns and f\"largehubairport{dest_col}\" in df.columns:\n",
    "            df[\"hub_to_hub\"] = (\n",
    "                (df[f\"largehubairport{origin_col}\"] == 1) &\n",
    "                (df[f\"largehubairport{dest_col}\"] == 1)\n",
    "            ).astype(int)\n",
    "        df[\"route\"] = df[origin_col].astype(str) + \"_\" + df[dest_col].astype(str)\n",
    "\n",
    "    if distance_col in df.columns:\n",
    "        df[\"distance_bin\"] = pd.cut(\n",
    "            df[distance_col],\n",
    "            bins=[0,500,1500,3000,10000],\n",
    "            labels=[\"short\",\"medium\",\"long\",\"ultra\"]\n",
    "        )\n",
    "    print(f\"Route & distance features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Congestion features ---\n",
    "    t0 = time.time()\n",
    "    if {\"date\",\"hour\",origin_col}.issubset(df.columns):\n",
    "        hourly_counts = (\n",
    "            df.groupby([origin_col,\"date\",\"hour\"])\n",
    "              .size()\n",
    "              .rename(\"hourly_origin_flights\")\n",
    "              .reset_index()\n",
    "        )\n",
    "        df = df.merge(hourly_counts, on=[origin_col,\"date\",\"hour\"], how=\"left\")\n",
    "\n",
    "    if {\"date\",origin_col,dest_col}.issubset(df.columns):\n",
    "        daily_counts = (\n",
    "            df.groupby([origin_col,dest_col,\"date\"])\n",
    "              .size()\n",
    "              .rename(\"daily_route_flights\")\n",
    "              .reset_index()\n",
    "        )\n",
    "        df = df.merge(daily_counts, on=[origin_col,dest_col,\"date\"], how=\"left\")\n",
    "    print(f\"Congestion features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Weather features ---\n",
    "    t0 = time.time()\n",
    "    if \"temperature\" in df.columns:\n",
    "        df[\"is_extreme_temp\"] = (df[\"temperature\"] < 0) | (df[\"temperature\"] > 35)\n",
    "        if \"month\" in df.columns:\n",
    "            monthly_means = df.groupby(\"month\")[\"temperature\"].transform(\"mean\")\n",
    "            df[\"temp_anomaly\"] = df[\"temperature\"] - monthly_means\n",
    "\n",
    "    if {\"raindummy\",\"snowdummy\",\"windgustdummy\"}.issubset(df.columns):\n",
    "        df[\"stormy\"] = (\n",
    "            (df[\"raindummy\"]==1) | (df[\"snowdummy\"]==1) | (df[\"windgustdummy\"]==1)\n",
    "        ).astype(int)\n",
    "    print(f\"Weather features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Rolling averages ---\n",
    "    t0 = time.time()\n",
    "    if {origin_col, delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([origin_col, datetime_col])\n",
    "        df[\"rolling_origin_delay\"] = (\n",
    "            df.groupby(origin_col)[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    if {dest_col, delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([dest_col, datetime_col])\n",
    "        df[\"rolling_dest_delay\"] = (\n",
    "            df.groupby(dest_col)[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    if {carrier_col, delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([carrier_col, datetime_col])\n",
    "        df[\"rolling_carrier_delay\"] = (\n",
    "            df.groupby(carrier_col)[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    if {\"route\", delay_col}.issubset(df.columns):\n",
    "        df = df.sort_values([\"route\", datetime_col])\n",
    "        df[\"rolling_route_delay\"] = (\n",
    "            df.groupby(\"route\")[delay_col]\n",
    "              .rolling(window, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    print(f\"Rolling averages done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Market/demand features ---\n",
    "    t0 = time.time()\n",
    "    if {\"capacity\",\"numflights\"}.issubset(df.columns):\n",
    "        df[\"capacity_utilization\"] = (\n",
    "            df[\"numflights\"] / df[\"capacity\"].replace(0, np.nan)\n",
    "        )\n",
    "\n",
    "    if {origin_col, dest_col, carrier_col}.issubset(df.columns):\n",
    "        df[\"route_carrier_count\"] = (\n",
    "            df.groupby([origin_col,dest_col])[carrier_col].transform(\"nunique\")\n",
    "        )\n",
    "    print(f\"Market/demand features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    # --- Interaction features ---\n",
    "    t0 = time.time()\n",
    "    if {\"is_holiday_period\",\"monopolyroute\"}.issubset(df.columns):\n",
    "        df[\"holiday_monopoly\"] = (\n",
    "            df[\"is_holiday_period\"].astype(int) * df[\"monopolyroute\"].astype(int)\n",
    "        )\n",
    "\n",
    "    if {\"is_extreme_temp\",\"hourly_origin_flights\"}.issubset(df.columns):\n",
    "        df[\"extreme_temp_congestion\"] = (\n",
    "            df[\"is_extreme_temp\"].astype(int) * df[\"hourly_origin_flights\"]\n",
    "        )\n",
    "    print(f\"Interaction features done in {time.time()-t0:.2f}s\")\n",
    "\n",
    "    print(f\"Total feature engineering time: {time.time()-start_time:.2f}s\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec784f5",
   "metadata": {
    "papermill": {
     "duration": 0.011,
     "end_time": "2025-10-19T04:43:04.188302",
     "exception": false,
     "start_time": "2025-10-19T04:43:04.177302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Prep Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2355e1df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:04.211302Z",
     "iopub.status.busy": "2025-10-19T04:43:04.211302Z",
     "iopub.status.idle": "2025-10-19T04:43:14.769819Z",
     "shell.execute_reply": "2025-10-19T04:43:14.769819Z"
    },
    "papermill": {
     "duration": 10.570522,
     "end_time": "2025-10-19T04:43:14.770825",
     "exception": false,
     "start_time": "2025-10-19T04:43:04.200303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before optimization: 1008.24 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization: 150.66 MB\n",
      "Reduced by 85.1%\n"
     ]
    }
   ],
   "source": [
    "# Load Mendeley Delay Data\n",
    "file_name = 'MendeleyDelayData.csv'\n",
    "df_mend = pd.read_csv(data_path + file_name)\n",
    "\n",
    "df_mend = optimize_dataframe(\n",
    "    df_mend,\n",
    "    datetime_cols=['scheduleddepartdatetime'],\n",
    "    fillna=True\n",
    ")\n",
    "df_mend = clean_column_names(df_mend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb3f6c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:14.796826Z",
     "iopub.status.busy": "2025-10-19T04:43:14.796826Z",
     "iopub.status.idle": "2025-10-19T04:43:16.540788Z",
     "shell.execute_reply": "2025-10-19T04:43:16.540788Z"
    },
    "papermill": {
     "duration": 1.757972,
     "end_time": "2025-10-19T04:43:16.541798",
     "exception": false,
     "start_time": "2025-10-19T04:43:14.783826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbran\\AppData\\Local\\Temp\\ipykernel_5396\\3905881234.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_reg = pd.read_csv(data_path + file_name)\n"
     ]
    }
   ],
   "source": [
    "file_name = \"FAA_AC_REGISTRATION_2021.csv\"\n",
    "df_reg = pd.read_csv(data_path + file_name)\n",
    "\n",
    "# Left join on stripped tail_num\n",
    "df_mend_clean_reg = df_mend.merge(\n",
    "    df_reg,\n",
    "    how=\"left\",\n",
    "    left_on=df_mend[\"tailnum\"].str.lstrip(\"N\"),\n",
    "    right_on=\"N-NUMBER\"\n",
    ")\n",
    "\n",
    "# Impute missing values with mode per column\n",
    "mode_dict = {col: df_reg[col].mode()[0] for col in df_reg.columns if col != \"N-NUMBER\"}\n",
    "df_mend_clean_reg = df_mend_clean_reg.fillna(mode_dict)\n",
    "\n",
    "# Drop duplicate join key\n",
    "df_mend_clean_reg = df_mend_clean_reg.drop(columns=[\"N-NUMBER\"])\n",
    "df_mend_clean_reg = clean_column_names(df_mend_clean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda36428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:16.565796Z",
     "iopub.status.busy": "2025-10-19T04:43:16.565796Z",
     "iopub.status.idle": "2025-10-19T04:43:16.570487Z",
     "shell.execute_reply": "2025-10-19T04:43:16.570487Z"
    },
    "papermill": {
     "duration": 0.017697,
     "end_time": "2025-10-19T04:43:16.571494",
     "exception": false,
     "start_time": "2025-10-19T04:43:16.553797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define encoders at module scope (picklable)\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=np.int8)\n",
    "te = TargetEncoder()\n",
    "ord_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "def build_dual_preprocessors(df, target, feature_cols,\n",
    "                             high_card_threshold=20, scale_numeric=False):\n",
    "    \"\"\"\n",
    "    Build regression + tree preprocessors using an explicit feature list.\n",
    "    Returns (regression_preprocessor, tree_preprocessor, X, y).\n",
    "    \"\"\"\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target]\n",
    "\n",
    "    # Identify column types\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "    # Split categorical into low/high cardinality\n",
    "    low_card_cols = [c for c in cat_cols if X[c].nunique() <= high_card_threshold]\n",
    "    high_card_cols = [c for c in cat_cols if X[c].nunique() > high_card_threshold]\n",
    "\n",
    "    # Regression preprocessor: OHE + TargetEncoder + scaling\n",
    "    regression_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"low_card\", ohe, low_card_cols),\n",
    "            (\"high_card\", te, high_card_cols),\n",
    "            (\"num\", StandardScaler() if scale_numeric else \"passthrough\", num_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Tree preprocessor: Ordinal encode categoricals, passthrough numerics\n",
    "    tree_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", ord_enc, cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return regression_preprocessor, tree_preprocessor, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce00485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:16.596495Z",
     "iopub.status.busy": "2025-10-19T04:43:16.596495Z",
     "iopub.status.idle": "2025-10-19T04:43:58.599579Z",
     "shell.execute_reply": "2025-10-19T04:43:58.599579Z"
    },
    "papermill": {
     "duration": 42.017092,
     "end_time": "2025-10-19T04:43:58.600586",
     "exception": false,
     "start_time": "2025-10-19T04:43:16.583494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding holiday features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding time-based features...\n",
      "Adding route feature...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding marketshare_diff...\n",
      "Adding hhi_diff...\n",
      "Adding temp_wind_interaction...\n",
      "Adding temp_windgust_interaction...\n",
      "Adding wind_gust_diff...\n",
      "Adding rain_wind_interaction...\n",
      "Adding snow_wind_interaction...\n",
      "Adding rain_wind_gust_interaction...\n",
      "Adding snow_wind_gust_interaction...\n",
      "Adding metropop_diff...\n",
      "Adding metrogdp_diff...\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers from dataframe\n",
    "df_mend_clean = df_mend_clean_reg[df_mend_clean_reg['depdelay'] >= -30]\n",
    "\n",
    "# Create engineered features\n",
    "df_mend_clean = create_features_mend(df_mend_clean)\n",
    "df_mend_clean = engineer_flight_features_light(df_mend_clean)\n",
    "\n",
    "# Get list of columns\n",
    "df_mend_id_cols = ['originairportid', 'destairportid', ]\n",
    "df_mend_cat_cols = ['origin', 'dest', 'uniquecarrier', 'tailnum', 'origincityname', 'originstate', ]\n",
    "df_mend_date_cols = ['scheduleddepartdatetime', ]\n",
    "df_mend_target_cols = ['depdelay','arrdelay',]\n",
    "df_mend_feature_cols = [col for col in df_mend_clean.columns if col not in df_mend_id_cols + df_mend_cat_cols + df_mend_date_cols + df_mend_target_cols]\n",
    "\n",
    "# drop leakage columns, ID columns, and date columns\n",
    "df_mend_clean = df_mend_clean.drop(columns=['arrdelay'] + df_mend_id_cols + df_mend_date_cols).copy()\n",
    "# Export df to csv\n",
    "df_mend_clean.to_csv(data_path + 'MendeleyDelayData_Cleaned.csv', index=False)\n",
    "\n",
    "reg_prep_mend, tree_prep_mend, X_mend, y_mend_numeric = build_dual_preprocessors(df_mend_clean, \n",
    "                                                                                 target='depdelay', \n",
    "                                                                                 feature_cols=df_mend_feature_cols, \n",
    "                                                                                 high_card_threshold=30, scale_numeric=True)\n",
    "\n",
    "# Create binary target for classification (15 min delay threshold)\n",
    "y_mend_binary_15 = (y_mend_numeric >= 15).astype(int)\n",
    "\n",
    "# Ensure categorical columns are string type (Some reg data has mixed types)\n",
    "cat_cols = ['type_acft', 'ac_weight', 'holiday_name', 'distance_bin']\n",
    "high_card_cols = ['mfr', 'model', 'route']\n",
    "\n",
    "for col in cat_cols + high_card_cols:\n",
    "    X_mend[col] = X_mend[col].astype(str)\n",
    "\n",
    "# Create seperate transformed datasets for regression and tree models\n",
    "X_reg_mend = transform_with_names(reg_prep_mend, X_mend, y_mend_numeric)\n",
    "X_tree_mend = transform_with_names(tree_prep_mend, X_mend, y_mend_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32311e0",
   "metadata": {
    "papermill": {
     "duration": 0.011001,
     "end_time": "2025-10-19T04:43:58.625588",
     "exception": false,
     "start_time": "2025-10-19T04:43:58.614587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Week 1 – Linear Regression 1\n",
    "Each week, you will apply the concepts of that week to your Integrated Capstone Project’s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data, using the methods you have learned about in this course and in this program, and draw interesting conclusions. \n",
    "\n",
    "For Week 1, include concepts such as linear regression with polynomial terms, interaction terms, multicollinearity, variance inflation factor and regression, and categorical and continuous features. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e5e44",
   "metadata": {
    "papermill": {
     "duration": 0.011998,
     "end_time": "2025-10-19T04:43:58.648585",
     "exception": false,
     "start_time": "2025-10-19T04:43:58.636587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Week 1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a83f844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:58.671585Z",
     "iopub.status.busy": "2025-10-19T04:43:58.671585Z",
     "iopub.status.idle": "2025-10-19T04:43:58.677060Z",
     "shell.execute_reply": "2025-10-19T04:43:58.677060Z"
    },
    "papermill": {
     "duration": 0.018478,
     "end_time": "2025-10-19T04:43:58.678067",
     "exception": false,
     "start_time": "2025-10-19T04:43:58.659589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regression_summary(X, y):\n",
    "    \"\"\"\n",
    "    Fits an OLS regression model using statsmodels and prints the summary.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    return model.summary()\n",
    "\n",
    "def fit_polynomial_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and returns the fitted model and transformed features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "\n",
    "def calculate_vif(df, features=None, vif_thresh=10.0):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) safely:\n",
    "    - Removes constant and perfectly collinear columns\n",
    "    - Returns a clean, sorted VIF table\n",
    "    \"\"\"\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "\n",
    "    # 1. Drop constant columns\n",
    "    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"Dropping constant columns: {constant_cols}\")\n",
    "        X.drop(columns=constant_cols, inplace=True)\n",
    "\n",
    "    # 2. Drop perfectly collinear columns\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    perfect_corr_cols = [col for col in upper.columns if any(upper[col] == 1.0)]\n",
    "    if perfect_corr_cols:\n",
    "        print(f\"Dropping perfectly collinear columns: {perfect_corr_cols}\")\n",
    "        X.drop(columns=perfect_corr_cols, inplace=True)\n",
    "\n",
    "    # 3. Calculate VIF\n",
    "    X_const = sm.add_constant(X)  # cleaner than assign(const=1)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"Feature\": X.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_const.values, i+1)  # skip const\n",
    "                for i in range(len(X.columns))]\n",
    "    })\n",
    "\n",
    "    # 4. Sort and format\n",
    "    vif_data[\"VIF\"] = vif_data[\"VIF\"].round(2)\n",
    "    vif_data[\"High_VIF\"] = vif_data[\"VIF\"] > vif_thresh\n",
    "    vif_data = vif_data.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e8fc9",
   "metadata": {
    "papermill": {
     "duration": 0.011,
     "end_time": "2025-10-19T04:43:58.700066",
     "exception": false,
     "start_time": "2025-10-19T04:43:58.689066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d437e363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:43:58.722065Z",
     "iopub.status.busy": "2025-10-19T04:43:58.722065Z",
     "iopub.status.idle": "2025-10-19T04:44:18.021486Z",
     "shell.execute_reply": "2025-10-19T04:44:18.021486Z"
    },
    "papermill": {
     "duration": 19.311427,
     "end_time": "2025-10-19T04:44:18.022493",
     "exception": false,
     "start_time": "2025-10-19T04:43:58.711066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.056\n",
      "Test R²: 0.047\n",
      "Test RMSE: 34.318\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mend, y_mend_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "linreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit\n",
    "linreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = linreg_pipe.predict(X_train)\n",
    "y_pred_test = linreg_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    'model': 'Linear Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f17fb",
   "metadata": {
    "papermill": {
     "duration": 0.012005,
     "end_time": "2025-10-19T04:44:18.046497",
     "exception": false,
     "start_time": "2025-10-19T04:44:18.034492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Polynomial Regression: Second Attempt\n",
    "\n",
    "Reduced the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482fc0ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:44:18.070010Z",
     "iopub.status.busy": "2025-10-19T04:44:18.070010Z",
     "iopub.status.idle": "2025-10-19T04:44:18.686596Z",
     "shell.execute_reply": "2025-10-19T04:44:18.686596Z"
    },
    "papermill": {
     "duration": 0.630105,
     "end_time": "2025-10-19T04:44:18.687603",
     "exception": false,
     "start_time": "2025-10-19T04:44:18.057498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.018\n",
      "Test R²: 0.017\n",
      "Test RMSE: 34.597\n"
     ]
    }
   ],
   "source": [
    "# Choose 5 features explicitly\n",
    "selected_features = [\"raindummy\",\"snowdummy\",\"windgustdummy\",\"temperature\",\"windspeed\",\"originmetropop\", \"originmetrogdppercapita\"]\n",
    "\n",
    "# Reduce dataset to 20% of original, but only keep those 5 columns\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend[selected_features],  # <--- subset here\n",
    "    y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Define a new preprocessor for just these features\n",
    "reg_prep_small = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), selected_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "poly_reg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_small),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit\n",
    "poly_reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = poly_reg_pipe.predict(X_train)\n",
    "y_pred_test = poly_reg_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    'model': 'Polynomial Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd84e3",
   "metadata": {
    "papermill": {
     "duration": 0.012002,
     "end_time": "2025-10-19T04:44:18.711605",
     "exception": false,
     "start_time": "2025-10-19T04:44:18.699603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### VIF: Variable Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91cb0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:44:18.736604Z",
     "iopub.status.busy": "2025-10-19T04:44:18.735603Z",
     "iopub.status.idle": "2025-10-19T04:44:49.931140Z",
     "shell.execute_reply": "2025-10-19T04:44:49.931140Z"
    },
    "papermill": {
     "duration": 31.220544,
     "end_time": "2025-10-19T04:44:49.943147",
     "exception": false,
     "start_time": "2025-10-19T04:44:18.722603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping constant columns: ['low_card__type_acft_7']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping perfectly collinear columns: ['num__day', 'num__hour']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tbran\\Python\\repos\\Semester 3 Repos\\capstone\\.venv\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Feature   VIF  High_VIF\n",
      "0         low_card__type_acft_4   inf      True\n",
      "1         low_card__type_acft_5   inf      True\n",
      "2         low_card__type_acft_6   inf      True\n",
      "3   low_card__ac_weight_CLASS 1   inf      True\n",
      "4   low_card__ac_weight_CLASS 4   inf      True\n",
      "..                          ...   ...       ...\n",
      "95           num__scheduledhour  1.07     False\n",
      "96     num__daily_route_flights  1.05     False\n",
      "97              num__dayofmonth  1.04     False\n",
      "98          num__raintracedummy  1.02     False\n",
      "99                  num__ac_cat  1.01     False\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "X_reg_mend_sample = X_reg_mend.sample(n=50000, random_state=42) if len(X_reg_mend) > 10000 else X_reg_mend\n",
    "vif_table = calculate_vif(X_reg_mend_sample, features=X_reg_mend_sample.columns.tolist(), vif_thresh=10.0)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9f2ac",
   "metadata": {
    "papermill": {
     "duration": 0.010999,
     "end_time": "2025-10-19T04:44:49.967146",
     "exception": false,
     "start_time": "2025-10-19T04:44:49.956147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Week 2 - Linear Regression 2\n",
    "\n",
    "For Week 2, include concepts such as linear regression with lasso, ridge, and elastic net regression. This homework will be submitted for peer review and feedback in Week 3 in the assignment titled 3.4 Peer Review: Week 2 Jupyter Notebook. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a18f",
   "metadata": {
    "papermill": {
     "duration": 0.011,
     "end_time": "2025-10-19T04:44:49.990147",
     "exception": false,
     "start_time": "2025-10-19T04:44:49.979147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0be88c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:44:50.016148Z",
     "iopub.status.busy": "2025-10-19T04:44:50.016148Z",
     "iopub.status.idle": "2025-10-19T04:44:54.001751Z",
     "shell.execute_reply": "2025-10-19T04:44:54.001751Z"
    },
    "papermill": {
     "duration": 3.999611,
     "end_time": "2025-10-19T04:44:54.002757",
     "exception": false,
     "start_time": "2025-10-19T04:44:50.003146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.066\n",
      "Test R²: 0.036\n",
      "Test RMSE: 34.256\n"
     ]
    }
   ],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Lasso\n",
    "lasso_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Lasso(alpha=0.1, max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lasso_pipe.predict(X_train)\n",
    "y_pred_test = lasso_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "mend_results.append({\n",
    "    'model': 'Lasso Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f6ee55",
   "metadata": {
    "papermill": {
     "duration": 0.011003,
     "end_time": "2025-10-19T04:44:54.025759",
     "exception": false,
     "start_time": "2025-10-19T04:44:54.014756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lasso Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "491616f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:44:54.048756Z",
     "iopub.status.busy": "2025-10-19T04:44:54.048756Z",
     "iopub.status.idle": "2025-10-19T04:46:25.842022Z",
     "shell.execute_reply": "2025-10-19T04:46:25.842022Z"
    },
    "papermill": {
     "duration": 91.82727,
     "end_time": "2025-10-19T04:46:25.864031",
     "exception": false,
     "start_time": "2025-10-19T04:44:54.036761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.001\n",
      "Test R²: 0.037\n",
      "Test RMSE: 34.246\n"
     ]
    }
   ],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + Lasso\n",
    "lasso_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Lasso(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "grid = GridSearchCV(\n",
    "    lasso_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best alpha:\", grid.best_params_[\"model__alpha\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    'model': 'Lasso Regression (Tuned)',\n",
    "    'best_alpha': grid.best_params_[\"model__alpha\"],\n",
    "    'train_r2': grid.best_score_,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6d279",
   "metadata": {
    "papermill": {
     "duration": 0.011999,
     "end_time": "2025-10-19T04:46:25.890029",
     "exception": false,
     "start_time": "2025-10-19T04:46:25.878030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c167ae51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:46:25.921028Z",
     "iopub.status.busy": "2025-10-19T04:46:25.920033Z",
     "iopub.status.idle": "2025-10-19T04:46:29.401681Z",
     "shell.execute_reply": "2025-10-19T04:46:29.401681Z"
    },
    "papermill": {
     "duration": 3.497659,
     "end_time": "2025-10-19T04:46:29.402687",
     "exception": false,
     "start_time": "2025-10-19T04:46:25.905028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.067\n",
      "Test R²: 0.037\n",
      "Test RMSE: 34.246\n"
     ]
    }
   ],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Ridge\n",
    "ridge_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Ridge(alpha=1.0, max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = ridge_pipe.predict(X_train)\n",
    "y_pred_test = ridge_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "mend_results.append({\n",
    "    'model': 'Ridge Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de903e5",
   "metadata": {
    "papermill": {
     "duration": 0.012007,
     "end_time": "2025-10-19T04:46:29.426694",
     "exception": false,
     "start_time": "2025-10-19T04:46:29.414687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ridge Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c19aaf1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:46:29.449691Z",
     "iopub.status.busy": "2025-10-19T04:46:29.449691Z",
     "iopub.status.idle": "2025-10-19T04:46:40.234498Z",
     "shell.execute_reply": "2025-10-19T04:46:40.234498Z"
    },
    "papermill": {
     "duration": 10.797814,
     "end_time": "2025-10-19T04:46:40.235504",
     "exception": false,
     "start_time": "2025-10-19T04:46:29.437690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge alpha: 100\n",
      "Ridge Test R²: 0.03657956549522978\n",
      "Ridge Test RMSE: 34.24586867799605\n"
     ]
    }
   ],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# Pipeline: preprocessing + Ridge\n",
    "ridge_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", Ridge(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "ridge_param_grid = {\n",
    "    \"model__alpha\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge_pipe,\n",
    "    ridge_param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Ridge alpha:\", ridge_grid.best_params_[\"model__alpha\"])\n",
    "\n",
    "y_pred_test = ridge_grid.predict(X_test)\n",
    "print(\"Ridge Test R²:\", r2_score(y_test, y_pred_test))\n",
    "print(\"Ridge Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "#store results\n",
    "mend_results.append({\n",
    "    'model': 'Ridge Regression (Tuned)',\n",
    "    'best_alpha': ridge_grid.best_params_[\"model__alpha\"],\n",
    "    'train_r2': ridge_grid.best_score_,\n",
    "    'test_r2': r2_score(y_test, y_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c99e1",
   "metadata": {
    "papermill": {
     "duration": 0.011996,
     "end_time": "2025-10-19T04:46:40.259504",
     "exception": false,
     "start_time": "2025-10-19T04:46:40.247508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a201c8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:46:40.283504Z",
     "iopub.status.busy": "2025-10-19T04:46:40.283504Z",
     "iopub.status.idle": "2025-10-19T04:46:45.327515Z",
     "shell.execute_reply": "2025-10-19T04:46:45.327515Z"
    },
    "papermill": {
     "duration": 5.05702,
     "end_time": "2025-10-19T04:46:45.328524",
     "exception": false,
     "start_time": "2025-10-19T04:46:40.271504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R²: 0.066\n",
      "Test R²: 0.036\n",
      "Test RMSE: 34.259\n"
     ]
    }
   ],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Elastic Net\n",
    "elasticnet_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000, random_state=42))\n",
    "])\n",
    "# Fit\n",
    "elasticnet_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = elasticnet_pipe.predict(X_train)\n",
    "y_pred_test = elasticnet_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "mend_results.append({\n",
    "    'model': 'Elastic Net Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d70713",
   "metadata": {
    "papermill": {
     "duration": 0.012,
     "end_time": "2025-10-19T04:46:45.352524",
     "exception": false,
     "start_time": "2025-10-19T04:46:45.340524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Elastic Net Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8a67925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-19T04:46:45.377520Z",
     "iopub.status.busy": "2025-10-19T04:46:45.376521Z",
     "iopub.status.idle": "2025-10-19T04:53:05.599285Z",
     "shell.execute_reply": "2025-10-19T04:53:05.599285Z"
    },
    "papermill": {
     "duration": 380.247767,
     "end_time": "2025-10-19T04:53:05.612291",
     "exception": false,
     "start_time": "2025-10-19T04:46:45.364524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ElasticNet params: {'model__alpha': 0.001, 'model__l1_ratio': 0.8}\n",
      "ElasticNet Test R²: 0.036592362709670034\n",
      "ElasticNet Test RMSE: 34.24564123151225\n"
     ]
    }
   ],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# Pipeline: preprocessing + ElasticNet\n",
    "elastic_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", ElasticNet(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 1, 10],\n",
    "    \"model__l1_ratio\": [0.2,  0.8]  # balance between L1 and L2\n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe,\n",
    "    elastic_param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "y_pred_test = elastic_grid.predict(X_test)\n",
    "print(\"ElasticNet Test R²:\", r2_score(y_test, y_pred_test))\n",
    "print(\"ElasticNet Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "#store results\n",
    "mend_results.append({\n",
    "    'model': 'Elastic Net Regression (Tuned)',\n",
    "    'best_params': elastic_grid.best_params_,\n",
    "    'train_r2': elastic_grid.best_score_,\n",
    "    'test_r2': r2_score(y_test, y_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35472fe2",
   "metadata": {
    "papermill": {
     "duration": 0.010999,
     "end_time": "2025-10-19T04:53:05.634289",
     "exception": false,
     "start_time": "2025-10-19T04:53:05.623290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Week 3 - Linear Regression 3\n",
    "\n",
    "For Week 3, include concepts such as linear regression with forward and backward selection, PCR, and PLSR. Complete your Jupyter Notebook homework by 11:59 pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdf8cc",
   "metadata": {
    "papermill": {
     "duration": 0.011,
     "end_time": "2025-10-19T04:53:05.657291",
     "exception": false,
     "start_time": "2025-10-19T04:53:05.646291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Forward & Backward Selection: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe3c38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-10-19T04:53:05.669290",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Shrink dataset to 20% ---\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    test_size=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess once\n",
    "X_proc = reg_prep_mend.fit_transform(X_sample, y_sample)\n",
    "y = y_sample\n",
    "\n",
    "# --- Forward Selection (limit to 30 features, step=2) ---\n",
    "forward_rmse = []\n",
    "k_range_fwd = range(1, min(31, X_proc.shape[1] + 1), 2)\n",
    "\n",
    "for k in k_range_fwd:\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=LinearRegression(),\n",
    "        n_features_to_select=k,\n",
    "        direction=\"forward\",\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sfs.fit(X_proc, y)\n",
    "    mask = sfs.get_support()\n",
    "    # Evaluate using CV on the reduced feature set\n",
    "    scores = cross_val_score(\n",
    "        LinearRegression(),\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rmse = np.sqrt(-scores.mean())\n",
    "    forward_rmse.append(rmse)\n",
    "\n",
    "best_idx_fwd = np.argmin(forward_rmse)\n",
    "best_k_fwd = list(k_range_fwd)[best_idx_fwd]\n",
    "best_rmse_fwd = forward_rmse[best_idx_fwd]\n",
    "\n",
    "mend_results.append({\n",
    "    'model': 'Forward Selection Linear Regression',\n",
    "    'num_features': best_k_fwd,\n",
    "    'best_cv_rmse': best_rmse_fwd,\n",
    "    'rmse_curve': forward_rmse,\n",
    "    'k_range': list(k_range_fwd)\n",
    "})\n",
    "\n",
    "# --- Backward Selection (step size=3, early stopping) ---\n",
    "backward_rmse = []\n",
    "k_range_bwd = range(1, X_proc.shape[1] + 1, 3)\n",
    "\n",
    "tolerance = 1e-3\n",
    "best_rmse_so_far = np.inf\n",
    "\n",
    "for k in k_range_bwd:\n",
    "    sbs = SequentialFeatureSelector(\n",
    "        estimator=LinearRegression(),\n",
    "        n_features_to_select=k,\n",
    "        direction=\"backward\",\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sbs.fit(X_proc, y)\n",
    "    mask = sbs.get_support()\n",
    "\n",
    "    # Evaluate this subset with CV\n",
    "    scores = cross_val_score(\n",
    "        LinearRegression(),\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rmse = np.sqrt(-scores.mean())\n",
    "    backward_rmse.append(rmse)\n",
    "\n",
    "    # Early stopping if curve flattens\n",
    "    if best_rmse_so_far - rmse < tolerance:\n",
    "        print(f\"Stopping early at {k} features (no significant improvement).\")\n",
    "        break\n",
    "    best_rmse_so_far = min(best_rmse_so_far, rmse)\n",
    "\n",
    "\n",
    "best_idx_bwd = np.argmin(backward_rmse)\n",
    "best_k_bwd = list(k_range_bwd)[:len(backward_rmse)][best_idx_bwd]\n",
    "best_rmse_bwd = backward_rmse[best_idx_bwd]\n",
    "\n",
    "mend_results.append({\n",
    "    'model': 'Backward Selection Linear Regression',\n",
    "    'num_features': best_k_bwd,\n",
    "    'best_cv_rmse': best_rmse_bwd,\n",
    "    'rmse_curve': backward_rmse,\n",
    "    'k_range': list(k_range_bwd)[:len(backward_rmse)]\n",
    "})\n",
    "\n",
    "# --- Plot curves ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_range_fwd, forward_rmse, marker=\"o\", label=\"Forward Selection\")\n",
    "plt.plot(list(k_range_bwd)[:len(backward_rmse)], backward_rmse, marker=\"s\", label=\"Backward Selection\")\n",
    "plt.xlabel(\"Number of Features Selected\")\n",
    "plt.ylabel(\"CV RMSE\")\n",
    "plt.title(\"Forward vs Backward Selection (optimized)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0461f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### PCR\n",
    "Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a7788",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PCR pipeline: preprocessing → PCA → Linear Regression\n",
    "pcr_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),   # your ColumnTransformer\n",
    "    (\"pca\", PCA()),                    # dimensionality reduction\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Grid search over number of components\n",
    "max_components = min(X_train.shape[0], X_train.shape[1])\n",
    "\n",
    "param_grid = {\n",
    "    \"pca__n_components\": list(range(5, max_components+1, 5))\n",
    "}\n",
    "\n",
    "pcr_grid = GridSearchCV(\n",
    "    pcr_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "pcr_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best number of components\n",
    "print(\"Best n_components:\", pcr_grid.best_params_[\"pca__n_components\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = pcr_grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"PCR Test R²: {test_r2:.3f}\")\n",
    "print(f\"PCR Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "mend_results.append({\n",
    "    \"Model\": \"PCR\",\n",
    "    \"Best Params\": pcr_grid.best_params_,\n",
    "    \"Test R²\": test_r2,\n",
    "    \"Test RMSE\": test_rmse\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a9361",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### PLSR\n",
    "Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02303914",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend, y_mend_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_mend_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PLSR pipeline: preprocessing → PLSRegression\n",
    "pls_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),   # your ColumnTransformer\n",
    "    (\"model\", PLSRegression())\n",
    "])\n",
    "\n",
    "# Grid search over number of components\n",
    "param_grid = {\n",
    "    \"model__n_components\": [2, 5, 10, 20, 40]  # tune based on dataset size\n",
    "}\n",
    "\n",
    "pls_grid = GridSearchCV(\n",
    "    pls_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "pls_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best number of components\n",
    "print(\"Best n_components:\", pls_grid.best_params_[\"model__n_components\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = pls_grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"PLSR Test R²: {test_r2:.3f}\")\n",
    "print(f\"PLSR Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "mend_results.append({\n",
    "    \"Model\": \"PLSR\",\n",
    "    \"Best Params\": pls_grid.best_params_,\n",
    "    \"Test R²\": test_r2,\n",
    "    \"Test RMSE\": test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd613b8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 4 - Logistic Regression and Feature Scaling\n",
    "\n",
    "For Week 4, include concepts such as logistic regression and feature scaling. This homework should be submitted for peer review in the assignment titled 4.3 Peer Review: Week 4 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526a100",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regression: Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701383b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_mend_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Build pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = logreg_pipe.predict(X_train)\n",
    "y_pred_test = logreg_pipe.predict(X_test)\n",
    "y_pred_proba = logreg_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "train_bal_acc = balanced_accuracy_score(y_train, y_pred_train)\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Train Balanced Accuracy: {train_bal_acc:.3f}\")\n",
    "print(f\"Test Balanced Accuracy: {test_bal_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    \"Model\": \"Logistic Regression\",\n",
    "    \"Train Accuracy\": train_acc,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Train Balanced Accuracy\": train_bal_acc,\n",
    "    \"Test Balanced Accuracy\": test_bal_acc,\n",
    "\n",
    "    \"Test AUC\": test_auc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d929d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regression: Forward & Backward Selection: Light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1abc0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Shrink dataset to 20% with stratification on binary target ---\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X_mend, y_mend_binary_15,\n",
    "    test_size=0.8,\n",
    "    random_state=42,\n",
    "    stratify=y_mend_binary_15\n",
    ")\n",
    "\n",
    "# Preprocess once\n",
    "X_proc = reg_prep_mend.fit_transform(X_sample)\n",
    "y = y_sample\n",
    "\n",
    "# Define estimator\n",
    "logreg = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "\n",
    "# --- Forward Selection (limit to 30 features) ---\n",
    "forward_scores = []\n",
    "k_range_fwd = range(1, min(41, X_proc.shape[1] + 1))\n",
    "\n",
    "tolerance = 1e-3\n",
    "best_score_so_far = -np.inf\n",
    "\n",
    "for k in k_range_fwd:\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=logreg,\n",
    "        n_features_to_select=k,\n",
    "        direction=\"forward\",\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sfs.fit(X_proc, y)\n",
    "    mask = sfs.get_support()\n",
    "    score = cross_val_score(\n",
    "        logreg,\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3\n",
    "    ).mean()\n",
    "    forward_scores.append(score)\n",
    "\n",
    "    # Early stopping if improvement is too small\n",
    "    if score - best_score_so_far < tolerance:\n",
    "        print(f\"Forward selection stopping early at {k} features (Δ<{tolerance}).\")\n",
    "        break\n",
    "    best_score_so_far = max(best_score_so_far, score)\n",
    "\n",
    "# Best forward\n",
    "best_idx_fwd = np.argmax(forward_scores)\n",
    "best_k_fwd = list(k_range_fwd)[best_idx_fwd]\n",
    "best_score_fwd = forward_scores[best_idx_fwd]\n",
    "\n",
    "mend_results.append({\n",
    "    'model': 'Forward Selection Logistic Regression',\n",
    "    'num_features': best_k_fwd,\n",
    "    'best_cv_bal_acc': best_score_fwd,\n",
    "    'score_curve': forward_scores,\n",
    "    'k_range': list(k_range_fwd)\n",
    "})\n",
    "\n",
    "# --- Backward Selection (step size = 3, early stopping) ---\n",
    "backward_scores = []\n",
    "n_features = X_proc.shape[1]\n",
    "\n",
    "tolerance = 1e-3\n",
    "best_score_so_far = -np.inf\n",
    "\n",
    "for k in range(n_features, 0, -1):  # from all features down to 1\n",
    "    sbs = SequentialFeatureSelector(\n",
    "        estimator=logreg,\n",
    "        n_features_to_select=k,\n",
    "        direction=\"backward\",\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sbs.fit(X_proc, y)\n",
    "    mask = sbs.get_support()\n",
    "    score = cross_val_score(\n",
    "        logreg,\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3\n",
    "    ).mean()\n",
    "    backward_scores.append(score)\n",
    "\n",
    "    # Early stopping if curve flattens\n",
    "    if score - best_score_so_far < tolerance:\n",
    "        print(f\"Backward selection stopping early at {k} features (Δ<{tolerance}).\")\n",
    "        break\n",
    "    best_score_so_far = max(best_score_so_far, score)\n",
    "\n",
    "# Reverse scores so they align with decreasing k\n",
    "backward_scores = backward_scores[::-1]\n",
    "k_range_bwd = list(range(n_features, n_features - len(backward_scores), -1))[::-1]\n",
    "\n",
    "# Best backward\n",
    "best_idx_bwd = np.argmax(backward_scores)\n",
    "best_k_bwd = list(k_range_bwd)[:len(backward_scores)][best_idx_bwd]\n",
    "best_score_bwd = backward_scores[best_idx_bwd]\n",
    "\n",
    "mend_results.append({\n",
    "    'model': 'Backward Selection Logistic Regression',\n",
    "    'num_features': best_k_bwd,\n",
    "    'best_cv_bal_acc': best_score_bwd,\n",
    "    'score_curve': backward_scores,\n",
    "    'k_range': list(k_range_bwd)[:len(backward_scores)]\n",
    "})\n",
    "\n",
    "# --- Plot curves ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_range_fwd, forward_scores, marker=\"o\", label=\"Forward Selection\")\n",
    "plt.plot(list(k_range_bwd)[:len(backward_scores)], backward_scores, marker=\"s\", label=\"Backward Selection\")\n",
    "plt.xlabel(\"Number of Features Selected\")\n",
    "plt.ylabel(\"CV Balanced Accuracy\")\n",
    "plt.title(\"Forward vs Backward Selection (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40f62e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regression: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab6b6d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_mend_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, solver=\"saga\", penalty=\"l1\"))  \n",
    "    # saga supports both L1 and L2\n",
    "])\n",
    "\n",
    "# Parameter distributions for random search\n",
    "param_distributions = {\n",
    "    \"model__C\": loguniform(1e-3, 1e3),   # inverse regularization strength\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],      # try both penalties\n",
    "    \"model__solver\": [\"saga\"]            # saga works with both l1 and l2\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,                # number of random samples\n",
    "    cv=3,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = random_search.predict(X_test)\n",
    "y_pred_proba = random_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(f\"Test Balanced Accuracy: {test_bal_acc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    \"Model\": \"Logistic Regression (Random Search)\",\n",
    "    \"Best Params\": random_search.best_params_,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test AUC\": test_auc,\n",
    "    \"Test Balanced Accuracy\": test_bal_acc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36167c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regession: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4e8fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_mend_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "# Pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, solver=\"saga\"))  \n",
    "    # saga supports both L1 and L2\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"model__C\": [0.001, 0.01, 0.1, 1, 10, 100],   # regularization strength\n",
    "    \"model__penalty\": [\"l1\", \"l2\"]                # L1 (Lasso) or L2 (Ridge)\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test)\n",
    "y_pred_proba = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    \"Model\": \"Logistic Regression (Grid Search)\",\n",
    "    \"Best Params\": grid.best_params_,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test AUC\": test_auc\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13b062",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 5 - Support Vector Machines\n",
    "\n",
    "For Week 5, include concepts such as support vector machines, the kernel trick, and regularization for support vector machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1e8e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVM Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304e5ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_mend_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Step 3: Pipeline: preprocessing + SVM (RBF kernel by default)\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_test = svm_pipe.predict(X_test)\n",
    "y_pred_proba = svm_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Basic SVM Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Basic SVM AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    \"Model\": \"SVM\",\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7213d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVM Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dea8a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_mend_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Pipeline: preprocessing + SVM\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter distributions\n",
    "param_distributions = {\n",
    "    \"model__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"model__C\": loguniform(1e-3, 1e3),   # regularization strength\n",
    "    \"model__gamma\": [\"scale\", \"auto\", 0.01, 0.1, 1]  # only relevant for RBF\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "svm_random = RandomizedSearchCV(\n",
    "    svm_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,                # number of random samples\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "svm_random.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Params:\", svm_random.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = svm_random.predict(X_test)\n",
    "y_pred_proba = svm_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Search SVM Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Search SVM AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "#store results\n",
    "mend_results.append({\n",
    "    \"Model\": \"SVM (Random Search)\",\n",
    "    \"Best Params\": svm_random.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf128055",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVM Grid Search + Kernel Trick: Linear vs. RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c40d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_mend_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Pipeline: preprocessing + SVM\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters (RBF only)\n",
    "param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"model__gamma\": [0.001, 0.01, 0.1, 1, \"scale\"]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Collect results into DataFrame\n",
    "cv_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "\n",
    "# Pivot table for heatmap (mean test AUC)\n",
    "heatmap_data = cv_results.pivot(\n",
    "    index=\"param_model__C\",\n",
    "    columns=\"param_model__gamma\",\n",
    "    values=\"mean_test_score\"\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "plt.title(\"SVM RBF Kernel: AUC across C and gamma\")\n",
    "plt.ylabel(\"C (Regularization)\")\n",
    "plt.xlabel(\"Gamma (Kernel Width)\")\n",
    "plt.show()\n",
    "\n",
    "# Best params + score\n",
    "print(\"Best Params:\", svm_grid.best_params_)\n",
    "print(\"Best CV AUC:\", svm_grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = svm_grid.predict(X_test)\n",
    "y_pred_proba = svm_grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "#store results\n",
    "mend_results.append({\n",
    "    \"Model\": \"SVM (Grid Search)\",\n",
    "    \"Best Params\": svm_grid.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de485de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 6 - Decision Trees and Random Forests \n",
    "\n",
    "For Week 6, include concepts such as decision trees and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2455e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ce3e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Reduce dataset to 20% (stratified) ---\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,\n",
    "    stratify=y_mend_binary_15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# --- Step 2: Train/test split (80/20 of reduced set) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# --- Pipeline: preprocessing + Decision Tree with class weights ---\n",
    "dt_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\"   # <-- handle class imbalance\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Parameter distributions for random search ---\n",
    "dt_param_dist = {\n",
    "    \"model__max_depth\": [None, 3, 5, 10, 20],\n",
    "    \"model__min_samples_split\": randint(2, 20),\n",
    "    \"model__min_samples_leaf\": randint(1, 10),\n",
    "    \"model__criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
    "}\n",
    "\n",
    "# --- Randomized search ---\n",
    "dt_random = RandomizedSearchCV(\n",
    "    dt_pipe,\n",
    "    dt_param_dist,\n",
    "    n_iter=20,              # number of random draws\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# --- Fit ---\n",
    "with parallel_backend(\"threading\"):\n",
    "    dt_random.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_test = dt_random.predict(X_test)\n",
    "y_pred_proba = dt_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Decision Tree Best Params:\", dt_random.best_params_)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Decision Tree AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# --- Store results ---\n",
    "mend_results.append({\n",
    "    \"Model\": \"Decision Tree (Random Search, class_weight=balanced)\",\n",
    "    \"Best Params\": dt_random.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})\n",
    "\n",
    "# ---- Feature Importances ----\n",
    "best_dt = dt_random.best_estimator_\n",
    "dt_model = best_dt.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_dt.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# Map importances back to feature names\n",
    "importances = pd.Series(dt_model.feature_importances_, index=feature_names)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=importances_sorted.head(20),\n",
    "    y=importances_sorted.head(20).index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Top 20 Feature Importances - Decision Tree (Random Search)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fbd142",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Random Forest Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1141235",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Reduce dataset to 20% (stratified) ---\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,\n",
    "    stratify=y_mend_binary_15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Train/test split (80/20 of reduced set) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Pipeline: preprocessing + Random Forest ---\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=200,       # number of trees\n",
    "        max_depth=None,         # let trees expand fully\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\" # handle class imbalance\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Fit ---\n",
    "with parallel_backend(\"threading\"):\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_test = rf_pipe.predict(X_test)\n",
    "y_pred_proba = rf_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# --- Feature Importances ---\n",
    "rf_model = rf_pipe.named_steps[\"model\"]\n",
    "feature_names = rf_pipe.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "importances = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=importances_sorted.head(20),\n",
    "    y=importances_sorted.head(20).index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Top 20 Feature Importances - Random Forest (Baseline)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "RocCurveDisplay.from_estimator(rf_pipe, X_test, y_test)\n",
    "plt.title(\"ROC Curve - Random Forest (Baseline)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(rf_pipe, X_test, y_test)\n",
    "plt.title(\"Precision-Recall Curve - Random Forest (Baseline)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(rf_pipe, X_test, y_test, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Random Forest (Baseline)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Sort by predicted probability\n",
    "results = pd.DataFrame({\"y_true\": y_test, \"y_score\": y_pred_proba})\n",
    "results = results.sort_values(\"y_score\", ascending=False).reset_index(drop=True)\n",
    "results[\"cumulative_positive\"] = results[\"y_true\"].cumsum()\n",
    "results[\"percent_samples\"] = (results.index + 1) / len(results)\n",
    "results[\"percent_positives\"] = results[\"cumulative_positive\"] / results[\"y_true\"].sum()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(results[\"percent_samples\"], results[\"percent_positives\"], label=\"Model\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\", label=\"Random\")\n",
    "plt.xlabel(\"Fraction of Samples\")\n",
    "plt.ylabel(\"Fraction of Positives Captured\")\n",
    "plt.title(\"Cumulative Gain Curve - Random Forest\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "with parallel_backend(\"threading\"):\n",
    "    perm_importance = permutation_importance(rf_pipe, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    \n",
    "# Align feature names with permutation importance output\n",
    "fitted_feature_names = rf_pipe.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "n_importances = len(perm_importance.importances_mean)\n",
    "\n",
    "# Slice feature names if there's a mismatch\n",
    "aligned_feature_names = fitted_feature_names[:n_importances]\n",
    "\n",
    "perm_sorted = pd.Series(\n",
    "    perm_importance.importances_mean,\n",
    "    index=aligned_feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=perm_sorted.head(20), y=perm_sorted.head(20).index, palette=\"magma\")\n",
    "plt.title(\"Top 20 Features - Permutation Importance\")\n",
    "plt.xlabel(\"Importance (mean decrease in score)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Store results ---\n",
    "mend_results.append({\n",
    "    \"Model\": \"Random Forest (Baseline)\",\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred_test)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98f33d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### RF w/ Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd357535",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Reduce dataset to 20% (stratified) ---\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,\n",
    "    stratify=y_mend_binary_15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# --- Step 2: Train/test split (80/20 of reduced set) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# --- Pipeline: preprocessing + Random Forest with class weights ---\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\"   # <-- key change\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Parameter distributions for random search ---\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": randint(100, 500),\n",
    "    \"model__max_depth\": [None, 5, 10, 20],\n",
    "    \"model__min_samples_split\": randint(2, 10),\n",
    "    \"model__min_samples_leaf\": randint(1, 5)\n",
    "}\n",
    "\n",
    "# --- Randomized search ---\n",
    "rf_random = RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    rf_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# --- Fit ---\n",
    "with parallel_backend(\"threading\"):\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_test = rf_random.predict(X_test)\n",
    "y_pred_proba = rf_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Best Params:\", rf_random.best_params_)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# --- Store results ---\n",
    "mend_results.append({\n",
    "    \"Model\": \"Random Forest (Random Search, class_weight=balanced)\",\n",
    "    \"Best Params\": rf_random.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred_test)\n",
    "})\n",
    "\n",
    "# ---- Feature Importances ----\n",
    "best_rf = rf_random.best_estimator_\n",
    "rf_model = best_rf.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_rf.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# Map importances back to feature names\n",
    "importances = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=importances_sorted.head(20),\n",
    "    y=importances_sorted.head(20).index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Top 20 Feature Importances - Random Forest (class_weight=balanced)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- ROC Curve ---\n",
    "RocCurveDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"ROC Curve - Random Forest (Random Search)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "PrecisionRecallDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"Precision-Recall Curve - Random Forest (Random Search)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix Heatmap ---\n",
    "ConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Random Forest (Random Search)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Permutation Importance (more robust than Gini) ---\n",
    "with parallel_backend(\"threading\"):\n",
    "    perm_importance = permutation_importance(\n",
    "        best_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "# Align feature names with the actual number of features used\n",
    "n_importances = len(perm_importance.importances_mean)\n",
    "aligned_feature_names = feature_names[:n_importances]\n",
    "\n",
    "perm_sorted = pd.Series(\n",
    "    perm_importance.importances_mean,\n",
    "    index=aligned_feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=perm_sorted.head(20), y=perm_sorted.head(20).index, palette=\"magma\")\n",
    "plt.title(\"Top 20 Features - Permutation Importance\")\n",
    "plt.xlabel(\"Importance (mean decrease in score)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e27e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Align feature names with the actual number of features used\n",
    "n_importances = len(perm_importance.importances_mean)\n",
    "aligned_feature_names = feature_names[:n_importances]\n",
    "\n",
    "perm_sorted = pd.Series(\n",
    "    perm_importance.importances_mean,\n",
    "    index=aligned_feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=perm_sorted.head(20), y=perm_sorted.head(20).index, palette=\"magma\")\n",
    "plt.title(\"Top 20 Features - Permutation Importance\")\n",
    "plt.xlabel(\"Importance (mean decrease in score)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980599c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### RF w/ Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e9b08",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_mend_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_mend.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Pipeline: preprocessing + Random Forest\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", RandomForestClassifier(random_state=42, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "rf_param_grid = {\n",
    "    \"model__n_estimators\": [50, 100],\n",
    "    \"model__max_depth\": [5, 10, 25],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe,\n",
    "    rf_param_grid,\n",
    "    cv=2,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "with parallel_backend(\"threading\"):\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    \n",
    "# Evaluate\n",
    "y_pred_test = rf_grid.predict(X_test)\n",
    "y_pred_proba = rf_grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Best Params:\", rf_grid.best_params_)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "mend_results.append({\n",
    "    \"Model\": \"Random Forest (Grid Search, class_weight=balanced)\",\n",
    "    \"Best Params\": rf_grid.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred_test)\n",
    "})\n",
    "\n",
    "importances = rf_grid.best_estimator_.named_steps[\"model\"].feature_importances_\n",
    "feature_names = rf_grid.best_estimator_.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "pd.Series(importances, index=feature_names).sort_values(ascending=False).head(20)\n",
    "\n",
    "# Get the best fitted pipeline\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "# Extract the trained RandomForest model\n",
    "rf_model = best_rf.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_rf.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# Pair feature names with importances\n",
    "importances = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "\n",
    "# Sort by importance\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances_sorted.head(20), y=importances_sorted.head(20).index, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Feature Importances - Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- ROC Curve ---\n",
    "RocCurveDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"ROC Curve - Random Forest (Grid Search)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "PrecisionRecallDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"Precision-Recall Curve - Random Forest (Grid Search)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix Heatmap ---\n",
    "ConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Random Forest (Grid Search)\")\n",
    "plt.show()\n",
    "\n",
    "# Transform X_test with the preprocessor to get the actual feature matrix\n",
    "X_test_transformed = best_rf.named_steps[\"preprocessor\"].transform(X_test)\n",
    "\n",
    "# Get the feature names aligned with the transformed matrix\n",
    "feature_names = best_rf.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "print(\"X_test_transformed shape:\", X_test_transformed.shape)\n",
    "print(\"Number of feature names:\", len(feature_names))\n",
    "\n",
    "with parallel_backend(\"threading\"):\n",
    "    perm_importance = permutation_importance(\n",
    "        best_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Align lengths\n",
    "perm_sorted = pd.Series(\n",
    "    perm_importance.importances_mean,\n",
    "    index=feature_names[:len(perm_importance.importances_mean)]\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=perm_sorted.head(20), y=perm_sorted.head(20).index, palette=\"magma\")\n",
    "plt.title(\"Top 20 Features - Permutation Importance\")\n",
    "plt.xlabel(\"Importance (mean decrease in score)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d941c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform X_test with the preprocessor to get the actual feature matrix\n",
    "X_test_transformed = best_rf.named_steps[\"preprocessor\"].transform(X_test)\n",
    "\n",
    "# Get the feature names aligned with the transformed matrix\n",
    "feature_names = best_rf.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "print(\"X_test_transformed shape:\", X_test_transformed.shape)\n",
    "print(\"Number of feature names:\", len(feature_names))\n",
    "with parallel_backend(\"threading\"):\n",
    "    perm_importance = permutation_importance(\n",
    "        best_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "# Align lengths\n",
    "perm_sorted = pd.Series(\n",
    "    perm_importance.importances_mean,\n",
    "    index=feature_names[:len(perm_importance.importances_mean)]\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=perm_sorted.head(20), y=perm_sorted.head(20).index, palette=\"magma\")\n",
    "plt.title(\"Top 20 Features - Permutation Importance\")\n",
    "plt.xlabel(\"Importance (mean decrease in score)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a890e26",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 7 - Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a45a13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920340c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert list of dicts into DataFrame\n",
    "mend_results_df = pd.DataFrame(mend_results)\n",
    "\n",
    "# Sort by test_r2 (descending) or test_rmse (ascending)\n",
    "# results_df_sorted = results_df.sort_values(by=\"test_r2\", ascending=False)\n",
    "\n",
    "print(mend_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda7a9f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=mend_results_df, x=\"model\", y=\"test_r2\", palette=\"viridis\")\n",
    "plt.title(\"Model Comparison (Test R²)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=mend_results_df, x=\"model\", y=\"test_rmse\", palette=\"magma\")\n",
    "plt.title(\"Model Comparison (Test RMSE)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87f20f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# USDOT Carrier On-Time Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5c243",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa14cb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Data Prep Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeabdca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def engineer_usdot_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- Datetime features ---\n",
    "    df['fl_date'] = pd.to_datetime(\n",
    "        df['fl_date'].astype(str),   # convert category → string\n",
    "        format=\"%m/%d/%Y %I:%M:%S %p\",  # matches \"6/27/2025 12:00:00 AM\"\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    df['year'] = df['fl_date'].dt.year\n",
    "    df['month'] = df['fl_date'].dt.month\n",
    "    df['day'] = df['fl_date'].dt.day\n",
    "    df['day_of_week'] = df['fl_date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "    # Scheduled departure hour\n",
    "    df['crs_dep_hour'] = (df['crs_dep_time'] // 100).astype(int)\n",
    "    df['part_of_day'] = pd.cut(\n",
    "        df['crs_dep_hour'],\n",
    "        bins=[-1,5,11,17,21,24],\n",
    "        labels=['night','morning','afternoon','evening','late_night']\n",
    "    )\n",
    "    \n",
    "    # --- Route features ---\n",
    "    df['route'] = df['origin'].astype(str) + \"_\" + df['dest'].astype(str)\n",
    "    \n",
    "    major_hubs = {'ATL','ORD','DFW','DEN','JFK','LAX','SFO','CLT','LAS','PHX','MIA'}\n",
    "    df['is_hub_origin'] = df['origin'].isin(major_hubs).astype(int)\n",
    "    df['is_hub_dest'] = df['dest'].isin(major_hubs).astype(int)\n",
    "    \n",
    "    # --- Operational features ---\n",
    "    df['taxi_time'] = df[['taxi_out','taxi_in']].sum(axis=1, skipna=True)\n",
    "    df['air_time_ratio'] = np.where(\n",
    "        df['crs_elapsed_time']>0,\n",
    "        df['air_time'] / df['crs_elapsed_time'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Flight distance buckets (custom)\n",
    "    df['distance_bucket'] = pd.cut(\n",
    "        df['distance'],\n",
    "        bins=[0,500,1000,1500,2000,3000,10000],\n",
    "        labels=['short','medium','long','xlong','xxlong','ultra']\n",
    "    )\n",
    "    \n",
    "    # --- Delay flags ---\n",
    "    df['is_delayed_15'] = (df['dep_delay'] >= 15).astype(int)\n",
    "    df['is_delayed_60'] = (df['dep_delay'] >= 60).astype(int)\n",
    "    \n",
    "    # --- Fill missing values ---\n",
    "    if 'tail_num' in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df['tail_num']):\n",
    "            df['tail_num'] = df['tail_num'].cat.add_categories([\"UNKNOWN_TAIL\"])\n",
    "        df['tail_num'] = df['tail_num'].fillna(\"UNKNOWN_TAIL\")\n",
    "\n",
    "    if 'cancellation_code' in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df['cancellation_code']):\n",
    "            df['cancellation_code'] = df['cancellation_code'].cat.add_categories([\"NONE\"])\n",
    "        df['cancellation_code'] = df['cancellation_code'].fillna(\"NONE\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072355e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Data Prep Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6b0594",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use glob to find all matching CSV files\n",
    "all_files = glob.glob(os.path.join(data_path, \"T_ONTIME_REPORTING_2025*.csv\"))\n",
    "\n",
    "# Read and combine them\n",
    "dfs = [pd.read_csv(f) for f in all_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Files combined:\", len(all_files))\n",
    "print(\"Final shape:\", combined_df.shape)\n",
    "\n",
    "# Drop diverted columns\n",
    "combined_df = combined_df.drop(combined_df.filter(regex=r\"^DIV\\d+\").columns, axis=1)\n",
    "\n",
    "df_usdot = optimize_dataframe(\n",
    "    combined_df,\n",
    "    datetime_cols=['fl_date'],\n",
    "    fillna=True\n",
    ")\n",
    "df_usdot = clean_column_names(df_usdot)\n",
    "\n",
    "# Remove outliers from dataset\n",
    "df_usdot_clean = df_usdot[df_usdot['dep_delay'] >= -30].copy()\n",
    "print(\"Cleaned shape:\", df_usdot_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856de0c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"FAA_AC_REGISTRATION_2021.csv\"\n",
    "df_reg = pd.read_csv(data_path + file_name)\n",
    "\n",
    "# Left join on stripped tail_num\n",
    "df_usdot_clean_reg = df_usdot_clean.merge(\n",
    "    df_reg,\n",
    "    how=\"left\",\n",
    "    left_on=df_usdot_clean[\"tail_num\"].str.lstrip(\"N\"),\n",
    "    right_on=\"N-NUMBER\"\n",
    ")\n",
    "\n",
    "# Impute missing values with mode per column\n",
    "mode_dict = {col: df_reg[col].mode()[0] for col in df_reg.columns if col != \"N-NUMBER\"}\n",
    "df_usdot_clean_reg = df_usdot_clean_reg.fillna(mode_dict)\n",
    "\n",
    "# Drop duplicate join key\n",
    "df_usdot_clean_reg = df_usdot_clean_reg.drop(columns=[\"N-NUMBER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ba988",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Engineer features\n",
    "df_usdot_clean_eng = engineer_usdot_features(df_usdot_clean_reg)\n",
    "\n",
    "# Get column categories\n",
    "df_usdot_id_cols = ['op_carrier_airline_id', 'origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'origin_state_fips', 'origin_wac', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'dest_state_fips', 'dest_wac', 'crs_dep_time', 'crs_arr_time']\n",
    "df_usdot_cat_cols = ['op_unique_carrier', 'op_carrier', 'tail_num', 'origin', 'origin_city_name', 'origin_state_abr', 'origin_state_nm', 'dest', 'dest_city_name', 'dest_state_abr', 'dest_state_nm', 'dest_state_fips', 'dest_wac', 'dep_time_blk', 'arr_time_blk', 'cancellation_code,']\n",
    "df_usdot_date_cols = ['fl_date', ]\n",
    "df_usdot_target_cols = ['dep_delay', 'dep_delay_new', 'dep_del15', 'dep_delay_group', 'arr_delay', 'arr_delay_new', 'arr_del15', \n",
    "                        'arr_delay_group', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay',\n",
    "                        'is_delayed_15', 'is_delayed_60']\n",
    "df_usdot_feature_cols = [col for col in df_usdot_clean_eng.columns if col not in df_usdot_id_cols + df_usdot_cat_cols + df_usdot_date_cols + df_usdot_target_cols]\n",
    "\n",
    "# drop leakage columns\n",
    "TARGET_COLUMN = 'dep_delay'\n",
    "df_usdot_leakage_cols = [x for x in df_usdot_target_cols if x != TARGET_COLUMN]\n",
    "df_usdot_clean_filter = df_usdot_clean_eng.drop(df_usdot_leakage_cols + df_usdot_id_cols + df_usdot_date_cols, axis=1, errors=\"ignore\").copy()\n",
    "# Export df to csv\n",
    "df_usdot_clean_filter.to_csv(data_path + 'USDOTDelayData_Cleaned.csv', index=False)\n",
    "\n",
    "reg_prep_usdot, tree_prep_usdot, X_usdot, y_usdot_numeric = build_dual_preprocessors(df_usdot_clean_filter, \n",
    "                                                                                 target=TARGET_COLUMN, \n",
    "                                                                                 feature_cols=df_usdot_feature_cols, \n",
    "                                                                                 high_card_threshold=30, scale_numeric=True)\n",
    "\n",
    "# Create binary target for classification (15 min delay threshold)\n",
    "y_usdot_binary_15 = (y_usdot_numeric >= 15).astype(int)\n",
    "\n",
    "# Ensure categorical columns are string type (Some reg data has mixed types)\n",
    "cat_cols = ['cancellation_code', 'MFR', 'TYPE-ACFT', 'AC-WEIGHT', 'part_of_day', 'distance_bucket']\n",
    "for col in cat_cols:\n",
    "    X_usdot[col] = X_usdot[col].astype(str)\n",
    "\n",
    "# Create seperate transformed datasets for regression and tree models\n",
    "X_reg_usdot = transform_with_names(reg_prep_usdot, X_usdot, y_usdot_numeric)\n",
    "X_tree_usdot = transform_with_names(tree_prep_usdot, X_usdot, y_usdot_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eac970",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 1: Linear Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19c6f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Week 1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0e83c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regression_summary(X, y):\n",
    "    \"\"\"\n",
    "    Fits an OLS regression model using statsmodels and prints the summary.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    return model.summary()\n",
    "\n",
    "def fit_polynomial_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial regression model and returns the fitted model and transformed features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "\n",
    "def calculate_vif(df, features=None, vif_thresh=10.0):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) safely:\n",
    "    - Removes constant columns\n",
    "    - Removes perfectly collinear columns\n",
    "    - Returns sorted VIF table\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with numeric features\n",
    "        features (list): Optional list of features to check; defaults to all numeric\n",
    "        vif_thresh (float): Threshold for flagging high VIF\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: VIF table\n",
    "    \"\"\"\n",
    "    # Select numeric columns if features not provided\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    X = df[features].copy()\n",
    "    \n",
    "    # 1. Drop constant columns\n",
    "    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        print(f\"Dropping constant columns: {constant_cols}\")\n",
    "        X.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    # 2. Drop perfectly collinear columns\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    perfect_corr_cols = [col for col in upper.columns if any(upper[col] == 1.0)]\n",
    "    if perfect_corr_cols:\n",
    "        print(f\"Dropping perfectly collinear columns: {perfect_corr_cols}\")\n",
    "        X.drop(columns=perfect_corr_cols, inplace=True)\n",
    "    \n",
    "    # 3. Calculate VIF\n",
    "    X_const = X.assign(const=1)\n",
    "    vif_data = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"VIF\": [variance_inflation_factor(X_const.values, i) for i in range(len(X.columns))]\n",
    "    })\n",
    "    \n",
    "    # 4. Sort by VIF\n",
    "    vif_data.sort_values(by=\"VIF\", ascending=False, inplace=True)\n",
    "    \n",
    "    # 5. Flag high VIF\n",
    "    vif_data[\"High_VIF\"] = vif_data[\"VIF\"] > vif_thresh\n",
    "    \n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135368d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ba88d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_usdot, y_usdot_numeric, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline\n",
    "linreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit\n",
    "linreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = linreg_pipe.predict(X_train)\n",
    "y_pred_test = linreg_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    'model': 'Linear Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcd2ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Reduced the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ddd0f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose 5 features explicitly\n",
    "selected_features = [\"air_time\", \"distance\", \"crs_dep_hour\", \"is_hub_dest\", \"is_weekend\",\n",
    "                     \"crs_elapsed_time\", \"taxi_time\", \"air_time_ratio\"]\n",
    "\n",
    "# Reduce dataset to 20% of original, but only keep those 5 columns\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot[selected_features],  # <-- subset here\n",
    "    y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Define a new preprocessor for just these features\n",
    "reg_prep_small = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), selected_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "poly_reg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_small),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit\n",
    "poly_reg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = poly_reg_pipe.predict(X_train)\n",
    "y_pred_test = poly_reg_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    'model': 'Polynomial Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a68b14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### VIF: Variable Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580328c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_reg_usdot_sample = X_reg_usdot.sample(n=50000, random_state=42) if len(X_reg_usdot) > 10000 else X_reg_usdot\n",
    "vif_table = calculate_vif(X_reg_usdot_sample, features=X_reg_usdot_sample.columns.tolist(), vif_thresh=10.0)\n",
    "print(vif_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6316f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 2: Linear Regression 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34155af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b104d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Lasso\n",
    "lasso_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", Lasso(alpha=0.1, max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lasso_pipe.predict(X_train)\n",
    "y_pred_test = lasso_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "usdot_results.append({\n",
    "    'model': 'Lasso Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693465c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Lasso Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040712c9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + Lasso\n",
    "lasso_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", Lasso(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "grid = GridSearchCV(\n",
    "    lasso_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best alpha:\", grid.best_params_[\"model__alpha\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    'model': 'Lasso Regression (Tuned)',\n",
    "    'best_alpha': grid.best_params_[\"model__alpha\"],\n",
    "    'train_r2': grid.best_score_,\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a05178",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774d2a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Ridge\n",
    "ridge_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", Ridge(alpha=1.0, max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = ridge_pipe.predict(X_train)\n",
    "y_pred_test = ridge_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "usdot_results.append({\n",
    "    'model': 'Ridge Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e162c66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Ridge Regression Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1533b0b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# Pipeline: preprocessing + Ridge\n",
    "ridge_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", Ridge(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "ridge_param_grid = {\n",
    "    \"model__alpha\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    ridge_pipe,\n",
    "    ridge_param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Ridge alpha:\", ridge_grid.best_params_[\"model__alpha\"])\n",
    "\n",
    "y_pred_test = ridge_grid.predict(X_test)\n",
    "print(\"Ridge Test R²:\", r2_score(y_test, y_pred_test))\n",
    "print(\"Ridge Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "#store results\n",
    "usdot_results.append({\n",
    "    'model': 'Ridge Regression (Tuned)',\n",
    "    'best_alpha': ridge_grid.best_params_[\"model__alpha\"],\n",
    "    'train_r2': ridge_grid.best_score_,\n",
    "    'test_r2': r2_score(y_test, y_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19407c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de4aa1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build pipeline with preprocessing + Elastic Net\n",
    "elasticnet_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=10000, random_state=42))\n",
    "])\n",
    "# Fit\n",
    "elasticnet_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = elasticnet_pipe.predict(X_train)\n",
    "y_pred_test = elasticnet_pipe.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Train R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "#store results\n",
    "usdot_results.append({\n",
    "    'model': 'Elastic Net Regression',\n",
    "    'train_r2': train_r2,\n",
    "    'test_r2': test_r2, \n",
    "    'test_rmse': test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873e9f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Elastic Net Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a139b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# Pipeline: preprocessing + ElasticNet\n",
    "elastic_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", ElasticNet(max_iter=10000, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "elastic_param_grid = {\n",
    "    \"model__alpha\": [0.001, 0.01, 1, 10],\n",
    "    \"model__l1_ratio\": [0.2,  0.8]  # balance between L1 and L2\n",
    "}\n",
    "\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_pipe,\n",
    "    elastic_param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best ElasticNet params:\", elastic_grid.best_params_)\n",
    "\n",
    "y_pred_test = elastic_grid.predict(X_test)\n",
    "print(\"ElasticNet Test R²:\", r2_score(y_test, y_pred_test))\n",
    "print(\"ElasticNet Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "#store results\n",
    "usdot_results.append({\n",
    "    'model': 'Elastic Net Regression (Tuned)',\n",
    "    'best_params': elastic_grid.best_params_,\n",
    "    'train_r2': elastic_grid.best_score_,\n",
    "    'test_r2': r2_score(y_test, y_pred_test),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c5f4a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 3: Linear Regression 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c284f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Forward & Backward Selection: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb7760",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Shrink dataset to 20% ---\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    test_size=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess once\n",
    "X_proc = reg_prep_usdot.fit_transform(X_sample, y_sample)\n",
    "y = y_sample\n",
    "\n",
    "# --- Forward Selection (limit to 30 features, step=2) ---\n",
    "forward_rmse = []\n",
    "k_range_fwd = range(1, min(31, X_proc.shape[1] + 1), 2)\n",
    "\n",
    "for k in k_range_fwd:\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=LinearRegression(),\n",
    "        n_features_to_select=k,\n",
    "        direction=\"forward\",\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sfs.fit(X_proc, y)\n",
    "    mask = sfs.get_support()\n",
    "    # Evaluate using CV on the reduced feature set\n",
    "    scores = cross_val_score(\n",
    "        LinearRegression(),\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rmse = np.sqrt(-scores.mean())\n",
    "    forward_rmse.append(rmse)\n",
    "\n",
    "best_idx_fwd = np.argmin(forward_rmse)\n",
    "best_k_fwd = list(k_range_fwd)[best_idx_fwd]\n",
    "best_rmse_fwd = forward_rmse[best_idx_fwd]\n",
    "\n",
    "usdot_results.append({\n",
    "    'model': 'Forward Selection Linear Regression',\n",
    "    'num_features': best_k_fwd,\n",
    "    'best_cv_rmse': best_rmse_fwd,\n",
    "    'rmse_curve': forward_rmse,\n",
    "    'k_range': list(k_range_fwd)\n",
    "})\n",
    "\n",
    "# --- Backward Selection (step size=3, early stopping) ---\n",
    "backward_rmse = []\n",
    "k_range_bwd = range(1, X_proc.shape[1] + 1, 3)\n",
    "\n",
    "tolerance = 1e-3\n",
    "best_rmse_so_far = np.inf\n",
    "\n",
    "for k in k_range_bwd:\n",
    "    sbs = SequentialFeatureSelector(\n",
    "        estimator=LinearRegression(),\n",
    "        n_features_to_select=k,\n",
    "        direction=\"backward\",\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sbs.fit(X_proc, y)\n",
    "    mask = sbs.get_support()\n",
    "\n",
    "    # Evaluate this subset with CV\n",
    "    scores = cross_val_score(\n",
    "        LinearRegression(),\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rmse = np.sqrt(-scores.mean())\n",
    "    backward_rmse.append(rmse)\n",
    "\n",
    "    # Early stopping if curve flattens\n",
    "    if best_rmse_so_far - rmse < tolerance:\n",
    "        print(f\"Stopping early at {k} features (no significant improvement).\")\n",
    "        break\n",
    "    best_rmse_so_far = min(best_rmse_so_far, rmse)\n",
    "\n",
    "\n",
    "best_idx_bwd = np.argmin(backward_rmse)\n",
    "best_k_bwd = list(k_range_bwd)[:len(backward_rmse)][best_idx_bwd]\n",
    "best_rmse_bwd = backward_rmse[best_idx_bwd]\n",
    "\n",
    "usdot_results.append({\n",
    "    'model': 'Backward Selection Linear Regression',\n",
    "    'num_features': best_k_bwd,\n",
    "    'best_cv_rmse': best_rmse_bwd,\n",
    "    'rmse_curve': backward_rmse,\n",
    "    'k_range': list(k_range_bwd)[:len(backward_rmse)]\n",
    "})\n",
    "\n",
    "# --- Plot curves ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_range_fwd, forward_rmse, marker=\"o\", label=\"Forward Selection\")\n",
    "plt.plot(list(k_range_bwd)[:len(backward_rmse)], backward_rmse, marker=\"s\", label=\"Backward Selection\")\n",
    "plt.xlabel(\"Number of Features Selected\")\n",
    "plt.ylabel(\"CV RMSE\")\n",
    "plt.title(\"Forward vs Backward Selection (optimized)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382530b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### PCR\n",
    "Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8dfa1f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PCR pipeline: preprocessing → PCA → Linear Regression\n",
    "pcr_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),   # your ColumnTransformer\n",
    "    (\"pca\", PCA()),                    # dimensionality reduction\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Grid search over number of components\n",
    "max_components = min(X_train.shape[0], X_train.shape[1])\n",
    "\n",
    "param_grid = {\n",
    "    \"pca__n_components\": list(range(5, max_components+1, 5))\n",
    "}\n",
    "\n",
    "pcr_grid = GridSearchCV(\n",
    "    pcr_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "pcr_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best number of components\n",
    "print(\"Best n_components:\", pcr_grid.best_params_[\"pca__n_components\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = pcr_grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"PCR Test R²: {test_r2:.3f}\")\n",
    "print(f\"PCR Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "usdot_results.append({\n",
    "    \"Model\": \"PCR\",\n",
    "    \"Best Params\": pcr_grid.best_params_,\n",
    "    \"Test R²\": test_r2,\n",
    "    \"Test RMSE\": test_rmse\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a5b8e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### PLSR\n",
    "Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b264b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce dataset to 20% of original\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot, y_usdot_numeric,\n",
    "    train_size=0.2,\n",
    "    stratify=None,   # or stratify=y_usdot_binary_15 if classification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train/test split on reduced set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# PLSR pipeline: preprocessing → PLSRegression\n",
    "pls_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),   # your ColumnTransformer\n",
    "    (\"model\", PLSRegression())\n",
    "])\n",
    "\n",
    "# Grid search over number of components\n",
    "param_grid = {\n",
    "    \"model__n_components\": [2, 5, 10, 20, 40]  # tune based on dataset size\n",
    "}\n",
    "\n",
    "pls_grid = GridSearchCV(\n",
    "    pls_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "pls_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best number of components\n",
    "print(\"Best n_components:\", pls_grid.best_params_[\"model__n_components\"])\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = pls_grid.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"PLSR Test R²: {test_r2:.3f}\")\n",
    "print(f\"PLSR Test RMSE: {test_rmse:.3f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "usdot_results.append({\n",
    "    \"Model\": \"PLSR\",\n",
    "    \"Best Params\": pls_grid.best_params_,\n",
    "    \"Test R²\": test_r2,\n",
    "    \"Test RMSE\": test_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb8545",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 4: Log Regression and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1c481",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regression: Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30a9fd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_usdot_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Build pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = logreg_pipe.predict(X_train)\n",
    "y_pred_test = logreg_pipe.predict(X_test)\n",
    "y_pred_proba = logreg_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "train_bal_acc = balanced_accuracy_score(y_train, y_pred_train)\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Train Balanced Accuracy: {train_bal_acc:.3f}\")\n",
    "print(f\"Test Balanced Accuracy: {test_bal_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    \"Model\": \"Logistic Regression\",\n",
    "    \"Train Accuracy\": train_acc,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Train Balanced Accuracy\": train_bal_acc,\n",
    "    \"Test Balanced Accuracy\": test_bal_acc,\n",
    "    \"Test AUC\": test_auc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957663b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regression: Forward & Backward Selection: Light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba8aed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Shrink dataset to 20% with stratification on binary target ---\n",
    "X_sample, _, y_sample, _ = train_test_split(\n",
    "    X_usdot, y_usdot_binary_15,\n",
    "    test_size=0.8,\n",
    "    random_state=42,\n",
    "    stratify=y_usdot_binary_15\n",
    ")\n",
    "\n",
    "# Preprocess once\n",
    "X_proc = reg_prep_usdot.fit_transform(X_sample, y_sample)\n",
    "y = y_sample\n",
    "\n",
    "# Define estimator\n",
    "logreg = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
    "\n",
    "# --- Forward Selection (limit to 30 features) ---\n",
    "forward_scores = []\n",
    "k_range_fwd = range(1, min(31, X_proc.shape[1] + 1))\n",
    "\n",
    "tolerance = 1e-3\n",
    "best_score_so_far = -np.inf\n",
    "\n",
    "for k in k_range_fwd:\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=logreg,\n",
    "        n_features_to_select=k,\n",
    "        direction=\"forward\",\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sfs.fit(X_proc, y)\n",
    "    mask = sfs.get_support()\n",
    "    score = cross_val_score(\n",
    "        logreg,\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3\n",
    "    ).mean()\n",
    "    forward_scores.append(score)\n",
    "\n",
    "    # Early stopping if improvement is too small\n",
    "    if score - best_score_so_far < tolerance:\n",
    "        print(f\"Forward selection stopping early at {k} features (Δ<{tolerance}).\")\n",
    "        break\n",
    "    best_score_so_far = max(best_score_so_far, score)\n",
    "\n",
    "# Best forward\n",
    "best_idx_fwd = np.argmax(forward_scores)\n",
    "best_k_fwd = list(k_range_fwd)[best_idx_fwd]\n",
    "best_score_fwd = forward_scores[best_idx_fwd]\n",
    "\n",
    "usdot_results.append({\n",
    "    'model': 'Forward Selection Logistic Regression',\n",
    "    'num_features': best_k_fwd,\n",
    "    'best_cv_bal_acc': best_score_fwd,\n",
    "    'score_curve': forward_scores,\n",
    "    'k_range': list(k_range_fwd)\n",
    "})\n",
    "\n",
    "# --- Backward Selection (step size = 3, early stopping) ---\n",
    "backward_scores = []\n",
    "n_features = X_proc.shape[1]\n",
    "\n",
    "tolerance = 1e-3\n",
    "best_score_so_far = -np.inf\n",
    "\n",
    "for k in range(n_features, 0, -1):  # from all features down to 1\n",
    "    sbs = SequentialFeatureSelector(\n",
    "        estimator=logreg,\n",
    "        n_features_to_select=k,\n",
    "        direction=\"backward\",\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sbs.fit(X_proc, y)\n",
    "    mask = sbs.get_support()\n",
    "    score = cross_val_score(\n",
    "        logreg,\n",
    "        X_proc[:, mask],\n",
    "        y,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=3\n",
    "    ).mean()\n",
    "    backward_scores.append(score)\n",
    "\n",
    "    # Early stopping if curve flattens\n",
    "    if score - best_score_so_far < tolerance:\n",
    "        print(f\"Backward selection stopping early at {k} features (Δ<{tolerance}).\")\n",
    "        break\n",
    "    best_score_so_far = max(best_score_so_far, score)\n",
    "\n",
    "# Reverse scores so they align with decreasing k\n",
    "backward_scores = backward_scores[::-1]\n",
    "k_range_bwd = list(range(n_features, n_features - len(backward_scores), -1))[::-1]\n",
    "\n",
    "# Best backward\n",
    "best_idx_bwd = np.argmax(backward_scores)\n",
    "best_k_bwd = list(k_range_bwd)[:len(backward_scores)][best_idx_bwd]\n",
    "best_score_bwd = backward_scores[best_idx_bwd]\n",
    "\n",
    "usdot_results.append({\n",
    "    'model': 'Backward Selection Logistic Regression',\n",
    "    'num_features': best_k_bwd,\n",
    "    'best_cv_bal_acc': best_score_bwd,\n",
    "    'score_curve': backward_scores,\n",
    "    'k_range': list(k_range_bwd)[:len(backward_scores)]\n",
    "})\n",
    "\n",
    "# --- Plot curves ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(k_range_fwd, forward_scores, marker=\"o\", label=\"Forward Selection\")\n",
    "plt.plot(list(k_range_bwd)[:len(backward_scores)], backward_scores, marker=\"s\", label=\"Backward Selection\")\n",
    "plt.xlabel(\"Number of Features Selected\")\n",
    "plt.ylabel(\"CV Balanced Accuracy\")\n",
    "plt.title(\"Forward vs Backward Selection (Logistic Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb6603",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regression: Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d38708",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_usdot_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, solver=\"saga\", penalty=\"l1\"))  \n",
    "    # saga supports both L1 and L2\n",
    "])\n",
    "\n",
    "# Parameter distributions for random search\n",
    "param_distributions = {\n",
    "    \"model__C\": loguniform(1e-3, 1e3),   # inverse regularization strength\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],      # try both penalties\n",
    "    \"model__solver\": [\"saga\"]            # saga works with both l1 and l2\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,                # number of random samples\n",
    "    cv=3,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = random_search.predict(X_test)\n",
    "y_pred_proba = random_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "test_bal_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(f\"Test Balanced Accuracy: {test_bal_acc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    \"Model\": \"Logistic Regression (Random Search)\",\n",
    "    \"Best Params\": random_search.best_params_,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test AUC\": test_auc,\n",
    "    \"Test Balanced Accuracy\": test_bal_acc\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd14f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Log Regession: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8bbe8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_usdot_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "# Pipeline: preprocessing + logistic regression\n",
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, solver=\"saga\"))  \n",
    "    # saga supports both L1 and L2\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"model__C\": [0.001, 0.01, 0.1, 1, 10, 100],   # regularization strength\n",
    "    \"model__penalty\": [\"l1\", \"l2\"]                # L1 (Lasso) or L2 (Ridge)\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = grid.predict(X_test)\n",
    "y_pred_proba = grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Test AUC: {test_auc:.3f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    \"Model\": \"Logistic Regression (Grid Search)\",\n",
    "    \"Best Params\": grid.best_params_,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Test AUC\": test_auc\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8879af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 5 - Support Vector Machines\n",
    "\n",
    "For Week 5, include concepts such as support vector machines, the kernel trick, and regularization for support vector machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fef5c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVM Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280a983",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_usdot_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Step 3: Pipeline: preprocessing + SVM (RBF kernel by default)\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "svm_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_test = svm_pipe.predict(X_test)\n",
    "y_pred_proba = svm_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"Basic SVM Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Basic SVM AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    \"Model\": \"SVM\",\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f5f0f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVM Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f81d6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_usdot_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Pipeline: preprocessing + SVM\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter distributions\n",
    "param_distributions = {\n",
    "    \"model__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"model__C\": loguniform(1e-3, 1e3),   # regularization strength\n",
    "    \"model__gamma\": [\"scale\", \"auto\", 0.01, 0.1, 1]  # only relevant for RBF\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "svm_random = RandomizedSearchCV(\n",
    "    svm_pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,                # number of random samples\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit\n",
    "svm_random.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Params:\", svm_random.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = svm_random.predict(X_test)\n",
    "y_pred_proba = svm_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Search SVM Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Search SVM AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "#store results\n",
    "usdot_results.append({\n",
    "    \"Model\": \"SVM (Random Search)\",\n",
    "    \"Best Params\": svm_random.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc84f90",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVM Grid Search + Kernel Trick: Linear vs. RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ebb6b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_usdot_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Pipeline: preprocessing + SVM\n",
    "svm_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters (RBF only)\n",
    "param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"model__gamma\": [0.001, 0.01, 0.1, 1, \"scale\"]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Collect results into DataFrame\n",
    "cv_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "\n",
    "# Pivot table for heatmap (mean test AUC)\n",
    "heatmap_data = cv_results.pivot(\n",
    "    index=\"param_model__C\",\n",
    "    columns=\"param_model__gamma\",\n",
    "    values=\"mean_test_score\"\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "plt.title(\"SVM RBF Kernel: AUC across C and gamma\")\n",
    "plt.ylabel(\"C (Regularization)\")\n",
    "plt.xlabel(\"Gamma (Kernel Width)\")\n",
    "plt.show()\n",
    "\n",
    "# Best params + score\n",
    "print(\"Best Params:\", svm_grid.best_params_)\n",
    "print(\"Best CV AUC:\", svm_grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test = svm_grid.predict(X_test)\n",
    "y_pred_proba = svm_grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "#store results\n",
    "usdot_results.append({\n",
    "    \"Model\": \"SVM (Grid Search)\",\n",
    "    \"Best Params\": svm_grid.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a20c9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 6 - Decision Trees and Random Forests \n",
    "\n",
    "For Week 6, include concepts such as decision trees and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667851a3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03143240",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Reduce dataset to 20% (stratified) ---\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,\n",
    "    stratify=y_usdot_binary_15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# --- Step 2: Train/test split (80/20 of reduced set) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# --- Pipeline: preprocessing + Decision Tree with class weights ---\n",
    "dt_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\"   # <-- handle class imbalance\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Parameter distributions for random search ---\n",
    "dt_param_dist = {\n",
    "    \"model__max_depth\": [None, 3, 5, 10, 20],\n",
    "    \"model__min_samples_split\": randint(2, 20),\n",
    "    \"model__min_samples_leaf\": randint(1, 10),\n",
    "    \"model__criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
    "}\n",
    "\n",
    "# --- Randomized search ---\n",
    "dt_random = RandomizedSearchCV(\n",
    "    dt_pipe,\n",
    "    dt_param_dist,\n",
    "    n_iter=20,              # number of random draws\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# --- Fit ---\n",
    "with parallel_backend(\"threading\"):\n",
    "    dt_random.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_test = dt_random.predict(X_test)\n",
    "y_pred_proba = dt_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Decision Tree Best Params:\", dt_random.best_params_)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Decision Tree AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# --- Store results ---\n",
    "usdot_results.append({\n",
    "    \"Model\": \"Decision Tree (Random Search, class_weight=balanced)\",\n",
    "    \"Best Params\": dt_random.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred_test)\n",
    "})\n",
    "\n",
    "# ---- Feature Importances ----\n",
    "best_dt = dt_random.best_estimator_\n",
    "dt_model = best_dt.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_dt.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# Map importances back to feature names\n",
    "importances = pd.Series(dt_model.feature_importances_, index=feature_names)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=importances_sorted.head(20),\n",
    "    y=importances_sorted.head(20).index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Top 20 Feature Importances - Decision Tree (Random Search)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85867d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Random Forecast Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b793531",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Reduce dataset to 20% (stratified) ---\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_mend,\n",
    "    y_mend_binary_15,\n",
    "    train_size=0.2,\n",
    "    stratify=y_mend_binary_15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 2: Train/test split (80/20 of reduced set) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Pipeline: preprocessing + Random Forest ---\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_mend),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=200,       # number of trees\n",
    "        max_depth=None,         # let trees expand fully\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\" # handle class imbalance\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Fit ---\n",
    "with parallel_backend(\"threading\"):\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_test = rf_pipe.predict(X_test)\n",
    "y_pred_proba = rf_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# --- Feature Importances ---\n",
    "rf_model = rf_pipe.named_steps[\"model\"]\n",
    "feature_names = rf_pipe.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "importances = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=importances_sorted.head(20),\n",
    "    y=importances_sorted.head(20).index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Top 20 Feature Importances - Random Forest (Baseline)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Store results ---\n",
    "usdot_results.append({\n",
    "    \"Model\": \"Random Forest (Baseline, class_weight=balanced)\",\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred_test)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2d92b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### RF w/ Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fac90",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 1: Reduce dataset to 20% (stratified) ---\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,\n",
    "    stratify=y_usdot_binary_15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# --- Step 2: Train/test split (80/20 of reduced set) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# --- Pipeline: preprocessing + Random Forest with class weights ---\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced\"   # <-- key change\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Parameter distributions for random search ---\n",
    "rf_param_dist = {\n",
    "    \"model__n_estimators\": randint(100, 500),\n",
    "    \"model__max_depth\": [None, 5, 10, 20],\n",
    "    \"model__min_samples_split\": randint(2, 10),\n",
    "    \"model__min_samples_leaf\": randint(1, 5)\n",
    "}\n",
    "\n",
    "# --- Randomized search ---\n",
    "rf_random = RandomizedSearchCV(\n",
    "    rf_pipe,\n",
    "    rf_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# --- Fit ---\n",
    "with parallel_backend(\"threading\"):\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_test = rf_random.predict(X_test)\n",
    "y_pred_proba = rf_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Best Params:\", rf_random.best_params_)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# --- Store results ---\n",
    "usdot_results.append({\n",
    "    \"Model\": \"Random Forest (Random Search, class_weight=balanced)\",\n",
    "    \"Best Params\": rf_random.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred_test)\n",
    "})\n",
    "\n",
    "# ---- Feature Importances ----\n",
    "best_rf = rf_random.best_estimator_\n",
    "rf_model = best_rf.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_rf.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# Map importances back to feature names\n",
    "importances = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    x=importances_sorted.head(20),\n",
    "    y=importances_sorted.head(20).index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Top 20 Feature Importances - Random Forest (class_weight=balanced)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc102904",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### RF w/ Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b7d55",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Reduce dataset to 20% of original size (stratified)\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_usdot,\n",
    "    y_usdot_binary_15,\n",
    "    train_size=0.2,          # keep 20% of original\n",
    "    stratify=y_usdot_binary_15, # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original size:\", X_usdot.shape)\n",
    "print(\"Reduced size:\", X_small.shape)\n",
    "\n",
    "# Step 2: Split reduced dataset into train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small,\n",
    "    y_small,\n",
    "    test_size=0.2,           # 20% of reduced set → 16% train, 4% test of original\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# Pipeline: preprocessing + Random Forest\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", reg_prep_usdot),\n",
    "    (\"model\", RandomForestClassifier(random_state=42, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Grid of hyperparameters\n",
    "rf_param_grid = {\n",
    "    \"model__n_estimators\": [50, 100],\n",
    "    \"model__max_depth\": [5, 10, 25],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe,\n",
    "    rf_param_grid,\n",
    "    cv=2,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "with parallel_backend(\"threading\"):\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_test = rf_grid.predict(X_test)\n",
    "y_pred_proba = rf_grid.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest Best Params:\", rf_grid.best_params_)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Random Forest AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Store results\n",
    "usdot_results.append({\n",
    "    \"Model\": \"Random Forest (Grid Search, class_weight=balanced)\",\n",
    "    \"Best Params\": rf_grid.best_params_,\n",
    "    \"Test Accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"Test AUC\": roc_auc_score(y_test, y_pred_proba),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_test, y_pred_test)\n",
    "})\n",
    "\n",
    "importances = rf_grid.best_estimator_.named_steps[\"model\"].feature_importances_\n",
    "feature_names = rf_grid.best_estimator_.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "pd.Series(importances, index=feature_names).sort_values(ascending=False).head(20)\n",
    "\n",
    "# Get the best fitted pipeline\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "# Extract the trained RandomForest model\n",
    "rf_model = best_rf.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_rf.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "# Pair feature names with importances\n",
    "importances = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "\n",
    "# Sort by importance\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances_sorted.head(20), y=importances_sorted.head(20).index, palette=\"viridis\")\n",
    "plt.title(\"Top 20 Feature Importances - Random Forest\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- ROC Curve ---\n",
    "RocCurveDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"ROC Curve - Random Forest (Grid Search)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "PrecisionRecallDisplay.from_estimator(best_rf, X_test, y_test)\n",
    "plt.title(\"Precision-Recall Curve - Random Forest (Grid Search)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix Heatmap ---\n",
    "ConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Random Forest (Grid Search)\")\n",
    "plt.show()\n",
    "\n",
    "# Transform X_test with the preprocessor to get the actual feature matrix\n",
    "X_test_transformed = best_rf.named_steps[\"preprocessor\"].transform(X_test)\n",
    "\n",
    "# Get the feature names aligned with the transformed matrix\n",
    "feature_names = best_rf.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "print(\"X_test_transformed shape:\", X_test_transformed.shape)\n",
    "print(\"Number of feature names:\", len(feature_names))\n",
    "with parallel_backend(\"threading\"):\n",
    "    perm_importance = permutation_importance(\n",
    "        best_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "# Align lengths\n",
    "perm_sorted = pd.Series(\n",
    "    perm_importance.importances_mean,\n",
    "    index=feature_names[:len(perm_importance.importances_mean)]\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=perm_sorted.head(20), y=perm_sorted.head(20).index, palette=\"magma\")\n",
    "plt.title(\"Top 20 Features - Permutation Importance\")\n",
    "plt.xlabel(\"Importance (mean decrease in score)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445330e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Week 7 - Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f48fcf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Review Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e823aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert list of dicts into DataFrame\n",
    "usdot_results_df = pd.DataFrame(usdot_results)\n",
    "\n",
    "# Sort by test_r2 (descending) or test_rmse (ascending)\n",
    "usdot_results_df_sorted = usdot_results_df.sort_values(by=\"Test AUC\", ascending=False)\n",
    "\n",
    "print(usdot_results_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20273408",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=usdot_results_df, x=\"model\", y=\"test_r2\", palette=\"viridis\")\n",
    "plt.title(\"Model Comparison (Test R²)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=usdot_results_df, x=\"model\", y=\"test_rmse\", palette=\"magma\")\n",
    "plt.title(\"Model Comparison (Test RMSE)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/799-p1-Notebook overnight run.ipynb",
   "output_path": "notebooks/799-p1-Notebook_out.ipynb",
   "parameters": {},
   "start_time": "2025-10-19T04:43:01.150384",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}